{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://www.kaggle.com/blackcore/the-nature-conservancy-fisheries-monitoring/fish-keras-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import datetime\n",
    "import time\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import (Convolution2D, MaxPooling2D,\n",
    "                                        ZeroPadding2D, AveragePooling2D)\n",
    "from keras.optimizers import SGD, Adagrad\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.constraints import maxnorm\n",
    "from sklearn.metrics import log_loss\n",
    "from keras import __version__ as keras_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 1.2.0\n"
     ]
    }
   ],
   "source": [
    "print('Keras version: {}'.format(keras_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1989)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_im_cv2(path):\n",
    "    img = cv2.imread(path)[:,:,::-1] # BGR -> RGB\n",
    "    resized = cv2.resize(img, (64, 64), cv2.INTER_LINEAR)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_train():\n",
    "    X_train = []\n",
    "    X_train_id = []\n",
    "    y_train = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    tqdm.write('Read train images')\n",
    "    folders = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "    for fld in folders:\n",
    "        index = folders.index(fld)\n",
    "        tqdm.write('Load folder {} (Index: {})'.format(fld, index))\n",
    "        path = os.path.join('..', 'data', 'train', fld, '*.jpg')\n",
    "        files = glob.glob(path)\n",
    "        for fl in tqdm_notebook(files):\n",
    "            flbase = os.path.basename(fl)\n",
    "            img = get_im_cv2(fl)\n",
    "            X_train.append(img)\n",
    "            X_train_id.append(flbase)\n",
    "            y_train.append(index)\n",
    "\n",
    "    tqdm.write('Read train data time: {} seconds'.format(\n",
    "        round(time.time() - start_time, 2)))\n",
    "    return X_train, y_train, X_train_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_test():\n",
    "    path = os.path.join('..', 'data', 'test_stg1', '*.jpg')\n",
    "    files = sorted(glob.glob(path))\n",
    "\n",
    "    X_test = []\n",
    "    X_test_id = []\n",
    "    for fl in tqdm_notebook(files):\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_im_cv2(fl)\n",
    "        X_test.append(img)\n",
    "        X_test_id.append(flbase)\n",
    "\n",
    "    return X_test, X_test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_submission(predictions, test_id, info):\n",
    "    result1 = pd.DataFrame(predictions,\n",
    "                           columns=['ALB', 'BET', 'DOL', 'LAG',\n",
    "                                    'NoF', 'OTHER', 'SHARK', 'YFT'])\n",
    "    result1.loc[:, 'image'] = pd.Series(test_id, index=result1.index)\n",
    "    now = datetime.datetime.now()\n",
    "    sub_file = ('../data/submission/' + 'submission_' + info + '_'\n",
    "                + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv')\n",
    "    result1.to_csv(sub_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_and_normalize_train_data():\n",
    "    train_data, train_target, train_id = load_train()\n",
    "\n",
    "    print('Convert to numpy...')\n",
    "    train_data = np.array(train_data, dtype=np.uint8)\n",
    "    train_target = np.array(train_target, dtype=np.uint8)\n",
    "\n",
    "    print('Reshape...')\n",
    "    #uncomment for theano\n",
    "    #train_data = train_data.transpose((0, 3, 1, 2))\n",
    "\n",
    "    print('Convert to float...')\n",
    "    train_data = train_data.astype('float32')\n",
    "    train_data = train_data / 255\n",
    "    train_target = np_utils.to_categorical(train_target, 8)\n",
    "\n",
    "    print('Train shape:', train_data.shape)\n",
    "    print(train_data.shape[0], 'train samples')\n",
    "    return train_data, train_target, train_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_and_normalize_test_data():\n",
    "    start_time = time.time()\n",
    "    test_data, test_id = load_test()\n",
    "\n",
    "    test_data = np.array(test_data, dtype=np.uint8)\n",
    "    #uncomment for theano\n",
    "    #test_data = test_data.transpose((0, 3, 1, 2))\n",
    "\n",
    "    test_data = test_data.astype('float32')\n",
    "    test_data = test_data / 255\n",
    "\n",
    "    print('Test shape:', test_data.shape)\n",
    "    print(test_data.shape[0], 'test samples')\n",
    "    print('Read and process test data time: {} seconds'.format(\n",
    "        round(time.time() - start_time, 2)))\n",
    "    return test_data, test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dict_to_list(d):\n",
    "    ret = []\n",
    "    for i in d.items():\n",
    "        ret.append(i[1])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_several_folds_mean(data, nfolds):\n",
    "    a = np.array(data[0])\n",
    "    for i in range(1, nfolds):\n",
    "        a += np.array(data[i])\n",
    "    a /= nfolds\n",
    "    return a.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1, 1), input_shape=(64, 64, 3),\n",
    "                            dim_ordering='tf'))\n",
    "    model.add(Convolution2D(8, 3, 3, activation='relu',\n",
    "                            dim_ordering='tf', init='he_uniform'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2),\n",
    "                           dim_ordering='tf'))\n",
    "    model.add(ZeroPadding2D((1, 1), dim_ordering='tf'))\n",
    "    model.add(Convolution2D(16, 3, 3, activation='relu',\n",
    "                            dim_ordering='tf', init='he_uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2),\n",
    "                           dim_ordering='tf'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(96, activation='relu', init='he_uniform'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(24, activation='relu', init='he_uniform'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "    sgd = SGD(lr=1e-2, decay=1e-4, momentum=0.88, nesterov=False)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_validation_predictions(train_data, predictions_valid):\n",
    "    pv = []\n",
    "    for i in range(len(train_data)):\n",
    "        pv.append(predictions_valid[i])\n",
    "    return pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_cross_validation_create_models(train_data, train_target, train_id,\n",
    "                                       nfolds=10):\n",
    "    # input image dimensions\n",
    "    batch_size = 32\n",
    "    nb_epoch = 8\n",
    "    random_state = 51\n",
    "    first_rl = 96\n",
    "\n",
    "    yfull_train = dict()\n",
    "    kf = KFold(n_splits=nfolds, shuffle=True,\n",
    "               random_state=random_state)\n",
    "    num_fold = 0\n",
    "    sum_score = 0\n",
    "    models = []\n",
    "    for train_index, test_index in kf.split(range(len(train_id))):\n",
    "        model = create_model()\n",
    "        X_train = train_data[train_index]\n",
    "        Y_train = train_target[train_index]\n",
    "        X_valid = train_data[test_index]\n",
    "        Y_valid = train_target[test_index]\n",
    "\n",
    "        num_fold += 1\n",
    "        print('Start KFold number {} from {}'.format(num_fold, nfolds))\n",
    "        print('Split train: ', len(X_train), len(Y_train))\n",
    "        print('Split valid: ', len(X_valid), len(Y_valid))\n",
    "\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n",
    "        ]\n",
    "        model.fit(X_train, Y_train, batch_size=batch_size,\n",
    "                  nb_epoch=nb_epoch, shuffle=True, verbose=2,\n",
    "                  validation_data=(X_valid, Y_valid), callbacks=callbacks)\n",
    "\n",
    "        predictions_valid = model.predict(X_valid.astype('float32'),\n",
    "                                          batch_size=batch_size, verbose=2)\n",
    "        score = log_loss(Y_valid, predictions_valid)\n",
    "        print('Score log_loss: ', score)\n",
    "        sum_score += score*len(test_index)\n",
    "\n",
    "        # Store valid predictions\n",
    "        for i in range(len(test_index)):\n",
    "            yfull_train[test_index[i]] = predictions_valid[i]\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "    score = sum_score/len(train_data)\n",
    "    print(\"Log_loss train independent avg: \", score)\n",
    "\n",
    "    info_string = ('_' + str(np.round(score,3)) + '_flds_' + str(nfolds)\n",
    "                   + '_eps_' + str(nb_epoch) + '_fl_' + str(first_rl))\n",
    "    return info_string, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_cross_validation_process_test(info_string, models):\n",
    "    batch_size = 24\n",
    "    num_fold = 0\n",
    "    yfull_test = []\n",
    "    test_id = []\n",
    "    nfolds = len(models)\n",
    "    test_data, test_id = read_and_normalize_test_data()\n",
    "\n",
    "    for i in range(nfolds):\n",
    "        model = models[i]\n",
    "        num_fold += 1\n",
    "        print('Start KFold number {} from {}'.format(num_fold, nfolds))\n",
    "        test_prediction = model.predict(test_data,\n",
    "                                        batch_size=batch_size, verbose=2)\n",
    "        yfull_test.append(test_prediction)\n",
    "\n",
    "    test_res = merge_several_folds_mean(yfull_test, nfolds)\n",
    "    info_string = 'loss_' + info_string + '_folds_' + str(nfolds)\n",
    "    create_submission(test_res, test_id, info_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read train images\n",
      "Load folder ALB (Index: 0)\n",
      "\n",
      "Load folder BET (Index: 1)\n",
      "\n",
      "Load folder DOL (Index: 2)\n",
      "\n",
      "Load folder LAG (Index: 3)\n",
      "\n",
      "Load folder NoF (Index: 4)\n",
      "\n",
      "Load folder OTHER (Index: 5)\n",
      "\n",
      "Load folder SHARK (Index: 6)\n",
      "\n",
      "Load folder YFT (Index: 7)\n",
      "\n",
      "Read train data time: 103.3 seconds\n",
      "Convert to numpy...\n",
      "Reshape...\n",
      "Convert to float...\n",
      "Train shape: (3777, 64, 64, 3)\n",
      "3777 train samples\n"
     ]
    }
   ],
   "source": [
    "num_folds = 3\n",
    "train_data, train_target, train_id = read_and_normalize_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start KFold number 1 from 3\n",
      "Split train:  2518 2518\n",
      "Split valid:  1259 1259\n",
      "Train on 2518 samples, validate on 1259 samples\n",
      "Epoch 1/8\n",
      "2s - loss: 1.6843 - val_loss: 1.5718\n",
      "Epoch 2/8\n",
      "0s - loss: 1.5357 - val_loss: 1.4274\n",
      "Epoch 3/8\n",
      "0s - loss: 1.3770 - val_loss: 1.2500\n",
      "Epoch 4/8\n",
      "0s - loss: 1.2660 - val_loss: 1.2107\n",
      "Epoch 5/8\n",
      "0s - loss: 1.1478 - val_loss: 1.0646\n",
      "Epoch 6/8\n",
      "0s - loss: 1.0084 - val_loss: 0.8901\n",
      "Epoch 7/8\n",
      "0s - loss: 0.8832 - val_loss: 0.7676\n",
      "Epoch 8/8\n",
      "0s - loss: 0.7912 - val_loss: 0.6779\n",
      "Score log_loss:  0.677940759999\n",
      "Start KFold number 2 from 3\n",
      "Split train:  2518 2518\n",
      "Split valid:  1259 1259\n",
      "Train on 2518 samples, validate on 1259 samples\n",
      "Epoch 1/8\n",
      "1s - loss: 1.7077 - val_loss: 1.5981\n",
      "Epoch 2/8\n",
      "0s - loss: 1.5533 - val_loss: 1.4235\n",
      "Epoch 3/8\n",
      "0s - loss: 1.4160 - val_loss: 1.2790\n",
      "Epoch 4/8\n",
      "0s - loss: 1.2519 - val_loss: 1.1230\n",
      "Epoch 5/8\n",
      "0s - loss: 1.1213 - val_loss: 1.0233\n",
      "Epoch 6/8\n",
      "0s - loss: 0.9412 - val_loss: 0.8254\n",
      "Epoch 7/8\n",
      "0s - loss: 0.8179 - val_loss: 0.6892\n",
      "Epoch 8/8\n",
      "0s - loss: 0.7227 - val_loss: 0.6049\n",
      "Score log_loss:  0.604898461975\n",
      "Start KFold number 3 from 3\n",
      "Split train:  2518 2518\n",
      "Split valid:  1259 1259\n",
      "Train on 2518 samples, validate on 1259 samples\n",
      "Epoch 1/8\n",
      "1s - loss: 1.7130 - val_loss: 1.5956\n",
      "Epoch 2/8\n",
      "0s - loss: 1.5726 - val_loss: 1.4433\n",
      "Epoch 3/8\n",
      "0s - loss: 1.4649 - val_loss: 1.3131\n",
      "Epoch 4/8\n",
      "0s - loss: 1.2905 - val_loss: 1.1495\n",
      "Epoch 5/8\n",
      "0s - loss: 1.1390 - val_loss: 0.9849\n",
      "Epoch 6/8\n",
      "0s - loss: 0.9944 - val_loss: 0.7530\n",
      "Epoch 7/8\n",
      "0s - loss: 0.8699 - val_loss: 0.6496\n",
      "Epoch 8/8\n",
      "0s - loss: 0.7521 - val_loss: 0.6257\n",
      "Score log_loss:  0.625709298895\n",
      "Log_loss train independent avg:  0.63618284029\n"
     ]
    }
   ],
   "source": [
    "info_string, models = run_cross_validation_create_models(\n",
    "    train_data, train_target, train_id, num_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test shape: (1000, 64, 64, 3)\n",
      "1000 test samples\n",
      "Read and process test data time: 27.42 seconds\n",
      "Start KFold number 1 from 3\n",
      "Start KFold number 2 from 3\n",
      "Start KFold number 3 from 3\n"
     ]
    }
   ],
   "source": [
    "run_cross_validation_process_test(info_string, models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pydata]",
   "language": "python",
   "name": "conda-env-pydata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "0087d1c379034289869f51d8159750cf": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "3bf274e709344676bd9b10573b121cb7": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "61b825caf00d48e093befc3e944cc849": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "6a0cca2b6b9a4c0ba3448e04b532c382": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "81d9ea5fb3da4fb5bc89717a4552080a": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "84cf789659b64799ade0604b09461e27": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "8e284fb8b428410ca859ed28f4f3d3ec": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "91ef17b1deb44d72a64444d314ad14ed": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "92442421d8fb486b8997a8be2fb9420c": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "9c70ff41d06a44509aaeb61fbedf2e5f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "c6d6c1ba6a7e44048a461e00617f946a": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "c89e8f4595dc4e93a9a1f1987f242485": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "d0137e2dbda14b028c0f21cf81921930": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "e7f277b287904721b9be0e785df0b62d": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "eea7d68475d04afa822053d90c2f47ab": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "efc5166a189c4f688c89b68cd562d35f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
