{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Nature Conservancy Fish Classification - Conv Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Imports & environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import default libraries\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import time\n",
    "import argparse\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 1060 6GB (CNMeM is enabled with initial size: 80.0% of memory, cuDNN 5105)\n",
      "/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import GlobalAveragePooling2D, Activation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model\n",
    "\n",
    "from utils import * \n",
    "from vgg16bn import Vgg16BN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Config & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# global parameters\n",
    "args = {\n",
    "    \"pretrained\": True,\n",
    "    \"datadir\": \"../data\",\n",
    "    \"optim\": \"adam\", # sgd, adam, rmsprop\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 32,\n",
    "    \"lr\": 1e-3,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"seed\": 7,\n",
    "    \"nb_augs\": 10,\n",
    "    \"cv\": 5\n",
    "}\n",
    "args = argparse.Namespace(**args)\n",
    "\n",
    "n_filters = 158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# data folders\n",
    "traindir_full = os.path.join(args.datadir, \"train\")\n",
    "testdir = os.path.join(args.datadir, \"test_stg1\")\n",
    "# intermediate folder\n",
    "intermediate_path = os.path.join(\"..\", \"intermediate\")\n",
    "submission_path = os.path.join(intermediate_path, \"submissions\")\n",
    "if not os.path.isdir(submission_path):\n",
    "    os.makedirs(submission_path)\n",
    "# get classes\n",
    "classes = sorted([x.split(\"/\")[-1] for x in glob.glob(traindir_full+\"/*\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "testdir1 = os.path.join(intermediate_path, \"test_stg1\")\n",
    "testdir2 = os.path.join(testdir1, \"test\")\n",
    "if not os.path.isdir(testdir2):\n",
    "    shutil.copytree(testdir, testdir2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# split train/val cross validation\n",
    "traindir = []\n",
    "valdir = []\n",
    "\n",
    "g = glob.glob(traindir_full + \"/*/*.jpg\")\n",
    "gg = [\"/\"+x.split(\"/\")[-2]+\"/\"+x.split(\"/\")[-1] for x in g]\n",
    "np.random.seed(args.seed)\n",
    "shuf = np.random.permutation(gg)\n",
    "ticks = []\n",
    "for i in range(args.cv):\n",
    "    ticks.append(i * (len(gg)//args.cv))\n",
    "ticks.append(len(gg))\n",
    "\n",
    "for i in range(args.cv):\n",
    "    traindir.append(os.path.join(intermediate_path, \"train{}_{}\".format(\n",
    "        args.cv, str(i))))\n",
    "    valdir.append(os.path.join(intermediate_path, \"val{}_{}\".format(\n",
    "        args.cv, str(i))))\n",
    "    if not os.path.isdir(traindir[i]):\n",
    "        shutil.copytree(traindir_full, traindir[i])\n",
    "    if not os.path.isdir(valdir[i]):\n",
    "        vals = shuf[ticks[i]:ticks[i+1]]\n",
    "        for val in vals:\n",
    "            os.renames(traindir[i] + val, valdir[i] + val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_classes(trn_path, val_path):\n",
    "    batches = get_batches(trn_path, shuffle=False, batch_size=1)\n",
    "    val_batches = get_batches(val_path, shuffle=False, batch_size=1)\n",
    "    return (val_batches.classes, batches.classes, onehot(val_batches.classes),\n",
    "            onehot(batches.classes), val_batches.filenames, batches.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "conv_test_feat = load_array(intermediate_path + '/precomputed/test_ft_640.dat')\n",
    "test_batches = get_batches(testdir1, shuffle=False, batch_size=1)\n",
    "test_filenames = test_batches.filenames\n",
    "nb_test_samples = len(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_lrg_layers():\n",
    "    return [\n",
    "        BatchNormalization(axis=1,\n",
    "                           input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Convolution2D(n_filters, 3, 3, activation='relu',\n",
    "                      border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(n_filters, 3, 3, activation='relu',\n",
    "                      border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(n_filters, 3, 3, activation='relu',\n",
    "                      border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((1, 2)),\n",
    "        Convolution2D(8, 3, 3, border_mode='same'),\n",
    "        Dropout(0.5),\n",
    "        GlobalAveragePooling2D(),\n",
    "        Activation('softmax')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gen_preds(model):\n",
    "        if args.nb_augs:\n",
    "            gen = ImageDataGenerator(rotation_range=10, width_shift_range=0.05,\n",
    "                                     zoom_range=0.05, channel_shift_range=10,\n",
    "                                     height_shift_range=0.05, shear_range=0.05,\n",
    "                                     horizontal_flip=True)\n",
    "            predictions = np.zeros(shape=(nb_test_samples, len(classes)))\n",
    "\n",
    "            for aug in range(args.nb_augs):\n",
    "                predictions += model.predict(conv_test_feat,\n",
    "                                             batch_size=args.batch_size)\n",
    "\n",
    "            predictions /= args.nb_augs\n",
    "        else:\n",
    "            predictions = model.predict(conv_test_feat, batch_size=batch_size)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(os.path.join(intermediate_path, \"best\")):\n",
    "    os.makedirs(os.path.join(intermediate_path, \"best\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3022 images belonging to 8 classes.\n",
      "Found 755 images belonging to 8 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/site-packages/keras/layers/core.py:622: UserWarning: `output_shape` argument not specified for layer lambda_1 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 3, 360, 640)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3022 images belonging to 8 classes.\n",
      "Found 755 images belonging to 8 classes.\n",
      "Train on 3022 samples, validate on 755 samples\n",
      "Epoch 1/2\n",
      "10s - loss: 0.7406 - acc: 0.7667 - val_loss: 1.2640 - val_acc: 0.6967\n",
      "Epoch 2/2\n",
      "10s - loss: 0.2082 - acc: 0.9398 - val_loss: 0.2809 - val_acc: 0.9219\n",
      "Train on 3022 samples, validate on 755 samples\n",
      "Epoch 1/5\n",
      "10s - loss: 0.0889 - acc: 0.9735 - val_loss: 0.2297 - val_acc: 0.9351\n",
      "Epoch 2/5\n",
      "10s - loss: 0.0753 - acc: 0.9805 - val_loss: 0.2328 - val_acc: 0.9338\n",
      "Epoch 3/5\n",
      "10s - loss: 0.0524 - acc: 0.9821 - val_loss: 0.2760 - val_acc: 0.9311\n",
      "Epoch 4/5\n",
      "10s - loss: 0.0409 - acc: 0.9894 - val_loss: 0.4914 - val_acc: 0.8556\n",
      "Epoch 5/5\n",
      "10s - loss: 0.0515 - acc: 0.9854 - val_loss: 0.3012 - val_acc: 0.9510\n",
      "Train on 3022 samples, validate on 755 samples\n",
      "Epoch 1/5\n",
      "10s - loss: 0.0344 - acc: 0.9887 - val_loss: 0.2941 - val_acc: 0.9377\n",
      "Epoch 2/5\n",
      "10s - loss: 0.0461 - acc: 0.9884 - val_loss: 0.2563 - val_acc: 0.9603\n",
      "Epoch 3/5\n",
      "10s - loss: 0.0311 - acc: 0.9911 - val_loss: 0.3180 - val_acc: 0.9510\n",
      "Epoch 4/5\n",
      "10s - loss: 0.0176 - acc: 0.9944 - val_loss: 0.2600 - val_acc: 0.9497\n",
      "Epoch 5/5\n",
      "10s - loss: 0.0094 - acc: 0.9974 - val_loss: 0.2690 - val_acc: 0.9603\n",
      "Train on 3022 samples, validate on 755 samples\n",
      "Epoch 1/5\n",
      "10s - loss: 0.0065 - acc: 0.9983 - val_loss: 0.2355 - val_acc: 0.9616\n",
      "Epoch 2/5\n",
      "10s - loss: 0.0087 - acc: 0.9974 - val_loss: 0.2804 - val_acc: 0.9563\n",
      "Epoch 3/5\n",
      "10s - loss: 0.0104 - acc: 0.9970 - val_loss: 0.2413 - val_acc: 0.9642\n",
      "Epoch 4/5\n",
      "10s - loss: 0.0051 - acc: 0.9990 - val_loss: 0.2439 - val_acc: 0.9656\n",
      "Epoch 5/5\n",
      "10s - loss: 0.0018 - acc: 1.0000 - val_loss: 0.2467 - val_acc: 0.9589\n",
      "Found 3022 images belonging to 8 classes.\n",
      "Found 755 images belonging to 8 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/site-packages/keras/layers/core.py:622: UserWarning: `output_shape` argument not specified for layer lambda_2 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 3, 360, 640)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3022 images belonging to 8 classes.\n",
      "Found 755 images belonging to 8 classes.\n",
      "Train on 3022 samples, validate on 755 samples\n",
      "Epoch 1/2\n",
      "10s - loss: 0.7784 - acc: 0.7528 - val_loss: 0.6795 - val_acc: 0.8053\n",
      "Epoch 2/2\n",
      "10s - loss: 0.2094 - acc: 0.9351 - val_loss: 0.7747 - val_acc: 0.7669\n",
      "Train on 3022 samples, validate on 755 samples\n",
      "Epoch 1/5\n",
      "10s - loss: 0.0906 - acc: 0.9735 - val_loss: 0.4309 - val_acc: 0.8848\n",
      "Epoch 2/5\n",
      "10s - loss: 0.0580 - acc: 0.9838 - val_loss: 0.3255 - val_acc: 0.9179\n",
      "Epoch 3/5\n",
      "10s - loss: 0.0656 - acc: 0.9798 - val_loss: 0.2891 - val_acc: 0.9404\n",
      "Epoch 4/5\n",
      "10s - loss: 0.0672 - acc: 0.9808 - val_loss: 0.3104 - val_acc: 0.9311\n",
      "Epoch 5/5\n",
      "10s - loss: 0.0441 - acc: 0.9868 - val_loss: 0.2870 - val_acc: 0.9404\n",
      "Train on 3022 samples, validate on 755 samples\n",
      "Epoch 1/5\n",
      "10s - loss: 0.0192 - acc: 0.9960 - val_loss: 0.2116 - val_acc: 0.9510\n",
      "Epoch 2/5\n",
      "10s - loss: 0.0042 - acc: 0.9993 - val_loss: 0.2299 - val_acc: 0.9536\n",
      "Epoch 3/5\n",
      "10s - loss: 0.0035 - acc: 0.9993 - val_loss: 0.2730 - val_acc: 0.9497\n",
      "Epoch 4/5\n",
      "10s - loss: 0.0073 - acc: 0.9983 - val_loss: 0.2061 - val_acc: 0.9589\n",
      "Epoch 5/5\n",
      "10s - loss: 0.0283 - acc: 0.9917 - val_loss: 0.5532 - val_acc: 0.9046\n",
      "Train on 3022 samples, validate on 755 samples\n",
      "Epoch 1/5\n",
      "10s - loss: 0.1173 - acc: 0.9672 - val_loss: 0.6236 - val_acc: 0.8781\n",
      "Epoch 2/5\n",
      "10s - loss: 0.0809 - acc: 0.9805 - val_loss: 0.4437 - val_acc: 0.9272\n",
      "Epoch 3/5\n",
      "10s - loss: 0.0334 - acc: 0.9891 - val_loss: 0.3693 - val_acc: 0.9338\n",
      "Epoch 4/5\n",
      "10s - loss: 0.0249 - acc: 0.9924 - val_loss: 0.2655 - val_acc: 0.9550\n",
      "Epoch 5/5\n",
      "10s - loss: 0.0075 - acc: 0.9977 - val_loss: 0.2685 - val_acc: 0.9563\n",
      "Found 3022 images belonging to 8 classes.\n",
      "Found 755 images belonging to 8 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/site-packages/keras/layers/core.py:622: UserWarning: `output_shape` argument not specified for layer lambda_3 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 3, 360, 640)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3022 images belonging to 8 classes.\n",
      "Found 755 images belonging to 8 classes.\n",
      "Train on 3022 samples, validate on 755 samples\n",
      "Epoch 1/2\n",
      "10s - loss: 0.8088 - acc: 0.7465 - val_loss: 1.3794 - val_acc: 0.6093\n",
      "Epoch 2/2\n",
      "10s - loss: 0.2243 - acc: 0.9381 - val_loss: 0.4533 - val_acc: 0.8781\n",
      "Train on 3022 samples, validate on 755 samples\n",
      "Epoch 1/5\n",
      "10s - loss: 0.1085 - acc: 0.9699 - val_loss: 0.2628 - val_acc: 0.9166\n",
      "Epoch 2/5\n",
      "10s - loss: 0.0902 - acc: 0.9752 - val_loss: 0.3052 - val_acc: 0.9325\n",
      "Epoch 3/5\n",
      "10s - loss: 0.0560 - acc: 0.9848 - val_loss: 0.3489 - val_acc: 0.9192\n",
      "Epoch 4/5\n",
      "10s - loss: 0.0493 - acc: 0.9881 - val_loss: 0.1986 - val_acc: 0.9616\n",
      "Epoch 5/5\n",
      "10s - loss: 0.0232 - acc: 0.9924 - val_loss: 0.2807 - val_acc: 0.9483\n",
      "Train on 3022 samples, validate on 755 samples\n",
      "Epoch 1/5\n",
      "10s - loss: 0.0435 - acc: 0.9901 - val_loss: 0.2733 - val_acc: 0.9298\n",
      "Epoch 2/5\n",
      "10s - loss: 0.0321 - acc: 0.9921 - val_loss: 0.3703 - val_acc: 0.9417\n",
      "Epoch 3/5\n",
      "10s - loss: 0.0378 - acc: 0.9881 - val_loss: 0.3861 - val_acc: 0.9285\n",
      "Epoch 4/5\n",
      "10s - loss: 0.0271 - acc: 0.9911 - val_loss: 0.2308 - val_acc: 0.9483\n",
      "Epoch 5/5\n",
      "10s - loss: 0.0176 - acc: 0.9934 - val_loss: 0.2458 - val_acc: 0.9377\n",
      "Train on 3022 samples, validate on 755 samples\n",
      "Epoch 1/5\n",
      "10s - loss: 0.0517 - acc: 0.9848 - val_loss: 0.2423 - val_acc: 0.9497\n",
      "Epoch 2/5\n",
      "10s - loss: 0.0141 - acc: 0.9960 - val_loss: 0.2102 - val_acc: 0.9616\n",
      "Epoch 3/5\n",
      "10s - loss: 0.0055 - acc: 0.9983 - val_loss: 0.2318 - val_acc: 0.9589\n",
      "Epoch 4/5\n",
      "10s - loss: 0.0048 - acc: 0.9993 - val_loss: 0.1969 - val_acc: 0.9576\n",
      "Epoch 5/5\n",
      "10s - loss: 0.0298 - acc: 0.9957 - val_loss: 0.2380 - val_acc: 0.9457\n",
      "Found 3020 images belonging to 8 classes.\n",
      "Found 757 images belonging to 8 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fehiepsi/miniconda3/envs/pydata/lib/python3.5/site-packages/keras/layers/core.py:622: UserWarning: `output_shape` argument not specified for layer lambda_4 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 3, 360, 640)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3020 images belonging to 8 classes.\n",
      "Found 757 images belonging to 8 classes.\n",
      "Train on 3020 samples, validate on 757 samples\n",
      "Epoch 1/2\n",
      "10s - loss: 0.7713 - acc: 0.7490 - val_loss: 0.6032 - val_acc: 0.7886\n",
      "Epoch 2/2\n",
      "10s - loss: 0.1989 - acc: 0.9454 - val_loss: 0.2104 - val_acc: 0.9564\n",
      "Train on 3020 samples, validate on 757 samples\n",
      "Epoch 1/5\n",
      "10s - loss: 0.0929 - acc: 0.9735 - val_loss: 0.1771 - val_acc: 0.9524\n",
      "Epoch 2/5\n",
      "10s - loss: 0.0626 - acc: 0.9825 - val_loss: 0.2097 - val_acc: 0.9353\n",
      "Epoch 3/5\n",
      "10s - loss: 0.0466 - acc: 0.9861 - val_loss: 0.1777 - val_acc: 0.9577\n",
      "Epoch 4/5\n",
      "10s - loss: 0.0355 - acc: 0.9911 - val_loss: 0.2583 - val_acc: 0.9445\n",
      "Epoch 5/5\n",
      "10s - loss: 0.0256 - acc: 0.9927 - val_loss: 0.2353 - val_acc: 0.9326\n",
      "Train on 3020 samples, validate on 757 samples\n",
      "Epoch 1/5\n",
      "10s - loss: 0.0545 - acc: 0.9871 - val_loss: 0.3236 - val_acc: 0.9036\n",
      "Epoch 2/5\n",
      "10s - loss: 0.0720 - acc: 0.9805 - val_loss: 0.2655 - val_acc: 0.9379\n",
      "Epoch 3/5\n",
      "10s - loss: 0.0485 - acc: 0.9861 - val_loss: 0.2422 - val_acc: 0.9339\n",
      "Epoch 4/5\n",
      "10s - loss: 0.0338 - acc: 0.9907 - val_loss: 0.2273 - val_acc: 0.9458\n",
      "Epoch 5/5\n",
      "10s - loss: 0.0253 - acc: 0.9957 - val_loss: 0.1729 - val_acc: 0.9657\n",
      "Train on 3020 samples, validate on 757 samples\n",
      "Epoch 1/5\n",
      "10s - loss: 0.0071 - acc: 0.9977 - val_loss: 0.1232 - val_acc: 0.9709\n",
      "Epoch 2/5\n",
      "10s - loss: 0.0221 - acc: 0.9930 - val_loss: 0.1652 - val_acc: 0.9657\n",
      "Epoch 3/5\n",
      "10s - loss: 0.0336 - acc: 0.9907 - val_loss: 0.2074 - val_acc: 0.9577\n",
      "Epoch 4/5\n",
      "10s - loss: 0.0215 - acc: 0.9940 - val_loss: 0.1915 - val_acc: 0.9590\n",
      "Epoch 5/5\n",
      "10s - loss: 0.0339 - acc: 0.9891 - val_loss: 0.2910 - val_acc: 0.9392\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, args.cv):\n",
    "    (val_classes, trn_classes, val_labels, trn_labels,\n",
    "     val_filenames, filenames) = get_classes(traindir[i], valdir[i])\n",
    "\n",
    "    vgg640 = Vgg16BN((360, 640)).model\n",
    "    vgg640.pop()\n",
    "    vgg640.compile(Adam(), 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    train_fn = (intermediate_path\n",
    "                + '/precomputed/xtrn_ft_cv{}{}_640.dat'.format(args.cv, i))\n",
    "    val_fn = (intermediate_path\n",
    "              + '/precomputed/xval_ft_cv{}{}_640.dat'.format(args.cv, i))\n",
    "    if not (os.path.exists(train_fn) and os.path.exists(val_fn)):\n",
    "        nb_split_train_samples = len(glob.glob(traindir[i] + \"/*/*.jpg\"))\n",
    "        batches = get_batches(traindir[i], batch_size=1,\n",
    "                              target_size=(360, 640), shuffle=False,\n",
    "                              class_mode=None)\n",
    "        conv_trn_feat = vgg640.predict_generator(batches,\n",
    "                                                 nb_split_train_samples)\n",
    "        save_array(train_fn, conv_trn_feat)\n",
    "\n",
    "        nb_valid_samples = len(glob.glob(valdir[i] + \"/*/*.jpg\"))\n",
    "        val_batches = get_batches(valdir[i], batch_size=1,\n",
    "                                  target_size=(360, 640), shuffle=False,\n",
    "                                  class_mode=None)\n",
    "        conv_val_feat = vgg640.predict_generator(val_batches, nb_valid_samples)\n",
    "        save_array(val_fn, conv_val_feat)\n",
    "    else:\n",
    "        conv_trn_feat = load_array(train_fn)\n",
    "        conv_val_feat = load_array(val_fn)\n",
    "\n",
    "    conv_layers, _ = split_at(vgg640, Convolution2D)\n",
    "    lrg_model = Sequential(get_lrg_layers())\n",
    "\n",
    "    lrg_model.compile(Adam(lr=args.lr), loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    model_fn = (intermediate_path\n",
    "                + '/best/cv{}_{}_640x360_vgg16bn.h5'\n",
    "                  .format(args.cv, i))\n",
    "    ckpt = ModelCheckpoint(filepath=model_fn, monitor='val_loss',\n",
    "                           save_best_only=True, save_weights_only=True)\n",
    "\n",
    "    for j in range(4):\n",
    "        nb_epoch = 2\n",
    "        if j != 0:\n",
    "            lrg_model.optimizer.lr /= 10\n",
    "            nb_epoch = 5\n",
    "        lrg_model.fit(conv_trn_feat, trn_labels, batch_size=args.batch_size,\n",
    "                      nb_epoch=nb_epoch, verbose=2,\n",
    "                      validation_data=(conv_val_feat, val_labels),\n",
    "                      callbacks=[ckpt])\n",
    "\n",
    "    lrg_model.load_weights(model_fn)\n",
    "    preds.append(gen_preds(lrg_model))\n",
    "    del lrg_model\n",
    "    del conv_trn_feat\n",
    "    del conv_val_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(\"../intermediate/best/predictions_vgg16bn.pkl\", \"wb\") as f:\n",
    "    pickle.dump(preds, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def write_submission(predictions, filenames, clip=0):\n",
    "    preds = [np.clip(pred, (1-clip)/7, clip) for pred in predictions]\n",
    "    preds = sum(preds)/len(preds)\n",
    "    sub_fn = submission_path + '/cv{}_{}augs_clip{}_vgg16bn'.format(\n",
    "        args.cv, args.nb_augs, clip)\n",
    "\n",
    "    with open(sub_fn + '.csv', 'w') as f:\n",
    "        print(\"Writing Predictions to CSV...\")\n",
    "        f.write('image,ALB,BET,DOL,LAG,NoF,OTHER,SHARK,YFT\\n')\n",
    "        for i, image_name in enumerate(filenames):\n",
    "            pred = ['%.6f' % p for p in preds[i, :]]\n",
    "            f.write('%s,%s\\n' % (os.path.basename(image_name), ','.join(pred)))\n",
    "        print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Predictions to CSV...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "write_submission(preds, test_filenames, clip=0.82)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:pydata]",
   "language": "python",
   "name": "conda-env-pydata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
