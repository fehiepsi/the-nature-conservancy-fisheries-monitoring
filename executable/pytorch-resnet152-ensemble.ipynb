{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fish classification using ResNet152"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import default libraries\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import time\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import PyTorch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torchnet.meter import AverageValueMeter, ClassErrorMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# global parameters\n",
    "args = {\n",
    "    \"arch\": \"resnet152\", # resnet50, resnet101, resnet152\n",
    "    \"pretrained\": True,\n",
    "    \"datadir\": \"../data\",\n",
    "    \"cuda\": True,\n",
    "    \"optim\": \"adam\", # sgd, adam, rmsprop\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 16,\n",
    "    \"lr\": 1e-3,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"seed\": 7,\n",
    "    \"workers\": 4,\n",
    "    \"nb_augs\": 10,\n",
    "    \"cv\": 8\n",
    "}\n",
    "args = argparse.Namespace(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args.cuda = args.cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data folders\n",
    "traindir_full = os.path.join(args.datadir, \"train\")\n",
    "testdir = os.path.join(args.datadir, \"test_stg1\")\n",
    "# intermediate folder\n",
    "intermediate_path = os.path.join(\"..\", \"intermediate\")\n",
    "submission_path = os.path.join(intermediate_path, \"submissions\")\n",
    "if not os.path.isdir(submission_path):\n",
    "    os.makedirs(submission_path)\n",
    "# get classes\n",
    "classes = sorted([x.split(\"/\")[-1] for x in glob.glob(traindir_full+\"/*\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split train/val cross validation\n",
    "traindir = []\n",
    "valdir = []\n",
    "\n",
    "traindir_tmp = os.path.join(intermediate_path, \"train_tmp\")\n",
    "g = glob.glob(traindir_full + \"/*/*.jpg\")\n",
    "gg = [\"/\"+x.split(\"/\")[-2]+\"/\"+x.split(\"/\")[-1] for x in g]\n",
    "np.random.seed(args.seed)\n",
    "shuf = np.random.permutation(gg)\n",
    "ticks = []\n",
    "for i in range(args.cv):\n",
    "    ticks.append(i * (len(gg)//args.cv))\n",
    "ticks.append(len(gg))\n",
    "\n",
    "for i in range(args.cv):\n",
    "    traindir.append(os.path.join(intermediate_path, \"train{}_{}\".format(\n",
    "        args.cv, str(i))))\n",
    "    valdir.append(os.path.join(intermediate_path, \"val{}_{}\".format(\n",
    "        args.cv, str(i))))\n",
    "    if not os.path.isdir(traindir[i]):\n",
    "        shutil.copytree(traindir_full, traindir[i])\n",
    "    if not os.path.isdir(valdir[i]):\n",
    "        vals = shuf[ticks[i]:ticks[i+1]]\n",
    "        for val in vals:\n",
    "            os.renames(traindir[i] + val, valdir[i] + val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, modelbest_filepath,\n",
    "                    filename=\"checkpoint.pth.tar\"):\n",
    "    checkpoint_filepath = os.path.join(intermediate_path, filename)\n",
    "    torch.save(state, checkpoint_filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(checkpoint_filepath, modelbest_filepath)\n",
    "\n",
    "def adjust_learning_rate(args, optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\n",
    "    \"\"\"\n",
    "    lr = args.lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train/validate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train function\n",
    "def train(args, train_loader, model, criterion, optimizer, epoch):\n",
    "    # turn on train mode\n",
    "    model.train()\n",
    "    losses = AverageValueMeter()\n",
    "    top1 = ClassErrorMeter(accuracy=True) # accuracy instead of error\n",
    "    \n",
    "    for i, (input, target) in enumerate(train_loader):      \n",
    "        # here we should call cuda() for input;\n",
    "        # in the ImageNet example, the model is parallel by\n",
    "        # torch.nn.DataParallel(model).cuda(), so no need to call cuda() there;\n",
    "        # the option async=True works with pin_memory of DataLoader\n",
    "        # pin_memory slows down DataLoader but fastens data transfer from\n",
    "        # CPU to GPU\n",
    "        if args.cuda:\n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "        input_var = Variable(input)\n",
    "        target_var = Variable(target)\n",
    "\n",
    "        # compute output and loss\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # compute gradient and do backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.add(loss.data[0] * input.size(0), input.size(0))\n",
    "        top1.add(output.data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# validate function\n",
    "def validate(args, val_loader, model, criterion, epoch):\n",
    "    model.train(False) # turn off train mode\n",
    "    losses = AverageValueMeter()\n",
    "    top1 = ClassErrorMeter(accuracy=True)\n",
    "    \n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        if args.cuda:\n",
    "            input = input.cuda(async=True)\n",
    "            target = target.cuda(async=True)\n",
    "        input_var = Variable(input, volatile=True) # no gradient\n",
    "        target_var = Variable(target, volatile=True)\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "        losses.add(loss.data[0] * input.size(0), input.size(0))\n",
    "        top1.add(output.data, target)\n",
    "        \n",
    "    print(\"   * EPOCH {:>2} | Accuracy: {:.3f} | Loss: {:.4f}\"\n",
    "          .format(epoch, top1.value()[0], losses.value()[0]))\n",
    "    return losses.value()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_best(args, trainpath, valpath, modelbest_filename):\n",
    "    # create model\n",
    "    if args.pretrained:\n",
    "        print(\"=> Using pre-trained model '{}'\".format(args.arch))\n",
    "        model = models.__dict__[args.arch](pretrained=True)\n",
    "    else:\n",
    "        print(\"=> Creating model '{}'\".format(args.arch))\n",
    "        model = models.__dict__[args.arch]()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    # parameters of newly constructed modules have requires_grad=True by default\n",
    "    # replace the last fully-connected layer\n",
    "    model.fc = nn.Linear(2048, len(classes))\n",
    "    # for 1 GPU, it is unnecessary to use DataParallel\n",
    "    #model = torch.nn.DataParallel(model).cuda()\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    # define loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if args.cuda:\n",
    "        criterion.cuda()\n",
    "\n",
    "    # define optimizer\n",
    "    if args.optim == \"sgd\":\n",
    "        optimizer = optim.SGD(model.fc.parameters(),\n",
    "                              lr=args.lr,\n",
    "                              momentum=args.momentum,\n",
    "                              weight_decay=args.weight_decay)\n",
    "    elif args.optim == \"adam\":\n",
    "        optimizer = optim.Adam(model.fc.parameters(),\n",
    "                               lr=args.lr,\n",
    "                               weight_decay=args.weight_decay)\n",
    "    elif args.optim == \"rmsprop\":\n",
    "        optimizer = optim.RMSprop(model.fc.parameters(),\n",
    "                                  lr=args.lr,\n",
    "                                  weight_decay=args.weight_decay)\n",
    "\n",
    "    # Data loading code\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        datasets.ImageFolder(trainpath,\n",
    "                             transforms.Compose([\n",
    "                                 transforms.Scale(400),\n",
    "                                 transforms.RandomSizedCrop(224),\n",
    "                                 transforms.RandomHorizontalFlip(),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 normalize])),\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.workers,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        datasets.ImageFolder(valpath,\n",
    "                             transforms.Compose([\n",
    "                                 transforms.Scale(400),\n",
    "                                 transforms.RandomSizedCrop(224),\n",
    "                                 transforms.RandomHorizontalFlip(),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 normalize])),\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.workers,\n",
    "    )\n",
    "    \n",
    "    # best model path\n",
    "    modelbest_filepath = os.path.join(intermediate_path, modelbest_filename)\n",
    "\n",
    "    print(\"=> Starting to train on '{}' model\".format(args.arch))\n",
    "    best_loss = 2\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        start = time.time()\n",
    "        adjust_learning_rate(args, optimizer, epoch)\n",
    "\n",
    "        # train for one epoch\n",
    "        train(args, train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        loss = validate(args, val_loader, model, criterion, epoch)\n",
    "\n",
    "        # remember best loss and save checkpoint\n",
    "        is_best = loss < best_loss\n",
    "        best_loss = min(loss, best_loss)\n",
    "        save_checkpoint({\n",
    "            \"epoch\": epoch,\n",
    "            \"arch\": args.arch,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"best_loss\": best_loss,\n",
    "        }, is_best, modelbest_filepath)\n",
    "        print(\"   => Time: {}s\".format(round(time.time()-start)))\n",
    "    return modelbest_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Using pre-trained model 'resnet152'\n",
      "=> Starting to train on 'resnet152' model\n",
      "   * EPOCH  1 | Accuracy: 61.864 | Loss: 1.0869\n",
      "   => Time: 61s\n",
      "   * EPOCH  2 | Accuracy: 59.534 | Loss: 1.1313\n",
      "   => Time: 43s\n",
      "   * EPOCH  3 | Accuracy: 62.076 | Loss: 1.0864\n",
      "   => Time: 44s\n",
      "   * EPOCH  4 | Accuracy: 64.619 | Loss: 1.0157\n",
      "   => Time: 45s\n",
      "   * EPOCH  5 | Accuracy: 67.585 | Loss: 1.0248\n",
      "   => Time: 42s\n",
      "   * EPOCH  6 | Accuracy: 61.441 | Loss: 1.0382\n",
      "   => Time: 43s\n",
      "   * EPOCH  7 | Accuracy: 66.949 | Loss: 0.9382\n",
      "   => Time: 44s\n",
      "   * EPOCH  8 | Accuracy: 69.068 | Loss: 0.8866\n",
      "   => Time: 44s\n",
      "   * EPOCH  9 | Accuracy: 71.398 | Loss: 0.8267\n",
      "   => Time: 44s\n",
      "   * EPOCH 10 | Accuracy: 62.712 | Loss: 1.0595\n",
      "   => Time: 42s\n",
      "   * EPOCH 11 | Accuracy: 65.254 | Loss: 0.9537\n",
      "   => Time: 42s\n",
      "   * EPOCH 12 | Accuracy: 70.975 | Loss: 0.8021\n",
      "   => Time: 45s\n",
      "   * EPOCH 13 | Accuracy: 70.127 | Loss: 0.9456\n",
      "   => Time: 42s\n",
      "   * EPOCH 14 | Accuracy: 67.373 | Loss: 0.9020\n",
      "   => Time: 43s\n",
      "   * EPOCH 15 | Accuracy: 65.466 | Loss: 0.9399\n",
      "   => Time: 42s\n",
      "   * EPOCH 16 | Accuracy: 68.220 | Loss: 0.8897\n",
      "   => Time: 42s\n",
      "   * EPOCH 17 | Accuracy: 68.644 | Loss: 0.8351\n",
      "   => Time: 42s\n",
      "   * EPOCH 18 | Accuracy: 68.008 | Loss: 0.8851\n",
      "   => Time: 42s\n",
      "   * EPOCH 19 | Accuracy: 71.186 | Loss: 0.8055\n",
      "   => Time: 43s\n",
      "   * EPOCH 20 | Accuracy: 70.763 | Loss: 0.8212\n",
      "   => Time: 43s\n",
      "   * EPOCH 21 | Accuracy: 69.280 | Loss: 0.8705\n",
      "   => Time: 43s\n",
      "   * EPOCH 22 | Accuracy: 68.220 | Loss: 0.8712\n",
      "   => Time: 43s\n",
      "   * EPOCH 23 | Accuracy: 72.034 | Loss: 0.8295\n",
      "   => Time: 42s\n",
      "   * EPOCH 24 | Accuracy: 70.127 | Loss: 0.7954\n",
      "   => Time: 46s\n",
      "   * EPOCH 25 | Accuracy: 69.703 | Loss: 0.8304\n",
      "   => Time: 43s\n",
      "   * EPOCH 26 | Accuracy: 66.314 | Loss: 0.9623\n",
      "   => Time: 42s\n",
      "   * EPOCH 27 | Accuracy: 67.585 | Loss: 0.8921\n",
      "   => Time: 43s\n",
      "   * EPOCH 28 | Accuracy: 73.517 | Loss: 0.7899\n",
      "   => Time: 44s\n",
      "   * EPOCH 29 | Accuracy: 68.432 | Loss: 0.8492\n",
      "   => Time: 42s\n",
      "   * EPOCH 30 | Accuracy: 75.000 | Loss: 0.6930\n",
      "   => Time: 45s\n",
      "   * EPOCH 31 | Accuracy: 75.424 | Loss: 0.7133\n",
      "   => Time: 43s\n",
      "   * EPOCH 32 | Accuracy: 73.517 | Loss: 0.7321\n",
      "   => Time: 42s\n",
      "   * EPOCH 33 | Accuracy: 71.822 | Loss: 0.7387\n",
      "   => Time: 42s\n",
      "   * EPOCH 34 | Accuracy: 71.398 | Loss: 0.7720\n",
      "   => Time: 42s\n",
      "   * EPOCH 35 | Accuracy: 73.941 | Loss: 0.7415\n",
      "   => Time: 42s\n",
      "   * EPOCH 36 | Accuracy: 72.669 | Loss: 0.7614\n",
      "   => Time: 42s\n",
      "   * EPOCH 37 | Accuracy: 72.881 | Loss: 0.7208\n",
      "   => Time: 42s\n",
      "   * EPOCH 38 | Accuracy: 72.246 | Loss: 0.7782\n",
      "   => Time: 42s\n",
      "   * EPOCH 39 | Accuracy: 72.669 | Loss: 0.7734\n",
      "   => Time: 42s\n",
      "   * EPOCH 40 | Accuracy: 70.975 | Loss: 0.7638\n",
      "   => Time: 42s\n",
      "   * EPOCH 41 | Accuracy: 70.127 | Loss: 0.8075\n",
      "   => Time: 42s\n",
      "   * EPOCH 42 | Accuracy: 72.669 | Loss: 0.7568\n",
      "   => Time: 42s\n",
      "   * EPOCH 43 | Accuracy: 75.636 | Loss: 0.7150\n",
      "   => Time: 42s\n",
      "   * EPOCH 44 | Accuracy: 73.093 | Loss: 0.7508\n",
      "   => Time: 42s\n",
      "   * EPOCH 45 | Accuracy: 71.822 | Loss: 0.7899\n",
      "   => Time: 42s\n",
      "   * EPOCH 46 | Accuracy: 71.610 | Loss: 0.7731\n",
      "   => Time: 42s\n",
      "   * EPOCH 47 | Accuracy: 73.093 | Loss: 0.7676\n",
      "   => Time: 42s\n",
      "   * EPOCH 48 | Accuracy: 74.364 | Loss: 0.6871\n",
      "   => Time: 44s\n",
      "   * EPOCH 49 | Accuracy: 74.364 | Loss: 0.7543\n",
      "   => Time: 42s\n",
      "   * EPOCH 50 | Accuracy: 74.576 | Loss: 0.7505\n",
      "   => Time: 42s\n",
      "   * EPOCH 51 | Accuracy: 71.822 | Loss: 0.7584\n",
      "   => Time: 42s\n",
      "   * EPOCH 52 | Accuracy: 73.517 | Loss: 0.7391\n",
      "   => Time: 42s\n",
      "   * EPOCH 53 | Accuracy: 73.517 | Loss: 0.7091\n",
      "   => Time: 42s\n",
      "   * EPOCH 54 | Accuracy: 75.212 | Loss: 0.7056\n",
      "   => Time: 42s\n",
      "   * EPOCH 55 | Accuracy: 71.186 | Loss: 0.7770\n",
      "   => Time: 42s\n",
      "   * EPOCH 56 | Accuracy: 71.822 | Loss: 0.7734\n",
      "   => Time: 42s\n",
      "   * EPOCH 57 | Accuracy: 73.729 | Loss: 0.7726\n",
      "   => Time: 42s\n",
      "   * EPOCH 58 | Accuracy: 74.364 | Loss: 0.7095\n",
      "   => Time: 42s\n",
      "   * EPOCH 59 | Accuracy: 72.669 | Loss: 0.7505\n",
      "   => Time: 42s\n",
      "   * EPOCH 60 | Accuracy: 74.576 | Loss: 0.7220\n",
      "   => Time: 42s\n",
      "   * EPOCH 61 | Accuracy: 77.542 | Loss: 0.6815\n",
      "   => Time: 44s\n",
      "   * EPOCH 62 | Accuracy: 69.703 | Loss: 0.7810\n",
      "   => Time: 42s\n",
      "   * EPOCH 63 | Accuracy: 76.907 | Loss: 0.6652\n",
      "   => Time: 44s\n",
      "   * EPOCH 64 | Accuracy: 76.271 | Loss: 0.7091\n",
      "   => Time: 42s\n",
      "   * EPOCH 65 | Accuracy: 72.458 | Loss: 0.7677\n",
      "   => Time: 42s\n",
      "   * EPOCH 66 | Accuracy: 77.331 | Loss: 0.7381\n",
      "   => Time: 42s\n",
      "   * EPOCH 67 | Accuracy: 75.636 | Loss: 0.6935\n",
      "   => Time: 42s\n",
      "   * EPOCH 68 | Accuracy: 73.517 | Loss: 0.7204\n",
      "   => Time: 42s\n",
      "   * EPOCH 69 | Accuracy: 75.424 | Loss: 0.7214\n",
      "   => Time: 42s\n",
      "   * EPOCH 70 | Accuracy: 77.119 | Loss: 0.7406\n",
      "   => Time: 42s\n",
      "   * EPOCH 71 | Accuracy: 75.424 | Loss: 0.7155\n",
      "   => Time: 42s\n",
      "   * EPOCH 72 | Accuracy: 77.542 | Loss: 0.7019\n",
      "   => Time: 42s\n",
      "   * EPOCH 73 | Accuracy: 75.847 | Loss: 0.7917\n",
      "   => Time: 42s\n",
      "   * EPOCH 74 | Accuracy: 75.212 | Loss: 0.7087\n",
      "   => Time: 42s\n",
      "   * EPOCH 75 | Accuracy: 76.483 | Loss: 0.7274\n",
      "   => Time: 42s\n",
      "   * EPOCH 76 | Accuracy: 74.364 | Loss: 0.7300\n",
      "   => Time: 42s\n",
      "   * EPOCH 77 | Accuracy: 74.576 | Loss: 0.7176\n",
      "   => Time: 42s\n",
      "   * EPOCH 78 | Accuracy: 74.576 | Loss: 0.7755\n",
      "   => Time: 42s\n",
      "   * EPOCH 79 | Accuracy: 75.424 | Loss: 0.6999\n",
      "   => Time: 42s\n",
      "   * EPOCH 80 | Accuracy: 74.153 | Loss: 0.7576\n",
      "   => Time: 42s\n",
      "   * EPOCH 81 | Accuracy: 72.458 | Loss: 0.7443\n",
      "   => Time: 42s\n",
      "   * EPOCH 82 | Accuracy: 72.034 | Loss: 0.7661\n",
      "   => Time: 42s\n",
      "   * EPOCH 83 | Accuracy: 73.517 | Loss: 0.7417\n",
      "   => Time: 42s\n",
      "   * EPOCH 84 | Accuracy: 75.212 | Loss: 0.7633\n",
      "   => Time: 42s\n",
      "   * EPOCH 85 | Accuracy: 73.941 | Loss: 0.7805\n",
      "   => Time: 42s\n",
      "   * EPOCH 86 | Accuracy: 74.364 | Loss: 0.7015\n",
      "   => Time: 42s\n",
      "   * EPOCH 87 | Accuracy: 75.636 | Loss: 0.7165\n",
      "   => Time: 42s\n",
      "   * EPOCH 88 | Accuracy: 73.093 | Loss: 0.7611\n",
      "   => Time: 42s\n",
      "   * EPOCH 89 | Accuracy: 75.424 | Loss: 0.7069\n",
      "   => Time: 43s\n",
      "   * EPOCH 90 | Accuracy: 76.271 | Loss: 0.7089\n",
      "   => Time: 42s\n",
      "   * EPOCH 91 | Accuracy: 76.059 | Loss: 0.6791\n",
      "   => Time: 42s\n",
      "   * EPOCH 92 | Accuracy: 75.847 | Loss: 0.7292\n",
      "   => Time: 42s\n",
      "   * EPOCH 93 | Accuracy: 75.424 | Loss: 0.7802\n",
      "   => Time: 42s\n",
      "   * EPOCH 94 | Accuracy: 74.364 | Loss: 0.6906\n",
      "   => Time: 42s\n",
      "   * EPOCH 95 | Accuracy: 76.059 | Loss: 0.7190\n",
      "   => Time: 42s\n",
      "   * EPOCH 96 | Accuracy: 71.610 | Loss: 0.7419\n",
      "   => Time: 42s\n",
      "   * EPOCH 97 | Accuracy: 74.153 | Loss: 0.7297\n",
      "   => Time: 42s\n",
      "   * EPOCH 98 | Accuracy: 74.788 | Loss: 0.7182\n",
      "   => Time: 42s\n",
      "   * EPOCH 99 | Accuracy: 75.424 | Loss: 0.7007\n",
      "   => Time: 42s\n",
      "   * EPOCH 100 | Accuracy: 72.458 | Loss: 0.7591\n",
      "   => Time: 42s\n",
      "=> Using pre-trained model 'resnet152'\n",
      "=> Starting to train on 'resnet152' model\n",
      "   * EPOCH  1 | Accuracy: 56.568 | Loss: 1.2265\n",
      "   => Time: 52s\n",
      "   * EPOCH  2 | Accuracy: 61.864 | Loss: 1.0610\n",
      "   => Time: 45s\n",
      "   * EPOCH  3 | Accuracy: 63.136 | Loss: 0.9901\n",
      "   => Time: 45s\n",
      "   * EPOCH  4 | Accuracy: 64.831 | Loss: 1.0049\n",
      "   => Time: 42s\n",
      "   * EPOCH  5 | Accuracy: 65.678 | Loss: 0.9822\n",
      "   => Time: 44s\n",
      "   * EPOCH  6 | Accuracy: 68.220 | Loss: 0.9414\n",
      "   => Time: 44s\n",
      "   * EPOCH  7 | Accuracy: 68.220 | Loss: 0.9488\n",
      "   => Time: 42s\n",
      "   * EPOCH  8 | Accuracy: 71.822 | Loss: 0.8742\n",
      "   => Time: 44s\n",
      "   * EPOCH  9 | Accuracy: 62.076 | Loss: 0.9905\n",
      "   => Time: 42s\n",
      "   * EPOCH 10 | Accuracy: 68.432 | Loss: 0.8335\n",
      "   => Time: 44s\n",
      "   * EPOCH 11 | Accuracy: 70.551 | Loss: 0.8559\n",
      "   => Time: 42s\n",
      "   * EPOCH 12 | Accuracy: 72.034 | Loss: 0.8615\n",
      "   => Time: 42s\n",
      "   * EPOCH 13 | Accuracy: 71.186 | Loss: 0.8273\n",
      "   => Time: 45s\n",
      "   * EPOCH 14 | Accuracy: 68.856 | Loss: 0.8482\n",
      "   => Time: 42s\n",
      "   * EPOCH 15 | Accuracy: 66.314 | Loss: 0.9966\n",
      "   => Time: 42s\n",
      "   * EPOCH 16 | Accuracy: 70.127 | Loss: 0.8721\n",
      "   => Time: 42s\n",
      "   * EPOCH 17 | Accuracy: 68.856 | Loss: 0.8721\n",
      "   => Time: 42s\n",
      "   * EPOCH 18 | Accuracy: 70.551 | Loss: 0.7854\n",
      "   => Time: 45s\n",
      "   * EPOCH 19 | Accuracy: 70.127 | Loss: 0.7992\n",
      "   => Time: 42s\n",
      "   * EPOCH 20 | Accuracy: 73.517 | Loss: 0.7336\n",
      "   => Time: 45s\n",
      "   * EPOCH 21 | Accuracy: 70.339 | Loss: 0.7939\n",
      "   => Time: 42s\n",
      "   * EPOCH 22 | Accuracy: 68.432 | Loss: 0.8174\n",
      "   => Time: 42s\n",
      "   * EPOCH 23 | Accuracy: 68.008 | Loss: 0.8717\n",
      "   => Time: 42s\n",
      "   * EPOCH 24 | Accuracy: 70.339 | Loss: 0.8146\n",
      "   => Time: 42s\n",
      "   * EPOCH 25 | Accuracy: 70.339 | Loss: 0.8508\n",
      "   => Time: 42s\n",
      "   * EPOCH 26 | Accuracy: 70.975 | Loss: 0.7989\n",
      "   => Time: 42s\n",
      "   * EPOCH 27 | Accuracy: 74.364 | Loss: 0.7777\n",
      "   => Time: 42s\n",
      "   * EPOCH 28 | Accuracy: 73.729 | Loss: 0.7724\n",
      "   => Time: 44s\n",
      "   * EPOCH 29 | Accuracy: 75.212 | Loss: 0.7500\n",
      "   => Time: 42s\n",
      "   * EPOCH 30 | Accuracy: 75.212 | Loss: 0.7456\n",
      "   => Time: 42s\n",
      "   * EPOCH 31 | Accuracy: 75.000 | Loss: 0.7129\n",
      "   => Time: 45s\n",
      "   * EPOCH 32 | Accuracy: 73.305 | Loss: 0.7393\n",
      "   => Time: 42s\n",
      "   * EPOCH 33 | Accuracy: 74.788 | Loss: 0.7525\n",
      "   => Time: 42s\n",
      "   * EPOCH 34 | Accuracy: 73.093 | Loss: 0.7645\n",
      "   => Time: 43s\n",
      "   * EPOCH 35 | Accuracy: 76.907 | Loss: 0.7732\n",
      "   => Time: 42s\n",
      "   * EPOCH 36 | Accuracy: 74.576 | Loss: 0.7405\n",
      "   => Time: 42s\n",
      "   * EPOCH 37 | Accuracy: 76.695 | Loss: 0.7196\n",
      "   => Time: 42s\n",
      "   * EPOCH 38 | Accuracy: 74.153 | Loss: 0.7853\n",
      "   => Time: 42s\n",
      "   * EPOCH 39 | Accuracy: 72.246 | Loss: 0.7790\n",
      "   => Time: 42s\n",
      "   * EPOCH 40 | Accuracy: 75.424 | Loss: 0.7252\n",
      "   => Time: 42s\n",
      "   * EPOCH 41 | Accuracy: 73.941 | Loss: 0.7700\n",
      "   => Time: 42s\n",
      "   * EPOCH 42 | Accuracy: 76.059 | Loss: 0.7104\n",
      "   => Time: 45s\n",
      "   * EPOCH 43 | Accuracy: 73.517 | Loss: 0.7692\n",
      "   => Time: 42s\n",
      "   * EPOCH 44 | Accuracy: 75.424 | Loss: 0.7185\n",
      "   => Time: 42s\n",
      "   * EPOCH 45 | Accuracy: 75.212 | Loss: 0.7567\n",
      "   => Time: 42s\n",
      "   * EPOCH 46 | Accuracy: 75.847 | Loss: 0.7481\n",
      "   => Time: 43s\n",
      "   * EPOCH 47 | Accuracy: 75.000 | Loss: 0.7628\n",
      "   => Time: 42s\n",
      "   * EPOCH 48 | Accuracy: 73.729 | Loss: 0.7846\n",
      "   => Time: 42s\n",
      "   * EPOCH 49 | Accuracy: 74.576 | Loss: 0.7618\n",
      "   => Time: 42s\n",
      "   * EPOCH 50 | Accuracy: 75.212 | Loss: 0.7028\n",
      "   => Time: 44s\n",
      "   * EPOCH 51 | Accuracy: 73.941 | Loss: 0.6874\n",
      "   => Time: 45s\n",
      "   * EPOCH 52 | Accuracy: 75.847 | Loss: 0.7152\n",
      "   => Time: 42s\n",
      "   * EPOCH 53 | Accuracy: 74.788 | Loss: 0.7193\n",
      "   => Time: 42s\n",
      "   * EPOCH 54 | Accuracy: 78.814 | Loss: 0.7292\n",
      "   => Time: 42s\n",
      "   * EPOCH 55 | Accuracy: 75.000 | Loss: 0.7216\n",
      "   => Time: 42s\n",
      "   * EPOCH 56 | Accuracy: 74.153 | Loss: 0.7416\n",
      "   => Time: 42s\n",
      "   * EPOCH 57 | Accuracy: 74.153 | Loss: 0.7466\n",
      "   => Time: 43s\n",
      "   * EPOCH 58 | Accuracy: 73.941 | Loss: 0.7251\n",
      "   => Time: 42s\n",
      "   * EPOCH 59 | Accuracy: 75.847 | Loss: 0.7314\n",
      "   => Time: 43s\n",
      "   * EPOCH 60 | Accuracy: 75.000 | Loss: 0.7355\n",
      "   => Time: 44s\n",
      "   * EPOCH 61 | Accuracy: 76.059 | Loss: 0.7108\n",
      "   => Time: 42s\n",
      "   * EPOCH 62 | Accuracy: 76.907 | Loss: 0.6558\n",
      "   => Time: 45s\n",
      "   * EPOCH 63 | Accuracy: 77.331 | Loss: 0.7022\n",
      "   => Time: 42s\n",
      "   * EPOCH 64 | Accuracy: 74.153 | Loss: 0.7097\n",
      "   => Time: 42s\n",
      "   * EPOCH 65 | Accuracy: 78.178 | Loss: 0.6711\n",
      "   => Time: 42s\n",
      "   * EPOCH 66 | Accuracy: 77.966 | Loss: 0.7134\n",
      "   => Time: 42s\n",
      "   * EPOCH 67 | Accuracy: 76.059 | Loss: 0.7072\n",
      "   => Time: 42s\n",
      "   * EPOCH 68 | Accuracy: 76.695 | Loss: 0.6930\n",
      "   => Time: 42s\n",
      "   * EPOCH 69 | Accuracy: 77.331 | Loss: 0.6773\n",
      "   => Time: 42s\n",
      "   * EPOCH 70 | Accuracy: 73.941 | Loss: 0.7338\n",
      "   => Time: 42s\n",
      "   * EPOCH 71 | Accuracy: 76.059 | Loss: 0.6997\n",
      "   => Time: 42s\n",
      "   * EPOCH 72 | Accuracy: 76.907 | Loss: 0.6771\n",
      "   => Time: 43s\n",
      "   * EPOCH 73 | Accuracy: 72.881 | Loss: 0.7776\n",
      "   => Time: 42s\n",
      "   * EPOCH 74 | Accuracy: 73.941 | Loss: 0.7397\n",
      "   => Time: 42s\n",
      "   * EPOCH 75 | Accuracy: 76.271 | Loss: 0.6958\n",
      "   => Time: 42s\n",
      "   * EPOCH 76 | Accuracy: 74.576 | Loss: 0.7178\n",
      "   => Time: 41s\n",
      "   * EPOCH 77 | Accuracy: 74.788 | Loss: 0.7207\n",
      "   => Time: 42s\n",
      "   * EPOCH 78 | Accuracy: 74.788 | Loss: 0.7032\n",
      "   => Time: 42s\n",
      "   * EPOCH 79 | Accuracy: 74.364 | Loss: 0.7413\n",
      "   => Time: 42s\n",
      "   * EPOCH 80 | Accuracy: 72.669 | Loss: 0.7579\n",
      "   => Time: 42s\n",
      "   * EPOCH 81 | Accuracy: 75.636 | Loss: 0.7111\n",
      "   => Time: 42s\n",
      "   * EPOCH 82 | Accuracy: 78.390 | Loss: 0.6771\n",
      "   => Time: 42s\n",
      "   * EPOCH 83 | Accuracy: 76.059 | Loss: 0.7216\n",
      "   => Time: 42s\n",
      "   * EPOCH 84 | Accuracy: 75.000 | Loss: 0.7345\n",
      "   => Time: 42s\n",
      "   * EPOCH 85 | Accuracy: 77.966 | Loss: 0.6812\n",
      "   => Time: 42s\n",
      "   * EPOCH 86 | Accuracy: 73.941 | Loss: 0.7756\n",
      "   => Time: 42s\n",
      "   * EPOCH 87 | Accuracy: 73.517 | Loss: 0.7668\n",
      "   => Time: 42s\n",
      "   * EPOCH 88 | Accuracy: 74.788 | Loss: 0.7503\n",
      "   => Time: 42s\n",
      "   * EPOCH 89 | Accuracy: 74.153 | Loss: 0.7338\n",
      "   => Time: 42s\n",
      "   * EPOCH 90 | Accuracy: 74.576 | Loss: 0.7775\n",
      "   => Time: 42s\n",
      "   * EPOCH 91 | Accuracy: 76.907 | Loss: 0.6770\n",
      "   => Time: 42s\n",
      "   * EPOCH 92 | Accuracy: 76.695 | Loss: 0.7390\n",
      "   => Time: 42s\n",
      "   * EPOCH 93 | Accuracy: 73.729 | Loss: 0.7218\n",
      "   => Time: 43s\n",
      "   * EPOCH 94 | Accuracy: 75.847 | Loss: 0.7429\n",
      "   => Time: 42s\n",
      "   * EPOCH 95 | Accuracy: 78.602 | Loss: 0.6623\n",
      "   => Time: 42s\n",
      "   * EPOCH 96 | Accuracy: 73.941 | Loss: 0.7556\n",
      "   => Time: 42s\n",
      "   * EPOCH 97 | Accuracy: 74.576 | Loss: 0.7563\n",
      "   => Time: 42s\n",
      "   * EPOCH 98 | Accuracy: 77.542 | Loss: 0.6889\n",
      "   => Time: 42s\n",
      "   * EPOCH 99 | Accuracy: 77.542 | Loss: 0.7016\n",
      "   => Time: 42s\n",
      "   * EPOCH 100 | Accuracy: 78.390 | Loss: 0.7040\n",
      "   => Time: 42s\n",
      "=> Using pre-trained model 'resnet152'\n",
      "=> Starting to train on 'resnet152' model\n",
      "   * EPOCH  1 | Accuracy: 57.415 | Loss: 1.1597\n",
      "   => Time: 52s\n",
      "   * EPOCH  2 | Accuracy: 58.898 | Loss: 1.0767\n",
      "   => Time: 45s\n",
      "   * EPOCH  3 | Accuracy: 58.475 | Loss: 1.0919\n",
      "   => Time: 42s\n",
      "   * EPOCH  4 | Accuracy: 59.110 | Loss: 1.0198\n",
      "   => Time: 44s\n",
      "   * EPOCH  5 | Accuracy: 62.712 | Loss: 1.0660\n",
      "   => Time: 42s\n",
      "   * EPOCH  6 | Accuracy: 65.042 | Loss: 0.9562\n",
      "   => Time: 45s\n",
      "   * EPOCH  7 | Accuracy: 66.525 | Loss: 0.9762\n",
      "   => Time: 42s\n",
      "   * EPOCH  8 | Accuracy: 65.466 | Loss: 1.0213\n",
      "   => Time: 43s\n",
      "   * EPOCH  9 | Accuracy: 62.288 | Loss: 0.9324\n",
      "   => Time: 45s\n",
      "   * EPOCH 10 | Accuracy: 66.737 | Loss: 0.8863\n",
      "   => Time: 45s\n",
      "   * EPOCH 11 | Accuracy: 70.127 | Loss: 0.8827\n",
      "   => Time: 45s\n",
      "   * EPOCH 12 | Accuracy: 72.246 | Loss: 0.8576\n",
      "   => Time: 45s\n",
      "   * EPOCH 13 | Accuracy: 65.678 | Loss: 0.9117\n",
      "   => Time: 42s\n",
      "   * EPOCH 14 | Accuracy: 70.127 | Loss: 0.9054\n",
      "   => Time: 42s\n",
      "   * EPOCH 15 | Accuracy: 66.737 | Loss: 0.9408\n",
      "   => Time: 42s\n",
      "   * EPOCH 16 | Accuracy: 68.220 | Loss: 0.8816\n",
      "   => Time: 42s\n",
      "   * EPOCH 17 | Accuracy: 71.610 | Loss: 0.8109\n",
      "   => Time: 44s\n",
      "   * EPOCH 18 | Accuracy: 69.915 | Loss: 0.8233\n",
      "   => Time: 42s\n",
      "   * EPOCH 19 | Accuracy: 70.127 | Loss: 0.8315\n",
      "   => Time: 42s\n",
      "   * EPOCH 20 | Accuracy: 72.458 | Loss: 0.7677\n",
      "   => Time: 44s\n",
      "   * EPOCH 21 | Accuracy: 71.398 | Loss: 0.8186\n",
      "   => Time: 42s\n",
      "   * EPOCH 22 | Accuracy: 72.669 | Loss: 0.8209\n",
      "   => Time: 42s\n",
      "   * EPOCH 23 | Accuracy: 67.161 | Loss: 0.9251\n",
      "   => Time: 42s\n",
      "   * EPOCH 24 | Accuracy: 73.729 | Loss: 0.7797\n",
      "   => Time: 42s\n",
      "   * EPOCH 25 | Accuracy: 68.856 | Loss: 0.9135\n",
      "   => Time: 42s\n",
      "   * EPOCH 26 | Accuracy: 73.729 | Loss: 0.7401\n",
      "   => Time: 45s\n",
      "   * EPOCH 27 | Accuracy: 67.161 | Loss: 0.8834\n",
      "   => Time: 42s\n",
      "   * EPOCH 28 | Accuracy: 69.068 | Loss: 0.8990\n",
      "   => Time: 42s\n",
      "   * EPOCH 29 | Accuracy: 70.975 | Loss: 0.8780\n",
      "   => Time: 42s\n",
      "   * EPOCH 30 | Accuracy: 73.093 | Loss: 0.7522\n",
      "   => Time: 42s\n",
      "   * EPOCH 31 | Accuracy: 73.517 | Loss: 0.7133\n",
      "   => Time: 45s\n",
      "   * EPOCH 32 | Accuracy: 71.398 | Loss: 0.7276\n",
      "   => Time: 42s\n",
      "   * EPOCH 33 | Accuracy: 73.093 | Loss: 0.7552\n",
      "   => Time: 42s\n",
      "   * EPOCH 34 | Accuracy: 72.034 | Loss: 0.7657\n",
      "   => Time: 42s\n",
      "   * EPOCH 35 | Accuracy: 69.280 | Loss: 0.8014\n",
      "   => Time: 42s\n",
      "   * EPOCH 36 | Accuracy: 72.669 | Loss: 0.7272\n",
      "   => Time: 42s\n",
      "   * EPOCH 37 | Accuracy: 74.576 | Loss: 0.7378\n",
      "   => Time: 43s\n",
      "   * EPOCH 38 | Accuracy: 77.119 | Loss: 0.7312\n",
      "   => Time: 42s\n",
      "   * EPOCH 39 | Accuracy: 72.458 | Loss: 0.7480\n",
      "   => Time: 42s\n",
      "   * EPOCH 40 | Accuracy: 73.517 | Loss: 0.7324\n",
      "   => Time: 42s\n",
      "   * EPOCH 41 | Accuracy: 76.483 | Loss: 0.7164\n",
      "   => Time: 43s\n",
      "   * EPOCH 42 | Accuracy: 74.576 | Loss: 0.7340\n",
      "   => Time: 42s\n",
      "   * EPOCH 43 | Accuracy: 75.000 | Loss: 0.6843\n",
      "   => Time: 45s\n",
      "   * EPOCH 44 | Accuracy: 75.000 | Loss: 0.7118\n",
      "   => Time: 42s\n",
      "   * EPOCH 45 | Accuracy: 73.729 | Loss: 0.7868\n",
      "   => Time: 43s\n",
      "   * EPOCH 46 | Accuracy: 72.669 | Loss: 0.7439\n",
      "   => Time: 43s\n",
      "   * EPOCH 47 | Accuracy: 75.212 | Loss: 0.7174\n",
      "   => Time: 42s\n",
      "   * EPOCH 48 | Accuracy: 73.517 | Loss: 0.7519\n",
      "   => Time: 43s\n",
      "   * EPOCH 49 | Accuracy: 74.364 | Loss: 0.7195\n",
      "   => Time: 42s\n",
      "   * EPOCH 50 | Accuracy: 75.424 | Loss: 0.7101\n",
      "   => Time: 43s\n",
      "   * EPOCH 51 | Accuracy: 72.458 | Loss: 0.7417\n",
      "   => Time: 42s\n",
      "   * EPOCH 52 | Accuracy: 73.517 | Loss: 0.7187\n",
      "   => Time: 42s\n",
      "   * EPOCH 53 | Accuracy: 73.941 | Loss: 0.7617\n",
      "   => Time: 42s\n",
      "   * EPOCH 54 | Accuracy: 70.127 | Loss: 0.8150\n",
      "   => Time: 42s\n",
      "   * EPOCH 55 | Accuracy: 75.212 | Loss: 0.6979\n",
      "   => Time: 42s\n",
      "   * EPOCH 56 | Accuracy: 74.364 | Loss: 0.7389\n",
      "   => Time: 42s\n",
      "   * EPOCH 57 | Accuracy: 75.212 | Loss: 0.7317\n",
      "   => Time: 42s\n",
      "   * EPOCH 58 | Accuracy: 72.881 | Loss: 0.7725\n",
      "   => Time: 42s\n",
      "   * EPOCH 59 | Accuracy: 69.492 | Loss: 0.7894\n",
      "   => Time: 43s\n",
      "   * EPOCH 60 | Accuracy: 75.636 | Loss: 0.6878\n",
      "   => Time: 43s\n",
      "   * EPOCH 61 | Accuracy: 71.398 | Loss: 0.7859\n",
      "   => Time: 43s\n",
      "   * EPOCH 62 | Accuracy: 73.305 | Loss: 0.7561\n",
      "   => Time: 42s\n",
      "   * EPOCH 63 | Accuracy: 75.424 | Loss: 0.7424\n",
      "   => Time: 42s\n",
      "   * EPOCH 64 | Accuracy: 72.881 | Loss: 0.7720\n",
      "   => Time: 42s\n",
      "   * EPOCH 65 | Accuracy: 74.576 | Loss: 0.7238\n",
      "   => Time: 42s\n",
      "   * EPOCH 66 | Accuracy: 71.610 | Loss: 0.7630\n",
      "   => Time: 42s\n",
      "   * EPOCH 67 | Accuracy: 73.305 | Loss: 0.7442\n",
      "   => Time: 42s\n",
      "   * EPOCH 68 | Accuracy: 72.881 | Loss: 0.7437\n",
      "   => Time: 42s\n",
      "   * EPOCH 69 | Accuracy: 73.941 | Loss: 0.6926\n",
      "   => Time: 42s\n",
      "   * EPOCH 70 | Accuracy: 74.788 | Loss: 0.7256\n",
      "   => Time: 42s\n",
      "   * EPOCH 71 | Accuracy: 73.517 | Loss: 0.7660\n",
      "   => Time: 43s\n",
      "   * EPOCH 72 | Accuracy: 73.517 | Loss: 0.7476\n",
      "   => Time: 42s\n",
      "   * EPOCH 73 | Accuracy: 75.212 | Loss: 0.6915\n",
      "   => Time: 43s\n",
      "   * EPOCH 74 | Accuracy: 74.788 | Loss: 0.7292\n",
      "   => Time: 42s\n",
      "   * EPOCH 75 | Accuracy: 71.610 | Loss: 0.7411\n",
      "   => Time: 43s\n",
      "   * EPOCH 76 | Accuracy: 73.517 | Loss: 0.7233\n",
      "   => Time: 42s\n",
      "   * EPOCH 77 | Accuracy: 73.305 | Loss: 0.7627\n",
      "   => Time: 42s\n",
      "   * EPOCH 78 | Accuracy: 74.788 | Loss: 0.7387\n",
      "   => Time: 42s\n",
      "   * EPOCH 79 | Accuracy: 72.246 | Loss: 0.7465\n",
      "   => Time: 43s\n",
      "   * EPOCH 80 | Accuracy: 71.822 | Loss: 0.7056\n",
      "   => Time: 43s\n",
      "   * EPOCH 81 | Accuracy: 75.212 | Loss: 0.6987\n",
      "   => Time: 43s\n",
      "   * EPOCH 82 | Accuracy: 74.153 | Loss: 0.7172\n",
      "   => Time: 42s\n",
      "   * EPOCH 83 | Accuracy: 71.186 | Loss: 0.8105\n",
      "   => Time: 43s\n",
      "   * EPOCH 84 | Accuracy: 72.246 | Loss: 0.7868\n",
      "   => Time: 43s\n",
      "   * EPOCH 85 | Accuracy: 75.636 | Loss: 0.7149\n",
      "   => Time: 43s\n",
      "   * EPOCH 86 | Accuracy: 72.458 | Loss: 0.7204\n",
      "   => Time: 43s\n",
      "   * EPOCH 87 | Accuracy: 76.695 | Loss: 0.6814\n",
      "   => Time: 44s\n",
      "   * EPOCH 88 | Accuracy: 72.246 | Loss: 0.7506\n",
      "   => Time: 42s\n",
      "   * EPOCH 89 | Accuracy: 76.483 | Loss: 0.6992\n",
      "   => Time: 43s\n",
      "   * EPOCH 90 | Accuracy: 75.424 | Loss: 0.7219\n",
      "   => Time: 43s\n",
      "   * EPOCH 91 | Accuracy: 71.398 | Loss: 0.7663\n",
      "   => Time: 43s\n",
      "   * EPOCH 92 | Accuracy: 75.424 | Loss: 0.7050\n",
      "   => Time: 42s\n",
      "   * EPOCH 93 | Accuracy: 75.212 | Loss: 0.7222\n",
      "   => Time: 42s\n",
      "   * EPOCH 94 | Accuracy: 73.729 | Loss: 0.7774\n",
      "   => Time: 43s\n",
      "   * EPOCH 95 | Accuracy: 74.788 | Loss: 0.6933\n",
      "   => Time: 42s\n",
      "   * EPOCH 96 | Accuracy: 74.364 | Loss: 0.7284\n",
      "   => Time: 42s\n",
      "   * EPOCH 97 | Accuracy: 76.271 | Loss: 0.6760\n",
      "   => Time: 44s\n",
      "   * EPOCH 98 | Accuracy: 72.458 | Loss: 0.7869\n",
      "   => Time: 43s\n",
      "   * EPOCH 99 | Accuracy: 72.881 | Loss: 0.7957\n",
      "   => Time: 43s\n",
      "   * EPOCH 100 | Accuracy: 74.364 | Loss: 0.7008\n",
      "   => Time: 42s\n",
      "=> Using pre-trained model 'resnet152'\n",
      "=> Starting to train on 'resnet152' model\n",
      "   * EPOCH  1 | Accuracy: 58.051 | Loss: 1.1831\n",
      "   => Time: 52s\n",
      "   * EPOCH  2 | Accuracy: 62.076 | Loss: 1.0202\n",
      "   => Time: 45s\n",
      "   * EPOCH  3 | Accuracy: 65.890 | Loss: 1.0031\n",
      "   => Time: 45s\n",
      "   * EPOCH  4 | Accuracy: 59.958 | Loss: 1.0700\n",
      "   => Time: 43s\n",
      "   * EPOCH  5 | Accuracy: 67.373 | Loss: 0.8864\n",
      "   => Time: 45s\n",
      "   * EPOCH  6 | Accuracy: 68.644 | Loss: 0.9206\n",
      "   => Time: 43s\n",
      "   * EPOCH  7 | Accuracy: 62.288 | Loss: 1.0602\n",
      "   => Time: 43s\n",
      "   * EPOCH  8 | Accuracy: 67.161 | Loss: 0.8873\n",
      "   => Time: 43s\n",
      "   * EPOCH  9 | Accuracy: 68.856 | Loss: 0.8942\n",
      "   => Time: 43s\n",
      "   * EPOCH 10 | Accuracy: 66.949 | Loss: 0.8848\n",
      "   => Time: 45s\n",
      "   * EPOCH 11 | Accuracy: 67.161 | Loss: 0.8760\n",
      "   => Time: 44s\n",
      "   * EPOCH 12 | Accuracy: 68.008 | Loss: 0.8740\n",
      "   => Time: 45s\n",
      "   * EPOCH 13 | Accuracy: 71.398 | Loss: 0.8613\n",
      "   => Time: 45s\n",
      "   * EPOCH 14 | Accuracy: 55.085 | Loss: 1.1837\n",
      "   => Time: 43s\n",
      "   * EPOCH 15 | Accuracy: 67.373 | Loss: 0.9443\n",
      "   => Time: 43s\n",
      "   * EPOCH 16 | Accuracy: 73.941 | Loss: 0.7669\n",
      "   => Time: 45s\n",
      "   * EPOCH 17 | Accuracy: 70.339 | Loss: 0.8482\n",
      "   => Time: 43s\n",
      "   * EPOCH 18 | Accuracy: 69.703 | Loss: 0.8762\n",
      "   => Time: 43s\n",
      "   * EPOCH 19 | Accuracy: 71.398 | Loss: 0.7892\n",
      "   => Time: 43s\n",
      "   * EPOCH 20 | Accuracy: 70.763 | Loss: 0.8753\n",
      "   => Time: 43s\n",
      "   * EPOCH 21 | Accuracy: 67.161 | Loss: 0.8658\n",
      "   => Time: 43s\n",
      "   * EPOCH 22 | Accuracy: 67.585 | Loss: 0.9166\n",
      "   => Time: 43s\n",
      "   * EPOCH 23 | Accuracy: 72.034 | Loss: 0.8052\n",
      "   => Time: 43s\n",
      "   * EPOCH 24 | Accuracy: 72.034 | Loss: 0.7857\n",
      "   => Time: 43s\n",
      "   * EPOCH 25 | Accuracy: 72.458 | Loss: 0.8207\n",
      "   => Time: 43s\n",
      "   * EPOCH 26 | Accuracy: 71.398 | Loss: 0.8093\n",
      "   => Time: 43s\n",
      "   * EPOCH 27 | Accuracy: 73.093 | Loss: 0.8420\n",
      "   => Time: 43s\n",
      "   * EPOCH 28 | Accuracy: 71.398 | Loss: 0.8215\n",
      "   => Time: 43s\n",
      "   * EPOCH 29 | Accuracy: 70.551 | Loss: 0.8066\n",
      "   => Time: 43s\n",
      "   * EPOCH 30 | Accuracy: 73.517 | Loss: 0.7565\n",
      "   => Time: 45s\n",
      "   * EPOCH 31 | Accuracy: 72.034 | Loss: 0.7258\n",
      "   => Time: 45s\n",
      "   * EPOCH 32 | Accuracy: 74.364 | Loss: 0.7579\n",
      "   => Time: 43s\n",
      "   * EPOCH 33 | Accuracy: 76.059 | Loss: 0.7399\n",
      "   => Time: 43s\n",
      "   * EPOCH 34 | Accuracy: 73.517 | Loss: 0.7615\n",
      "   => Time: 43s\n",
      "   * EPOCH 35 | Accuracy: 75.424 | Loss: 0.7449\n",
      "   => Time: 43s\n",
      "   * EPOCH 36 | Accuracy: 73.305 | Loss: 0.7499\n",
      "   => Time: 43s\n",
      "   * EPOCH 37 | Accuracy: 73.517 | Loss: 0.7666\n",
      "   => Time: 43s\n",
      "   * EPOCH 38 | Accuracy: 72.246 | Loss: 0.8009\n",
      "   => Time: 43s\n",
      "   * EPOCH 39 | Accuracy: 73.729 | Loss: 0.7781\n",
      "   => Time: 43s\n",
      "   * EPOCH 40 | Accuracy: 73.941 | Loss: 0.7570\n",
      "   => Time: 43s\n",
      "   * EPOCH 41 | Accuracy: 72.669 | Loss: 0.8135\n",
      "   => Time: 43s\n",
      "   * EPOCH 42 | Accuracy: 70.975 | Loss: 0.8066\n",
      "   => Time: 43s\n",
      "   * EPOCH 43 | Accuracy: 72.034 | Loss: 0.7507\n",
      "   => Time: 43s\n",
      "   * EPOCH 44 | Accuracy: 76.059 | Loss: 0.7527\n",
      "   => Time: 43s\n",
      "   * EPOCH 45 | Accuracy: 73.093 | Loss: 0.7518\n",
      "   => Time: 43s\n",
      "   * EPOCH 46 | Accuracy: 75.212 | Loss: 0.7303\n",
      "   => Time: 43s\n",
      "   * EPOCH 47 | Accuracy: 76.059 | Loss: 0.7394\n",
      "   => Time: 43s\n",
      "   * EPOCH 48 | Accuracy: 73.941 | Loss: 0.7723\n",
      "   => Time: 43s\n",
      "   * EPOCH 49 | Accuracy: 73.941 | Loss: 0.7538\n",
      "   => Time: 43s\n",
      "   * EPOCH 50 | Accuracy: 75.424 | Loss: 0.6854\n",
      "   => Time: 45s\n",
      "   * EPOCH 51 | Accuracy: 77.966 | Loss: 0.7006\n",
      "   => Time: 43s\n",
      "   * EPOCH 52 | Accuracy: 72.034 | Loss: 0.8030\n",
      "   => Time: 43s\n",
      "   * EPOCH 53 | Accuracy: 72.246 | Loss: 0.7744\n",
      "   => Time: 43s\n",
      "   * EPOCH 54 | Accuracy: 75.000 | Loss: 0.7496\n",
      "   => Time: 43s\n",
      "   * EPOCH 55 | Accuracy: 76.271 | Loss: 0.7218\n",
      "   => Time: 43s\n",
      "   * EPOCH 56 | Accuracy: 72.669 | Loss: 0.7662\n",
      "   => Time: 43s\n",
      "   * EPOCH 57 | Accuracy: 74.788 | Loss: 0.7178\n",
      "   => Time: 43s\n",
      "   * EPOCH 58 | Accuracy: 73.305 | Loss: 0.7217\n",
      "   => Time: 43s\n",
      "   * EPOCH 59 | Accuracy: 74.576 | Loss: 0.7315\n",
      "   => Time: 43s\n",
      "   * EPOCH 60 | Accuracy: 75.847 | Loss: 0.7173\n",
      "   => Time: 43s\n",
      "   * EPOCH 61 | Accuracy: 75.212 | Loss: 0.7545\n",
      "   => Time: 43s\n",
      "   * EPOCH 62 | Accuracy: 74.576 | Loss: 0.6979\n",
      "   => Time: 43s\n",
      "   * EPOCH 63 | Accuracy: 74.153 | Loss: 0.7471\n",
      "   => Time: 43s\n",
      "   * EPOCH 64 | Accuracy: 73.729 | Loss: 0.7423\n",
      "   => Time: 43s\n",
      "   * EPOCH 65 | Accuracy: 76.907 | Loss: 0.7314\n",
      "   => Time: 43s\n",
      "   * EPOCH 66 | Accuracy: 72.034 | Loss: 0.7839\n",
      "   => Time: 43s\n",
      "   * EPOCH 67 | Accuracy: 74.153 | Loss: 0.7566\n",
      "   => Time: 43s\n",
      "   * EPOCH 68 | Accuracy: 73.305 | Loss: 0.7603\n",
      "   => Time: 43s\n",
      "   * EPOCH 69 | Accuracy: 74.364 | Loss: 0.7258\n",
      "   => Time: 43s\n",
      "   * EPOCH 70 | Accuracy: 75.000 | Loss: 0.7512\n",
      "   => Time: 43s\n",
      "   * EPOCH 71 | Accuracy: 75.424 | Loss: 0.7624\n",
      "   => Time: 43s\n",
      "   * EPOCH 72 | Accuracy: 73.517 | Loss: 0.7024\n",
      "   => Time: 43s\n",
      "   * EPOCH 73 | Accuracy: 74.153 | Loss: 0.7552\n",
      "   => Time: 43s\n",
      "   * EPOCH 74 | Accuracy: 75.847 | Loss: 0.7685\n",
      "   => Time: 43s\n",
      "   * EPOCH 75 | Accuracy: 75.636 | Loss: 0.6952\n",
      "   => Time: 43s\n",
      "   * EPOCH 76 | Accuracy: 75.000 | Loss: 0.7428\n",
      "   => Time: 43s\n",
      "   * EPOCH 77 | Accuracy: 76.271 | Loss: 0.7161\n",
      "   => Time: 43s\n",
      "   * EPOCH 78 | Accuracy: 75.424 | Loss: 0.7552\n",
      "   => Time: 43s\n",
      "   * EPOCH 79 | Accuracy: 76.483 | Loss: 0.7241\n",
      "   => Time: 43s\n",
      "   * EPOCH 80 | Accuracy: 74.364 | Loss: 0.7191\n",
      "   => Time: 43s\n",
      "   * EPOCH 81 | Accuracy: 76.271 | Loss: 0.6948\n",
      "   => Time: 43s\n",
      "   * EPOCH 82 | Accuracy: 76.271 | Loss: 0.7105\n",
      "   => Time: 43s\n",
      "   * EPOCH 83 | Accuracy: 72.881 | Loss: 0.7252\n",
      "   => Time: 43s\n",
      "   * EPOCH 84 | Accuracy: 73.729 | Loss: 0.7763\n",
      "   => Time: 43s\n",
      "   * EPOCH 85 | Accuracy: 76.483 | Loss: 0.7127\n",
      "   => Time: 43s\n",
      "   * EPOCH 86 | Accuracy: 75.424 | Loss: 0.7216\n",
      "   => Time: 43s\n",
      "   * EPOCH 87 | Accuracy: 74.576 | Loss: 0.7033\n",
      "   => Time: 43s\n",
      "   * EPOCH 88 | Accuracy: 77.119 | Loss: 0.7158\n",
      "   => Time: 43s\n",
      "   * EPOCH 89 | Accuracy: 71.186 | Loss: 0.7830\n",
      "   => Time: 43s\n",
      "   * EPOCH 90 | Accuracy: 73.093 | Loss: 0.7480\n",
      "   => Time: 43s\n",
      "   * EPOCH 91 | Accuracy: 76.907 | Loss: 0.7103\n",
      "   => Time: 43s\n",
      "   * EPOCH 92 | Accuracy: 75.212 | Loss: 0.7446\n",
      "   => Time: 43s\n",
      "   * EPOCH 93 | Accuracy: 76.695 | Loss: 0.7028\n",
      "   => Time: 43s\n",
      "   * EPOCH 94 | Accuracy: 76.271 | Loss: 0.7164\n",
      "   => Time: 43s\n",
      "   * EPOCH 95 | Accuracy: 72.881 | Loss: 0.7880\n",
      "   => Time: 43s\n",
      "   * EPOCH 96 | Accuracy: 74.364 | Loss: 0.7293\n",
      "   => Time: 43s\n",
      "   * EPOCH 97 | Accuracy: 73.941 | Loss: 0.7311\n",
      "   => Time: 43s\n",
      "   * EPOCH 98 | Accuracy: 71.610 | Loss: 0.7662\n",
      "   => Time: 43s\n",
      "   * EPOCH 99 | Accuracy: 70.763 | Loss: 0.7414\n",
      "   => Time: 43s\n",
      "   * EPOCH 100 | Accuracy: 73.729 | Loss: 0.7350\n",
      "   => Time: 43s\n",
      "=> Using pre-trained model 'resnet152'\n",
      "=> Starting to train on 'resnet152' model\n",
      "   * EPOCH  1 | Accuracy: 57.839 | Loss: 1.1921\n",
      "   => Time: 52s\n",
      "   * EPOCH  2 | Accuracy: 60.169 | Loss: 1.1328\n",
      "   => Time: 44s\n",
      "   * EPOCH  3 | Accuracy: 63.136 | Loss: 1.0197\n",
      "   => Time: 44s\n",
      "   * EPOCH  4 | Accuracy: 58.686 | Loss: 1.0890\n",
      "   => Time: 43s\n",
      "   * EPOCH  5 | Accuracy: 62.076 | Loss: 1.0799\n",
      "   => Time: 43s\n",
      "   * EPOCH  6 | Accuracy: 65.678 | Loss: 1.0260\n",
      "   => Time: 43s\n",
      "   * EPOCH  7 | Accuracy: 68.008 | Loss: 0.9285\n",
      "   => Time: 44s\n",
      "   * EPOCH  8 | Accuracy: 69.703 | Loss: 0.9437\n",
      "   => Time: 43s\n",
      "   * EPOCH  9 | Accuracy: 65.466 | Loss: 1.0250\n",
      "   => Time: 43s\n",
      "   * EPOCH 10 | Accuracy: 70.975 | Loss: 0.9127\n",
      "   => Time: 44s\n",
      "   * EPOCH 11 | Accuracy: 64.407 | Loss: 0.9641\n",
      "   => Time: 43s\n",
      "   * EPOCH 12 | Accuracy: 65.678 | Loss: 0.9804\n",
      "   => Time: 43s\n",
      "   * EPOCH 13 | Accuracy: 65.042 | Loss: 1.0208\n",
      "   => Time: 43s\n",
      "   * EPOCH 14 | Accuracy: 69.068 | Loss: 0.9283\n",
      "   => Time: 43s\n",
      "   * EPOCH 15 | Accuracy: 69.915 | Loss: 0.8726\n",
      "   => Time: 44s\n",
      "   * EPOCH 16 | Accuracy: 69.280 | Loss: 0.8421\n",
      "   => Time: 44s\n",
      "   * EPOCH 17 | Accuracy: 65.466 | Loss: 0.9915\n",
      "   => Time: 43s\n",
      "   * EPOCH 18 | Accuracy: 66.949 | Loss: 0.9412\n",
      "   => Time: 43s\n",
      "   * EPOCH 19 | Accuracy: 68.644 | Loss: 0.8608\n",
      "   => Time: 43s\n",
      "   * EPOCH 20 | Accuracy: 69.703 | Loss: 0.8875\n",
      "   => Time: 43s\n",
      "   * EPOCH 21 | Accuracy: 67.161 | Loss: 0.9032\n",
      "   => Time: 43s\n",
      "   * EPOCH 22 | Accuracy: 68.432 | Loss: 0.8562\n",
      "   => Time: 43s\n",
      "   * EPOCH 23 | Accuracy: 73.305 | Loss: 0.8828\n",
      "   => Time: 43s\n",
      "   * EPOCH 24 | Accuracy: 69.492 | Loss: 0.8461\n",
      "   => Time: 43s\n",
      "   * EPOCH 25 | Accuracy: 69.703 | Loss: 1.0380\n",
      "   => Time: 43s\n",
      "   * EPOCH 26 | Accuracy: 72.458 | Loss: 0.8018\n",
      "   => Time: 45s\n",
      "   * EPOCH 27 | Accuracy: 67.797 | Loss: 0.9115\n",
      "   => Time: 43s\n",
      "   * EPOCH 28 | Accuracy: 72.034 | Loss: 0.8544\n",
      "   => Time: 43s\n",
      "   * EPOCH 29 | Accuracy: 72.246 | Loss: 0.8382\n",
      "   => Time: 43s\n",
      "   * EPOCH 30 | Accuracy: 71.398 | Loss: 0.8199\n",
      "   => Time: 43s\n",
      "   * EPOCH 31 | Accuracy: 76.059 | Loss: 0.7406\n",
      "   => Time: 45s\n",
      "   * EPOCH 32 | Accuracy: 74.788 | Loss: 0.7925\n",
      "   => Time: 43s\n",
      "   * EPOCH 33 | Accuracy: 71.822 | Loss: 0.8031\n",
      "   => Time: 43s\n",
      "   * EPOCH 34 | Accuracy: 73.305 | Loss: 0.7939\n",
      "   => Time: 43s\n",
      "   * EPOCH 35 | Accuracy: 75.212 | Loss: 0.7887\n",
      "   => Time: 43s\n",
      "   * EPOCH 36 | Accuracy: 72.669 | Loss: 0.8111\n",
      "   => Time: 43s\n",
      "   * EPOCH 37 | Accuracy: 72.458 | Loss: 0.8195\n",
      "   => Time: 43s\n",
      "   * EPOCH 38 | Accuracy: 73.729 | Loss: 0.8235\n",
      "   => Time: 43s\n",
      "   * EPOCH 39 | Accuracy: 76.271 | Loss: 0.7500\n",
      "   => Time: 43s\n",
      "   * EPOCH 40 | Accuracy: 73.941 | Loss: 0.7722\n",
      "   => Time: 43s\n",
      "   * EPOCH 41 | Accuracy: 71.398 | Loss: 0.8333\n",
      "   => Time: 43s\n",
      "   * EPOCH 42 | Accuracy: 74.576 | Loss: 0.7862\n",
      "   => Time: 43s\n",
      "   * EPOCH 43 | Accuracy: 76.483 | Loss: 0.7815\n",
      "   => Time: 43s\n",
      "   * EPOCH 44 | Accuracy: 71.822 | Loss: 0.7602\n",
      "   => Time: 43s\n",
      "   * EPOCH 45 | Accuracy: 75.212 | Loss: 0.7082\n",
      "   => Time: 44s\n",
      "   * EPOCH 46 | Accuracy: 72.669 | Loss: 0.7720\n",
      "   => Time: 43s\n",
      "   * EPOCH 47 | Accuracy: 75.424 | Loss: 0.7675\n",
      "   => Time: 43s\n",
      "   * EPOCH 48 | Accuracy: 76.271 | Loss: 0.7690\n",
      "   => Time: 43s\n",
      "   * EPOCH 49 | Accuracy: 72.669 | Loss: 0.8030\n",
      "   => Time: 43s\n",
      "   * EPOCH 50 | Accuracy: 69.915 | Loss: 0.8576\n",
      "   => Time: 43s\n",
      "   * EPOCH 51 | Accuracy: 75.212 | Loss: 0.7542\n",
      "   => Time: 43s\n",
      "   * EPOCH 52 | Accuracy: 75.847 | Loss: 0.7442\n",
      "   => Time: 43s\n",
      "   * EPOCH 53 | Accuracy: 75.636 | Loss: 0.7315\n",
      "   => Time: 43s\n",
      "   * EPOCH 54 | Accuracy: 76.695 | Loss: 0.7539\n",
      "   => Time: 43s\n",
      "   * EPOCH 55 | Accuracy: 72.034 | Loss: 0.8154\n",
      "   => Time: 43s\n",
      "   * EPOCH 56 | Accuracy: 73.941 | Loss: 0.7901\n",
      "   => Time: 43s\n",
      "   * EPOCH 57 | Accuracy: 70.763 | Loss: 0.8495\n",
      "   => Time: 43s\n",
      "   * EPOCH 58 | Accuracy: 72.458 | Loss: 0.8269\n",
      "   => Time: 43s\n",
      "   * EPOCH 59 | Accuracy: 75.000 | Loss: 0.8011\n",
      "   => Time: 43s\n",
      "   * EPOCH 60 | Accuracy: 75.636 | Loss: 0.7452\n",
      "   => Time: 43s\n",
      "   * EPOCH 61 | Accuracy: 70.763 | Loss: 0.8360\n",
      "   => Time: 43s\n",
      "   * EPOCH 62 | Accuracy: 75.636 | Loss: 0.7822\n",
      "   => Time: 43s\n",
      "   * EPOCH 63 | Accuracy: 74.788 | Loss: 0.7829\n",
      "   => Time: 43s\n",
      "   * EPOCH 64 | Accuracy: 69.915 | Loss: 0.8372\n",
      "   => Time: 43s\n",
      "   * EPOCH 65 | Accuracy: 72.246 | Loss: 0.8372\n",
      "   => Time: 43s\n",
      "   * EPOCH 66 | Accuracy: 75.212 | Loss: 0.7932\n",
      "   => Time: 43s\n",
      "   * EPOCH 67 | Accuracy: 72.881 | Loss: 0.7848\n",
      "   => Time: 43s\n",
      "   * EPOCH 68 | Accuracy: 75.212 | Loss: 0.7698\n",
      "   => Time: 43s\n",
      "   * EPOCH 69 | Accuracy: 73.729 | Loss: 0.8114\n",
      "   => Time: 43s\n",
      "   * EPOCH 70 | Accuracy: 73.093 | Loss: 0.7267\n",
      "   => Time: 43s\n",
      "   * EPOCH 71 | Accuracy: 72.881 | Loss: 0.7800\n",
      "   => Time: 43s\n",
      "   * EPOCH 72 | Accuracy: 77.119 | Loss: 0.7814\n",
      "   => Time: 43s\n",
      "   * EPOCH 73 | Accuracy: 73.729 | Loss: 0.7804\n",
      "   => Time: 43s\n",
      "   * EPOCH 74 | Accuracy: 73.093 | Loss: 0.7963\n",
      "   => Time: 43s\n",
      "   * EPOCH 75 | Accuracy: 73.305 | Loss: 0.7933\n",
      "   => Time: 43s\n",
      "   * EPOCH 76 | Accuracy: 75.212 | Loss: 0.7706\n",
      "   => Time: 43s\n",
      "   * EPOCH 77 | Accuracy: 73.941 | Loss: 0.7376\n",
      "   => Time: 43s\n",
      "   * EPOCH 78 | Accuracy: 73.729 | Loss: 0.7946\n",
      "   => Time: 43s\n",
      "   * EPOCH 79 | Accuracy: 73.093 | Loss: 0.8242\n",
      "   => Time: 43s\n",
      "   * EPOCH 80 | Accuracy: 70.551 | Loss: 0.8096\n",
      "   => Time: 43s\n",
      "   * EPOCH 81 | Accuracy: 73.093 | Loss: 0.7768\n",
      "   => Time: 43s\n",
      "   * EPOCH 82 | Accuracy: 75.847 | Loss: 0.7408\n",
      "   => Time: 43s\n",
      "   * EPOCH 83 | Accuracy: 72.458 | Loss: 0.7575\n",
      "   => Time: 43s\n",
      "   * EPOCH 84 | Accuracy: 75.212 | Loss: 0.7523\n",
      "   => Time: 43s\n",
      "   * EPOCH 85 | Accuracy: 75.212 | Loss: 0.7523\n",
      "   => Time: 43s\n",
      "   * EPOCH 86 | Accuracy: 73.729 | Loss: 0.7754\n",
      "   => Time: 43s\n",
      "   * EPOCH 87 | Accuracy: 70.975 | Loss: 0.8167\n",
      "   => Time: 43s\n",
      "   * EPOCH 88 | Accuracy: 75.636 | Loss: 0.8088\n",
      "   => Time: 43s\n",
      "   * EPOCH 89 | Accuracy: 73.093 | Loss: 0.7975\n",
      "   => Time: 43s\n",
      "   * EPOCH 90 | Accuracy: 73.941 | Loss: 0.7811\n",
      "   => Time: 43s\n",
      "   * EPOCH 91 | Accuracy: 74.788 | Loss: 0.7092\n",
      "   => Time: 43s\n",
      "   * EPOCH 92 | Accuracy: 74.153 | Loss: 0.7766\n",
      "   => Time: 43s\n",
      "   * EPOCH 93 | Accuracy: 73.305 | Loss: 0.7763\n",
      "   => Time: 43s\n",
      "   * EPOCH 94 | Accuracy: 74.576 | Loss: 0.8123\n",
      "   => Time: 43s\n",
      "   * EPOCH 95 | Accuracy: 74.153 | Loss: 0.7421\n",
      "   => Time: 43s\n",
      "   * EPOCH 96 | Accuracy: 73.305 | Loss: 0.7581\n",
      "   => Time: 43s\n",
      "   * EPOCH 97 | Accuracy: 72.669 | Loss: 0.7758\n",
      "   => Time: 43s\n",
      "   * EPOCH 98 | Accuracy: 75.212 | Loss: 0.7604\n",
      "   => Time: 43s\n",
      "   * EPOCH 99 | Accuracy: 72.881 | Loss: 0.7701\n",
      "   => Time: 43s\n",
      "   * EPOCH 100 | Accuracy: 72.669 | Loss: 0.7600\n",
      "   => Time: 43s\n",
      "=> Using pre-trained model 'resnet152'\n",
      "=> Starting to train on 'resnet152' model\n",
      "   * EPOCH  1 | Accuracy: 57.839 | Loss: 1.1826\n",
      "   => Time: 52s\n",
      "   * EPOCH  2 | Accuracy: 58.686 | Loss: 1.1362\n",
      "   => Time: 44s\n",
      "   * EPOCH  3 | Accuracy: 63.136 | Loss: 1.0366\n",
      "   => Time: 45s\n",
      "   * EPOCH  4 | Accuracy: 60.169 | Loss: 1.1918\n",
      "   => Time: 43s\n",
      "   * EPOCH  5 | Accuracy: 64.831 | Loss: 1.0217\n",
      "   => Time: 45s\n",
      "   * EPOCH  6 | Accuracy: 66.314 | Loss: 0.9687\n",
      "   => Time: 45s\n",
      "   * EPOCH  7 | Accuracy: 68.644 | Loss: 0.8729\n",
      "   => Time: 45s\n",
      "   * EPOCH  8 | Accuracy: 68.008 | Loss: 0.9118\n",
      "   => Time: 43s\n",
      "   * EPOCH  9 | Accuracy: 67.585 | Loss: 0.9625\n",
      "   => Time: 43s\n",
      "   * EPOCH 10 | Accuracy: 68.432 | Loss: 0.8793\n",
      "   => Time: 43s\n",
      "   * EPOCH 11 | Accuracy: 70.127 | Loss: 0.8713\n",
      "   => Time: 45s\n",
      "   * EPOCH 12 | Accuracy: 64.619 | Loss: 0.9873\n",
      "   => Time: 43s\n",
      "   * EPOCH 13 | Accuracy: 72.034 | Loss: 0.8282\n",
      "   => Time: 44s\n",
      "   * EPOCH 14 | Accuracy: 68.008 | Loss: 0.8597\n",
      "   => Time: 43s\n",
      "   * EPOCH 15 | Accuracy: 71.186 | Loss: 0.8444\n",
      "   => Time: 43s\n",
      "   * EPOCH 16 | Accuracy: 63.983 | Loss: 1.0353\n",
      "   => Time: 43s\n",
      "   * EPOCH 17 | Accuracy: 75.212 | Loss: 0.7842\n",
      "   => Time: 44s\n",
      "   * EPOCH 18 | Accuracy: 73.305 | Loss: 0.7786\n",
      "   => Time: 44s\n",
      "   * EPOCH 19 | Accuracy: 71.186 | Loss: 0.8171\n",
      "   => Time: 43s\n",
      "   * EPOCH 20 | Accuracy: 69.915 | Loss: 0.8671\n",
      "   => Time: 43s\n",
      "   * EPOCH 21 | Accuracy: 65.466 | Loss: 0.8431\n",
      "   => Time: 43s\n",
      "   * EPOCH 22 | Accuracy: 69.280 | Loss: 0.7943\n",
      "   => Time: 43s\n",
      "   * EPOCH 23 | Accuracy: 72.669 | Loss: 0.7724\n",
      "   => Time: 45s\n",
      "   * EPOCH 24 | Accuracy: 70.127 | Loss: 0.8101\n",
      "   => Time: 43s\n",
      "   * EPOCH 25 | Accuracy: 73.093 | Loss: 0.8134\n",
      "   => Time: 43s\n",
      "   * EPOCH 26 | Accuracy: 71.186 | Loss: 0.7938\n",
      "   => Time: 43s\n",
      "   * EPOCH 27 | Accuracy: 71.822 | Loss: 0.7688\n",
      "   => Time: 45s\n",
      "   * EPOCH 28 | Accuracy: 73.093 | Loss: 0.7960\n",
      "   => Time: 43s\n",
      "   * EPOCH 29 | Accuracy: 72.881 | Loss: 0.7993\n",
      "   => Time: 43s\n",
      "   * EPOCH 30 | Accuracy: 73.729 | Loss: 0.7408\n",
      "   => Time: 45s\n",
      "   * EPOCH 31 | Accuracy: 77.542 | Loss: 0.6920\n",
      "   => Time: 45s\n",
      "   * EPOCH 32 | Accuracy: 74.153 | Loss: 0.7298\n",
      "   => Time: 43s\n",
      "   * EPOCH 33 | Accuracy: 75.847 | Loss: 0.7577\n",
      "   => Time: 43s\n",
      "   * EPOCH 34 | Accuracy: 76.271 | Loss: 0.7183\n",
      "   => Time: 43s\n",
      "   * EPOCH 35 | Accuracy: 74.364 | Loss: 0.7271\n",
      "   => Time: 43s\n",
      "   * EPOCH 36 | Accuracy: 73.941 | Loss: 0.7265\n",
      "   => Time: 43s\n",
      "   * EPOCH 37 | Accuracy: 74.576 | Loss: 0.7234\n",
      "   => Time: 43s\n",
      "   * EPOCH 38 | Accuracy: 73.941 | Loss: 0.7028\n",
      "   => Time: 43s\n",
      "   * EPOCH 39 | Accuracy: 75.424 | Loss: 0.7267\n",
      "   => Time: 43s\n",
      "   * EPOCH 40 | Accuracy: 75.636 | Loss: 0.6987\n",
      "   => Time: 43s\n",
      "   * EPOCH 41 | Accuracy: 73.093 | Loss: 0.7159\n",
      "   => Time: 43s\n",
      "   * EPOCH 42 | Accuracy: 75.212 | Loss: 0.7011\n",
      "   => Time: 43s\n",
      "   * EPOCH 43 | Accuracy: 75.847 | Loss: 0.7316\n",
      "   => Time: 43s\n",
      "   * EPOCH 44 | Accuracy: 74.788 | Loss: 0.7255\n",
      "   => Time: 43s\n",
      "   * EPOCH 45 | Accuracy: 74.576 | Loss: 0.7110\n",
      "   => Time: 43s\n",
      "   * EPOCH 46 | Accuracy: 77.119 | Loss: 0.7355\n",
      "   => Time: 43s\n",
      "   * EPOCH 47 | Accuracy: 74.788 | Loss: 0.6879\n",
      "   => Time: 44s\n",
      "   * EPOCH 48 | Accuracy: 74.576 | Loss: 0.6779\n",
      "   => Time: 45s\n",
      "   * EPOCH 49 | Accuracy: 75.000 | Loss: 0.7194\n",
      "   => Time: 43s\n",
      "   * EPOCH 50 | Accuracy: 75.000 | Loss: 0.6880\n",
      "   => Time: 43s\n",
      "   * EPOCH 51 | Accuracy: 77.542 | Loss: 0.6926\n",
      "   => Time: 43s\n",
      "   * EPOCH 52 | Accuracy: 75.212 | Loss: 0.6934\n",
      "   => Time: 43s\n",
      "   * EPOCH 53 | Accuracy: 76.059 | Loss: 0.7032\n",
      "   => Time: 43s\n",
      "   * EPOCH 54 | Accuracy: 76.907 | Loss: 0.6954\n",
      "   => Time: 43s\n",
      "   * EPOCH 55 | Accuracy: 74.364 | Loss: 0.7545\n",
      "   => Time: 43s\n",
      "   * EPOCH 56 | Accuracy: 75.000 | Loss: 0.7373\n",
      "   => Time: 43s\n",
      "   * EPOCH 57 | Accuracy: 75.000 | Loss: 0.6936\n",
      "   => Time: 43s\n",
      "   * EPOCH 58 | Accuracy: 76.059 | Loss: 0.6805\n",
      "   => Time: 43s\n",
      "   * EPOCH 59 | Accuracy: 76.059 | Loss: 0.6851\n",
      "   => Time: 43s\n",
      "   * EPOCH 60 | Accuracy: 74.153 | Loss: 0.7218\n",
      "   => Time: 43s\n",
      "   * EPOCH 61 | Accuracy: 75.847 | Loss: 0.7002\n",
      "   => Time: 43s\n",
      "   * EPOCH 62 | Accuracy: 77.119 | Loss: 0.7029\n",
      "   => Time: 43s\n",
      "   * EPOCH 63 | Accuracy: 75.424 | Loss: 0.6956\n",
      "   => Time: 43s\n",
      "   * EPOCH 64 | Accuracy: 75.000 | Loss: 0.7182\n",
      "   => Time: 43s\n",
      "   * EPOCH 65 | Accuracy: 73.517 | Loss: 0.7222\n",
      "   => Time: 43s\n",
      "   * EPOCH 66 | Accuracy: 75.212 | Loss: 0.7173\n",
      "   => Time: 43s\n",
      "   * EPOCH 67 | Accuracy: 73.729 | Loss: 0.7500\n",
      "   => Time: 43s\n",
      "   * EPOCH 68 | Accuracy: 76.271 | Loss: 0.7052\n",
      "   => Time: 43s\n",
      "   * EPOCH 69 | Accuracy: 75.636 | Loss: 0.7120\n",
      "   => Time: 43s\n",
      "   * EPOCH 70 | Accuracy: 76.907 | Loss: 0.6968\n",
      "   => Time: 43s\n",
      "   * EPOCH 71 | Accuracy: 74.576 | Loss: 0.7105\n",
      "   => Time: 43s\n",
      "   * EPOCH 72 | Accuracy: 78.602 | Loss: 0.7163\n",
      "   => Time: 43s\n",
      "   * EPOCH 73 | Accuracy: 76.271 | Loss: 0.7143\n",
      "   => Time: 43s\n",
      "   * EPOCH 74 | Accuracy: 75.847 | Loss: 0.6891\n",
      "   => Time: 43s\n",
      "   * EPOCH 75 | Accuracy: 73.517 | Loss: 0.7390\n",
      "   => Time: 43s\n",
      "   * EPOCH 76 | Accuracy: 77.119 | Loss: 0.6989\n",
      "   => Time: 43s\n",
      "   * EPOCH 77 | Accuracy: 75.212 | Loss: 0.7120\n",
      "   => Time: 43s\n",
      "   * EPOCH 78 | Accuracy: 73.305 | Loss: 0.7184\n",
      "   => Time: 43s\n",
      "   * EPOCH 79 | Accuracy: 73.093 | Loss: 0.7461\n",
      "   => Time: 43s\n",
      "   * EPOCH 80 | Accuracy: 77.542 | Loss: 0.6950\n",
      "   => Time: 43s\n",
      "   * EPOCH 81 | Accuracy: 75.847 | Loss: 0.7292\n",
      "   => Time: 43s\n",
      "   * EPOCH 82 | Accuracy: 76.907 | Loss: 0.6587\n",
      "   => Time: 45s\n",
      "   * EPOCH 83 | Accuracy: 74.576 | Loss: 0.7288\n",
      "   => Time: 43s\n",
      "   * EPOCH 84 | Accuracy: 76.695 | Loss: 0.6435\n",
      "   => Time: 45s\n",
      "   * EPOCH 85 | Accuracy: 76.059 | Loss: 0.6875\n",
      "   => Time: 43s\n",
      "   * EPOCH 86 | Accuracy: 76.907 | Loss: 0.6820\n",
      "   => Time: 43s\n",
      "   * EPOCH 87 | Accuracy: 75.424 | Loss: 0.7096\n",
      "   => Time: 43s\n",
      "   * EPOCH 88 | Accuracy: 77.754 | Loss: 0.6459\n",
      "   => Time: 43s\n",
      "   * EPOCH 89 | Accuracy: 74.788 | Loss: 0.7278\n",
      "   => Time: 43s\n",
      "   * EPOCH 90 | Accuracy: 77.966 | Loss: 0.6780\n",
      "   => Time: 43s\n",
      "   * EPOCH 91 | Accuracy: 76.271 | Loss: 0.6651\n",
      "   => Time: 43s\n",
      "   * EPOCH 92 | Accuracy: 74.153 | Loss: 0.7330\n",
      "   => Time: 43s\n",
      "   * EPOCH 93 | Accuracy: 73.093 | Loss: 0.7239\n",
      "   => Time: 43s\n",
      "   * EPOCH 94 | Accuracy: 76.059 | Loss: 0.7191\n",
      "   => Time: 43s\n",
      "   * EPOCH 95 | Accuracy: 76.695 | Loss: 0.7012\n",
      "   => Time: 43s\n",
      "   * EPOCH 96 | Accuracy: 80.085 | Loss: 0.6564\n",
      "   => Time: 43s\n",
      "   * EPOCH 97 | Accuracy: 75.212 | Loss: 0.7240\n",
      "   => Time: 43s\n",
      "   * EPOCH 98 | Accuracy: 75.212 | Loss: 0.7404\n",
      "   => Time: 43s\n",
      "   * EPOCH 99 | Accuracy: 73.517 | Loss: 0.6903\n",
      "   => Time: 43s\n",
      "   * EPOCH 100 | Accuracy: 79.025 | Loss: 0.6570\n",
      "   => Time: 43s\n",
      "=> Using pre-trained model 'resnet152'\n",
      "=> Starting to train on 'resnet152' model\n",
      "   * EPOCH  1 | Accuracy: 58.051 | Loss: 1.1561\n",
      "   => Time: 51s\n",
      "   * EPOCH  2 | Accuracy: 62.924 | Loss: 1.0462\n",
      "   => Time: 45s\n",
      "   * EPOCH  3 | Accuracy: 65.042 | Loss: 1.0127\n",
      "   => Time: 45s\n",
      "   * EPOCH  4 | Accuracy: 65.466 | Loss: 0.9942\n",
      "   => Time: 45s\n",
      "   * EPOCH  5 | Accuracy: 62.076 | Loss: 1.0524\n",
      "   => Time: 43s\n",
      "   * EPOCH  6 | Accuracy: 67.161 | Loss: 0.9793\n",
      "   => Time: 45s\n",
      "   * EPOCH  7 | Accuracy: 66.949 | Loss: 0.9701\n",
      "   => Time: 45s\n",
      "   * EPOCH  8 | Accuracy: 69.915 | Loss: 0.9319\n",
      "   => Time: 45s\n",
      "   * EPOCH  9 | Accuracy: 66.102 | Loss: 0.9335\n",
      "   => Time: 43s\n",
      "   * EPOCH 10 | Accuracy: 66.525 | Loss: 0.9390\n",
      "   => Time: 43s\n",
      "   * EPOCH 11 | Accuracy: 68.432 | Loss: 0.8637\n",
      "   => Time: 45s\n",
      "   * EPOCH 12 | Accuracy: 68.008 | Loss: 0.9213\n",
      "   => Time: 43s\n",
      "   * EPOCH 13 | Accuracy: 63.559 | Loss: 0.9758\n",
      "   => Time: 43s\n",
      "   * EPOCH 14 | Accuracy: 68.644 | Loss: 0.8496\n",
      "   => Time: 44s\n",
      "   * EPOCH 15 | Accuracy: 69.280 | Loss: 0.9279\n",
      "   => Time: 43s\n",
      "   * EPOCH 16 | Accuracy: 70.763 | Loss: 0.8140\n",
      "   => Time: 44s\n",
      "   * EPOCH 17 | Accuracy: 63.771 | Loss: 0.9766\n",
      "   => Time: 43s\n",
      "   * EPOCH 18 | Accuracy: 64.407 | Loss: 1.0025\n",
      "   => Time: 43s\n",
      "   * EPOCH 19 | Accuracy: 70.127 | Loss: 0.8592\n",
      "   => Time: 43s\n",
      "   * EPOCH 20 | Accuracy: 68.008 | Loss: 0.9528\n",
      "   => Time: 43s\n",
      "   * EPOCH 21 | Accuracy: 71.186 | Loss: 0.7947\n",
      "   => Time: 44s\n",
      "   * EPOCH 22 | Accuracy: 72.246 | Loss: 0.8142\n",
      "   => Time: 43s\n",
      "   * EPOCH 23 | Accuracy: 66.737 | Loss: 0.8971\n",
      "   => Time: 43s\n",
      "   * EPOCH 24 | Accuracy: 70.763 | Loss: 0.7965\n",
      "   => Time: 43s\n",
      "   * EPOCH 25 | Accuracy: 65.254 | Loss: 0.9814\n",
      "   => Time: 43s\n",
      "   * EPOCH 26 | Accuracy: 69.280 | Loss: 0.8122\n",
      "   => Time: 43s\n",
      "   * EPOCH 27 | Accuracy: 68.432 | Loss: 0.8861\n",
      "   => Time: 43s\n",
      "   * EPOCH 28 | Accuracy: 71.610 | Loss: 0.7894\n",
      "   => Time: 44s\n",
      "   * EPOCH 29 | Accuracy: 74.364 | Loss: 0.7880\n",
      "   => Time: 44s\n",
      "   * EPOCH 30 | Accuracy: 70.339 | Loss: 0.8151\n",
      "   => Time: 43s\n",
      "   * EPOCH 31 | Accuracy: 70.551 | Loss: 0.8292\n",
      "   => Time: 43s\n",
      "   * EPOCH 32 | Accuracy: 72.246 | Loss: 0.7613\n",
      "   => Time: 44s\n",
      "   * EPOCH 33 | Accuracy: 71.398 | Loss: 0.7995\n",
      "   => Time: 43s\n",
      "   * EPOCH 34 | Accuracy: 72.458 | Loss: 0.7458\n",
      "   => Time: 45s\n",
      "   * EPOCH 35 | Accuracy: 70.763 | Loss: 0.7929\n",
      "   => Time: 43s\n",
      "   * EPOCH 36 | Accuracy: 72.881 | Loss: 0.7545\n",
      "   => Time: 43s\n",
      "   * EPOCH 37 | Accuracy: 72.246 | Loss: 0.7828\n",
      "   => Time: 43s\n",
      "   * EPOCH 38 | Accuracy: 71.186 | Loss: 0.8079\n",
      "   => Time: 43s\n",
      "   * EPOCH 39 | Accuracy: 75.424 | Loss: 0.7293\n",
      "   => Time: 45s\n",
      "   * EPOCH 40 | Accuracy: 72.669 | Loss: 0.7778\n",
      "   => Time: 43s\n",
      "   * EPOCH 41 | Accuracy: 75.424 | Loss: 0.7211\n",
      "   => Time: 45s\n",
      "   * EPOCH 42 | Accuracy: 70.127 | Loss: 0.7770\n",
      "   => Time: 43s\n",
      "   * EPOCH 43 | Accuracy: 75.847 | Loss: 0.7313\n",
      "   => Time: 43s\n",
      "   * EPOCH 44 | Accuracy: 75.212 | Loss: 0.7351\n",
      "   => Time: 43s\n",
      "   * EPOCH 45 | Accuracy: 73.517 | Loss: 0.7723\n",
      "   => Time: 43s\n",
      "   * EPOCH 46 | Accuracy: 74.153 | Loss: 0.7676\n",
      "   => Time: 43s\n",
      "   * EPOCH 47 | Accuracy: 74.153 | Loss: 0.7757\n",
      "   => Time: 43s\n",
      "   * EPOCH 48 | Accuracy: 72.458 | Loss: 0.7541\n",
      "   => Time: 43s\n",
      "   * EPOCH 49 | Accuracy: 74.153 | Loss: 0.6882\n",
      "   => Time: 46s\n",
      "   * EPOCH 50 | Accuracy: 73.729 | Loss: 0.7549\n",
      "   => Time: 43s\n",
      "   * EPOCH 51 | Accuracy: 73.305 | Loss: 0.7569\n",
      "   => Time: 43s\n",
      "   * EPOCH 52 | Accuracy: 73.093 | Loss: 0.7978\n",
      "   => Time: 43s\n",
      "   * EPOCH 53 | Accuracy: 72.669 | Loss: 0.7672\n",
      "   => Time: 43s\n",
      "   * EPOCH 54 | Accuracy: 77.754 | Loss: 0.6903\n",
      "   => Time: 43s\n",
      "   * EPOCH 55 | Accuracy: 72.669 | Loss: 0.7890\n",
      "   => Time: 43s\n",
      "   * EPOCH 56 | Accuracy: 71.822 | Loss: 0.7773\n",
      "   => Time: 43s\n",
      "   * EPOCH 57 | Accuracy: 74.576 | Loss: 0.7649\n",
      "   => Time: 43s\n",
      "   * EPOCH 58 | Accuracy: 76.059 | Loss: 0.7248\n",
      "   => Time: 43s\n",
      "   * EPOCH 59 | Accuracy: 71.822 | Loss: 0.7767\n",
      "   => Time: 43s\n",
      "   * EPOCH 60 | Accuracy: 71.398 | Loss: 0.7979\n",
      "   => Time: 43s\n",
      "   * EPOCH 61 | Accuracy: 76.271 | Loss: 0.7289\n",
      "   => Time: 43s\n",
      "   * EPOCH 62 | Accuracy: 71.186 | Loss: 0.7914\n",
      "   => Time: 43s\n",
      "   * EPOCH 63 | Accuracy: 72.246 | Loss: 0.7778\n",
      "   => Time: 43s\n",
      "   * EPOCH 64 | Accuracy: 74.576 | Loss: 0.7061\n",
      "   => Time: 43s\n",
      "   * EPOCH 65 | Accuracy: 73.093 | Loss: 0.7447\n",
      "   => Time: 43s\n",
      "   * EPOCH 66 | Accuracy: 76.483 | Loss: 0.7213\n",
      "   => Time: 43s\n",
      "   * EPOCH 67 | Accuracy: 74.788 | Loss: 0.7693\n",
      "   => Time: 43s\n",
      "   * EPOCH 68 | Accuracy: 73.517 | Loss: 0.7447\n",
      "   => Time: 43s\n",
      "   * EPOCH 69 | Accuracy: 73.305 | Loss: 0.7686\n",
      "   => Time: 43s\n",
      "   * EPOCH 70 | Accuracy: 74.153 | Loss: 0.7376\n",
      "   => Time: 43s\n",
      "   * EPOCH 71 | Accuracy: 71.610 | Loss: 0.7729\n",
      "   => Time: 43s\n",
      "   * EPOCH 72 | Accuracy: 72.034 | Loss: 0.7892\n",
      "   => Time: 43s\n",
      "   * EPOCH 73 | Accuracy: 74.788 | Loss: 0.7445\n",
      "   => Time: 43s\n",
      "   * EPOCH 74 | Accuracy: 74.788 | Loss: 0.7208\n",
      "   => Time: 43s\n",
      "   * EPOCH 75 | Accuracy: 69.703 | Loss: 0.8325\n",
      "   => Time: 43s\n",
      "   * EPOCH 76 | Accuracy: 73.941 | Loss: 0.7243\n",
      "   => Time: 43s\n",
      "   * EPOCH 77 | Accuracy: 73.729 | Loss: 0.7579\n",
      "   => Time: 43s\n",
      "   * EPOCH 78 | Accuracy: 76.695 | Loss: 0.6769\n",
      "   => Time: 45s\n",
      "   * EPOCH 79 | Accuracy: 73.517 | Loss: 0.7843\n",
      "   => Time: 43s\n",
      "   * EPOCH 80 | Accuracy: 72.669 | Loss: 0.7696\n",
      "   => Time: 43s\n",
      "   * EPOCH 81 | Accuracy: 73.729 | Loss: 0.7772\n",
      "   => Time: 43s\n",
      "   * EPOCH 82 | Accuracy: 76.059 | Loss: 0.6988\n",
      "   => Time: 43s\n",
      "   * EPOCH 83 | Accuracy: 74.576 | Loss: 0.7482\n",
      "   => Time: 43s\n",
      "   * EPOCH 84 | Accuracy: 74.788 | Loss: 0.7398\n",
      "   => Time: 43s\n",
      "   * EPOCH 85 | Accuracy: 72.458 | Loss: 0.7576\n",
      "   => Time: 43s\n",
      "   * EPOCH 86 | Accuracy: 72.669 | Loss: 0.7827\n",
      "   => Time: 43s\n",
      "   * EPOCH 87 | Accuracy: 71.822 | Loss: 0.7547\n",
      "   => Time: 43s\n",
      "   * EPOCH 88 | Accuracy: 72.881 | Loss: 0.7982\n",
      "   => Time: 43s\n",
      "   * EPOCH 89 | Accuracy: 75.424 | Loss: 0.7279\n",
      "   => Time: 43s\n",
      "   * EPOCH 90 | Accuracy: 76.271 | Loss: 0.7219\n",
      "   => Time: 43s\n",
      "   * EPOCH 91 | Accuracy: 72.669 | Loss: 0.7549\n",
      "   => Time: 43s\n",
      "   * EPOCH 92 | Accuracy: 73.517 | Loss: 0.7332\n",
      "   => Time: 43s\n",
      "   * EPOCH 93 | Accuracy: 76.059 | Loss: 0.7243\n",
      "   => Time: 43s\n",
      "   * EPOCH 94 | Accuracy: 77.331 | Loss: 0.6868\n",
      "   => Time: 43s\n",
      "   * EPOCH 95 | Accuracy: 72.669 | Loss: 0.7481\n",
      "   => Time: 43s\n",
      "   * EPOCH 96 | Accuracy: 76.483 | Loss: 0.6867\n",
      "   => Time: 43s\n",
      "   * EPOCH 97 | Accuracy: 75.000 | Loss: 0.7586\n",
      "   => Time: 43s\n",
      "   * EPOCH 98 | Accuracy: 72.246 | Loss: 0.7664\n",
      "   => Time: 43s\n",
      "   * EPOCH 99 | Accuracy: 74.364 | Loss: 0.7669\n",
      "   => Time: 43s\n",
      "   * EPOCH 100 | Accuracy: 73.093 | Loss: 0.7578\n",
      "   => Time: 43s\n",
      "=> Using pre-trained model 'resnet152'\n",
      "=> Starting to train on 'resnet152' model\n",
      "   * EPOCH  1 | Accuracy: 54.757 | Loss: 1.2746\n",
      "   => Time: 52s\n",
      "   * EPOCH  2 | Accuracy: 57.082 | Loss: 1.1638\n",
      "   => Time: 45s\n",
      "   * EPOCH  3 | Accuracy: 60.888 | Loss: 1.1122\n",
      "   => Time: 45s\n",
      "   * EPOCH  4 | Accuracy: 63.425 | Loss: 1.0486\n",
      "   => Time: 45s\n",
      "   * EPOCH  5 | Accuracy: 66.596 | Loss: 0.9653\n",
      "   => Time: 45s\n",
      "   * EPOCH  6 | Accuracy: 63.214 | Loss: 0.9835\n",
      "   => Time: 43s\n",
      "   * EPOCH  7 | Accuracy: 61.734 | Loss: 1.0356\n",
      "   => Time: 43s\n",
      "   * EPOCH  8 | Accuracy: 65.116 | Loss: 0.9770\n",
      "   => Time: 43s\n",
      "   * EPOCH  9 | Accuracy: 55.603 | Loss: 1.1949\n",
      "   => Time: 43s\n",
      "   * EPOCH 10 | Accuracy: 68.499 | Loss: 0.8603\n",
      "   => Time: 45s\n",
      "   * EPOCH 11 | Accuracy: 63.002 | Loss: 1.0085\n",
      "   => Time: 43s\n",
      "   * EPOCH 12 | Accuracy: 64.482 | Loss: 0.9432\n",
      "   => Time: 43s\n",
      "   * EPOCH 13 | Accuracy: 67.653 | Loss: 0.9387\n",
      "   => Time: 43s\n",
      "   * EPOCH 14 | Accuracy: 66.808 | Loss: 0.9290\n",
      "   => Time: 43s\n",
      "   * EPOCH 15 | Accuracy: 67.230 | Loss: 0.9180\n",
      "   => Time: 43s\n",
      "   * EPOCH 16 | Accuracy: 69.556 | Loss: 0.8727\n",
      "   => Time: 43s\n",
      "   * EPOCH 17 | Accuracy: 68.288 | Loss: 0.9073\n",
      "   => Time: 43s\n",
      "   * EPOCH 18 | Accuracy: 65.962 | Loss: 0.8845\n",
      "   => Time: 43s\n",
      "   * EPOCH 19 | Accuracy: 71.247 | Loss: 0.8048\n",
      "   => Time: 45s\n",
      "   * EPOCH 20 | Accuracy: 70.402 | Loss: 0.8449\n",
      "   => Time: 43s\n",
      "   * EPOCH 21 | Accuracy: 69.556 | Loss: 0.8174\n",
      "   => Time: 43s\n",
      "   * EPOCH 22 | Accuracy: 68.499 | Loss: 0.8292\n",
      "   => Time: 43s\n",
      "   * EPOCH 23 | Accuracy: 69.556 | Loss: 0.8225\n",
      "   => Time: 43s\n",
      "   * EPOCH 24 | Accuracy: 65.328 | Loss: 0.9294\n",
      "   => Time: 43s\n",
      "   * EPOCH 25 | Accuracy: 68.710 | Loss: 0.8751\n",
      "   => Time: 43s\n",
      "   * EPOCH 26 | Accuracy: 72.093 | Loss: 0.7653\n",
      "   => Time: 45s\n",
      "   * EPOCH 27 | Accuracy: 67.653 | Loss: 0.8770\n",
      "   => Time: 44s\n",
      "   * EPOCH 28 | Accuracy: 71.247 | Loss: 0.8370\n",
      "   => Time: 45s\n",
      "   * EPOCH 29 | Accuracy: 73.573 | Loss: 0.7587\n",
      "   => Time: 47s\n",
      "   * EPOCH 30 | Accuracy: 70.402 | Loss: 0.8047\n",
      "   => Time: 45s\n",
      "   * EPOCH 31 | Accuracy: 70.825 | Loss: 0.7837\n",
      "   => Time: 44s\n",
      "   * EPOCH 32 | Accuracy: 69.345 | Loss: 0.8335\n",
      "   => Time: 45s\n",
      "   * EPOCH 33 | Accuracy: 70.613 | Loss: 0.8126\n",
      "   => Time: 44s\n",
      "   * EPOCH 34 | Accuracy: 72.516 | Loss: 0.7489\n",
      "   => Time: 46s\n",
      "   * EPOCH 35 | Accuracy: 72.727 | Loss: 0.7328\n",
      "   => Time: 45s\n",
      "   * EPOCH 36 | Accuracy: 72.939 | Loss: 0.8182\n",
      "   => Time: 43s\n",
      "   * EPOCH 37 | Accuracy: 71.670 | Loss: 0.7827\n",
      "   => Time: 43s\n",
      "   * EPOCH 38 | Accuracy: 71.459 | Loss: 0.7557\n",
      "   => Time: 43s\n",
      "   * EPOCH 39 | Accuracy: 72.093 | Loss: 0.7537\n",
      "   => Time: 43s\n",
      "   * EPOCH 40 | Accuracy: 74.630 | Loss: 0.7249\n",
      "   => Time: 46s\n",
      "   * EPOCH 41 | Accuracy: 73.150 | Loss: 0.7634\n",
      "   => Time: 43s\n",
      "   * EPOCH 42 | Accuracy: 72.727 | Loss: 0.7449\n",
      "   => Time: 43s\n",
      "   * EPOCH 43 | Accuracy: 73.150 | Loss: 0.7691\n",
      "   => Time: 43s\n",
      "   * EPOCH 44 | Accuracy: 72.939 | Loss: 0.7577\n",
      "   => Time: 43s\n",
      "   * EPOCH 45 | Accuracy: 71.459 | Loss: 0.7767\n",
      "   => Time: 43s\n",
      "   * EPOCH 46 | Accuracy: 74.841 | Loss: 0.7483\n",
      "   => Time: 43s\n",
      "   * EPOCH 47 | Accuracy: 77.801 | Loss: 0.6658\n",
      "   => Time: 46s\n",
      "   * EPOCH 48 | Accuracy: 73.150 | Loss: 0.7707\n",
      "   => Time: 43s\n",
      "   * EPOCH 49 | Accuracy: 72.304 | Loss: 0.7446\n",
      "   => Time: 43s\n",
      "   * EPOCH 50 | Accuracy: 72.727 | Loss: 0.7637\n",
      "   => Time: 44s\n",
      "   * EPOCH 51 | Accuracy: 74.207 | Loss: 0.7475\n",
      "   => Time: 43s\n",
      "   * EPOCH 52 | Accuracy: 67.653 | Loss: 0.8375\n",
      "   => Time: 45s\n",
      "   * EPOCH 53 | Accuracy: 76.956 | Loss: 0.7039\n",
      "   => Time: 43s\n",
      "   * EPOCH 54 | Accuracy: 74.207 | Loss: 0.7436\n",
      "   => Time: 43s\n",
      "   * EPOCH 55 | Accuracy: 75.899 | Loss: 0.7238\n",
      "   => Time: 44s\n",
      "   * EPOCH 56 | Accuracy: 71.882 | Loss: 0.7851\n",
      "   => Time: 43s\n",
      "   * EPOCH 57 | Accuracy: 74.841 | Loss: 0.6989\n",
      "   => Time: 43s\n",
      "   * EPOCH 58 | Accuracy: 72.727 | Loss: 0.7505\n",
      "   => Time: 43s\n",
      "   * EPOCH 59 | Accuracy: 71.882 | Loss: 0.7319\n",
      "   => Time: 43s\n",
      "   * EPOCH 60 | Accuracy: 72.304 | Loss: 0.7547\n",
      "   => Time: 43s\n",
      "   * EPOCH 61 | Accuracy: 72.939 | Loss: 0.6870\n",
      "   => Time: 43s\n",
      "   * EPOCH 62 | Accuracy: 71.882 | Loss: 0.7577\n",
      "   => Time: 43s\n",
      "   * EPOCH 63 | Accuracy: 73.573 | Loss: 0.7477\n",
      "   => Time: 43s\n",
      "   * EPOCH 64 | Accuracy: 71.247 | Loss: 0.7997\n",
      "   => Time: 43s\n",
      "   * EPOCH 65 | Accuracy: 72.516 | Loss: 0.7687\n",
      "   => Time: 43s\n",
      "   * EPOCH 66 | Accuracy: 76.744 | Loss: 0.6869\n",
      "   => Time: 43s\n",
      "   * EPOCH 67 | Accuracy: 73.996 | Loss: 0.7504\n",
      "   => Time: 43s\n",
      "   * EPOCH 68 | Accuracy: 75.899 | Loss: 0.7584\n",
      "   => Time: 43s\n",
      "   * EPOCH 69 | Accuracy: 73.784 | Loss: 0.7099\n",
      "   => Time: 44s\n",
      "   * EPOCH 70 | Accuracy: 70.402 | Loss: 0.8079\n",
      "   => Time: 44s\n",
      "   * EPOCH 71 | Accuracy: 73.150 | Loss: 0.7430\n",
      "   => Time: 45s\n",
      "   * EPOCH 72 | Accuracy: 73.996 | Loss: 0.7095\n",
      "   => Time: 44s\n",
      "   * EPOCH 73 | Accuracy: 73.784 | Loss: 0.7184\n",
      "   => Time: 43s\n",
      "   * EPOCH 74 | Accuracy: 71.459 | Loss: 0.7630\n",
      "   => Time: 44s\n",
      "   * EPOCH 75 | Accuracy: 72.093 | Loss: 0.8039\n",
      "   => Time: 43s\n",
      "   * EPOCH 76 | Accuracy: 74.630 | Loss: 0.7383\n",
      "   => Time: 44s\n",
      "   * EPOCH 77 | Accuracy: 74.630 | Loss: 0.7440\n",
      "   => Time: 43s\n",
      "   * EPOCH 78 | Accuracy: 74.419 | Loss: 0.7577\n",
      "   => Time: 43s\n",
      "   * EPOCH 79 | Accuracy: 72.093 | Loss: 0.7448\n",
      "   => Time: 43s\n",
      "   * EPOCH 80 | Accuracy: 72.093 | Loss: 0.7491\n",
      "   => Time: 43s\n",
      "   * EPOCH 81 | Accuracy: 75.053 | Loss: 0.7215\n",
      "   => Time: 43s\n",
      "   * EPOCH 82 | Accuracy: 68.499 | Loss: 0.7655\n",
      "   => Time: 43s\n",
      "   * EPOCH 83 | Accuracy: 74.419 | Loss: 0.7239\n",
      "   => Time: 43s\n",
      "   * EPOCH 84 | Accuracy: 73.150 | Loss: 0.7657\n",
      "   => Time: 43s\n",
      "   * EPOCH 85 | Accuracy: 74.419 | Loss: 0.7163\n",
      "   => Time: 44s\n",
      "   * EPOCH 86 | Accuracy: 74.841 | Loss: 0.7399\n",
      "   => Time: 43s\n",
      "   * EPOCH 87 | Accuracy: 73.784 | Loss: 0.7189\n",
      "   => Time: 43s\n",
      "   * EPOCH 88 | Accuracy: 73.150 | Loss: 0.7986\n",
      "   => Time: 43s\n",
      "   * EPOCH 89 | Accuracy: 73.362 | Loss: 0.7418\n",
      "   => Time: 43s\n",
      "   * EPOCH 90 | Accuracy: 72.304 | Loss: 0.7514\n",
      "   => Time: 43s\n",
      "   * EPOCH 91 | Accuracy: 76.744 | Loss: 0.6933\n",
      "   => Time: 44s\n",
      "   * EPOCH 92 | Accuracy: 74.419 | Loss: 0.7228\n",
      "   => Time: 45s\n",
      "   * EPOCH 93 | Accuracy: 70.402 | Loss: 0.8167\n",
      "   => Time: 43s\n",
      "   * EPOCH 94 | Accuracy: 72.304 | Loss: 0.7680\n",
      "   => Time: 43s\n",
      "   * EPOCH 95 | Accuracy: 69.133 | Loss: 0.8492\n",
      "   => Time: 43s\n",
      "   * EPOCH 96 | Accuracy: 73.784 | Loss: 0.7021\n",
      "   => Time: 43s\n",
      "   * EPOCH 97 | Accuracy: 73.362 | Loss: 0.7847\n",
      "   => Time: 43s\n",
      "   * EPOCH 98 | Accuracy: 71.247 | Loss: 0.7384\n",
      "   => Time: 43s\n",
      "   * EPOCH 99 | Accuracy: 71.459 | Loss: 0.7534\n",
      "   => Time: 43s\n",
      "   * EPOCH 100 | Accuracy: 71.670 | Loss: 0.8139\n",
      "   => Time: 43s\n"
     ]
    }
   ],
   "source": [
    "model_filepaths = []\n",
    "for i in range(args.cv):\n",
    "    modelbest_filename = \"{}_cv{}_b{}_best{}.pth.tar\".format(\n",
    "        args.arch, args.cv, args.batch_size, i)\n",
    "    best = train_best(args, traindir[i], valdir[i], modelbest_filename)\n",
    "    model_filepaths.append(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TestImageFolder(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        images = []\n",
    "        for filepath in sorted(glob.glob(root + \"/*.jpg\")):\n",
    "            images.append(filepath.split(\"/\")[-1])\n",
    "\n",
    "        self.root = root\n",
    "        self.imgs = images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filename = self.imgs[index]\n",
    "        img = Image.open(os.path.join(self.root, filename))\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, filename\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "test_loader = DataLoader(\n",
    "    TestImageFolder(testdir, \n",
    "                    transforms.Compose([\n",
    "                        transforms.Scale(400),\n",
    "                        transforms.RandomSizedCrop(224),\n",
    "                        transforms.RandomHorizontalFlip(),\n",
    "                        transforms.ToTensor(),\n",
    "                        normalize])),\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=args.workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(args, test_loader, model):\n",
    "    # placeholder arrays for predictions and id column\n",
    "    preds = np.zeros(shape=(len(test_loader), len(classes)))\n",
    "    id_col = []\n",
    "    \n",
    "    # turn off train mode\n",
    "    model.train(False)\n",
    "    \n",
    "    # average predictions across several different augmentations\n",
    "    for aug in range(args.nb_augs):\n",
    "        print(\"   * Predicting on test augmentation {}\".format(aug + 1))\n",
    "        \n",
    "        # iterate through image data, one file at a time\n",
    "        # (assuming batch size set to 1)\n",
    "        for i, (input, filename) in enumerate(test_loader):\n",
    "            # batch_size = 1\n",
    "            filename = filename[0]\n",
    "                     \n",
    "            if args.cuda:\n",
    "                input = input.cuda()\n",
    "            input_var = Variable(input, volatile=True) # no gradient\n",
    "            output = model(input_var)\n",
    "            softmax = F.softmax(output)[0].data.cpu().numpy()\n",
    "            \n",
    "            # add the scaled class probabilities\n",
    "            preds[i] += softmax\n",
    "            if aug == 0:\n",
    "                id_col.append(filename)\n",
    "       \n",
    "    # convert averaged prediction array to pandas dataframe\n",
    "    preds /= args.nb_augs\n",
    "    return preds, id_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_ensemble(args, test_loader, model, model_filepaths):\n",
    "    preds = np.zeros(shape=(len(test_loader), len(classes)))\n",
    "    for model_filepath in model_filepaths:\n",
    "        if os.path.isfile(model_filepath):\n",
    "            print(\"=> Loading checkpoint '{}'\".format(model_filepath))\n",
    "            checkpoint = torch.load(model_filepath)\n",
    "            model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "            print(\"=> Loaded checkpoint '{}' (epoch {}, loss {})\"\n",
    "                  .format(model_filepath, checkpoint[\"epoch\"],\n",
    "                          checkpoint[\"best_loss\"]))\n",
    "            pred, id_col = test(args, test_loader, model)\n",
    "            preds += pred\n",
    "        else:\n",
    "            print(\"=> No checkpoint found at '{}'\".format(model_filepath))\n",
    "            return None\n",
    "    preds /= len(model_filepaths)\n",
    "    pred_df = pd.DataFrame(preds, columns=[classes])\n",
    "    pred_df[\"image\"] = id_col\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Starting to test on 'resnet152' model\n",
      "=> Loading checkpoint '../intermediate/resnet152_cv8_b16_best0.pth.tar'\n",
      "=> Loaded checkpoint '../intermediate/resnet152_cv8_b16_best0.pth.tar' (epoch 63, loss 0.6652075572539184)\n",
      "   * Predicting on test augmentation 1\n",
      "   * Predicting on test augmentation 2\n",
      "   * Predicting on test augmentation 3\n",
      "   * Predicting on test augmentation 4\n",
      "   * Predicting on test augmentation 5\n",
      "   * Predicting on test augmentation 6\n",
      "   * Predicting on test augmentation 7\n",
      "   * Predicting on test augmentation 8\n",
      "   * Predicting on test augmentation 9\n",
      "   * Predicting on test augmentation 10\n",
      "=> Loading checkpoint '../intermediate/resnet152_cv8_b16_best1.pth.tar'\n",
      "=> Loaded checkpoint '../intermediate/resnet152_cv8_b16_best1.pth.tar' (epoch 62, loss 0.6558186881623026)\n",
      "   * Predicting on test augmentation 1\n",
      "   * Predicting on test augmentation 2\n",
      "   * Predicting on test augmentation 3\n",
      "   * Predicting on test augmentation 4\n",
      "   * Predicting on test augmentation 5\n",
      "   * Predicting on test augmentation 6\n",
      "   * Predicting on test augmentation 7\n",
      "   * Predicting on test augmentation 8\n",
      "   * Predicting on test augmentation 9\n",
      "   * Predicting on test augmentation 10\n",
      "=> Loading checkpoint '../intermediate/resnet152_cv8_b16_best2.pth.tar'\n",
      "=> Loaded checkpoint '../intermediate/resnet152_cv8_b16_best2.pth.tar' (epoch 97, loss 0.6760462077997499)\n",
      "   * Predicting on test augmentation 1\n",
      "   * Predicting on test augmentation 2\n",
      "   * Predicting on test augmentation 3\n",
      "   * Predicting on test augmentation 4\n",
      "   * Predicting on test augmentation 5\n",
      "   * Predicting on test augmentation 6\n",
      "   * Predicting on test augmentation 7\n",
      "   * Predicting on test augmentation 8\n",
      "   * Predicting on test augmentation 9\n",
      "   * Predicting on test augmentation 10\n",
      "=> Loading checkpoint '../intermediate/resnet152_cv8_b16_best3.pth.tar'\n",
      "=> Loaded checkpoint '../intermediate/resnet152_cv8_b16_best3.pth.tar' (epoch 50, loss 0.6854078789888802)\n",
      "   * Predicting on test augmentation 1\n",
      "   * Predicting on test augmentation 2\n",
      "   * Predicting on test augmentation 3\n",
      "   * Predicting on test augmentation 4\n",
      "   * Predicting on test augmentation 5\n",
      "   * Predicting on test augmentation 6\n",
      "   * Predicting on test augmentation 7\n",
      "   * Predicting on test augmentation 8\n",
      "   * Predicting on test augmentation 9\n",
      "   * Predicting on test augmentation 10\n",
      "=> Loading checkpoint '../intermediate/resnet152_cv8_b16_best4.pth.tar'\n",
      "=> Loaded checkpoint '../intermediate/resnet152_cv8_b16_best4.pth.tar' (epoch 45, loss 0.708232156806073)\n",
      "   * Predicting on test augmentation 1\n",
      "   * Predicting on test augmentation 2\n",
      "   * Predicting on test augmentation 3\n",
      "   * Predicting on test augmentation 4\n",
      "   * Predicting on test augmentation 5\n",
      "   * Predicting on test augmentation 6\n",
      "   * Predicting on test augmentation 7\n",
      "   * Predicting on test augmentation 8\n",
      "   * Predicting on test augmentation 9\n",
      "   * Predicting on test augmentation 10\n",
      "=> Loading checkpoint '../intermediate/resnet152_cv8_b16_best5.pth.tar'\n",
      "=> Loaded checkpoint '../intermediate/resnet152_cv8_b16_best5.pth.tar' (epoch 84, loss 0.6435169769545733)\n",
      "   * Predicting on test augmentation 1\n",
      "   * Predicting on test augmentation 2\n",
      "   * Predicting on test augmentation 3\n",
      "   * Predicting on test augmentation 4\n",
      "   * Predicting on test augmentation 5\n",
      "   * Predicting on test augmentation 6\n",
      "   * Predicting on test augmentation 7\n",
      "   * Predicting on test augmentation 8\n",
      "   * Predicting on test augmentation 9\n",
      "   * Predicting on test augmentation 10\n",
      "=> Loading checkpoint '../intermediate/resnet152_cv8_b16_best6.pth.tar'\n",
      "=> Loaded checkpoint '../intermediate/resnet152_cv8_b16_best6.pth.tar' (epoch 78, loss 0.6768846410815998)\n",
      "   * Predicting on test augmentation 1\n",
      "   * Predicting on test augmentation 2\n",
      "   * Predicting on test augmentation 3\n",
      "   * Predicting on test augmentation 4\n",
      "   * Predicting on test augmentation 5\n",
      "   * Predicting on test augmentation 6\n",
      "   * Predicting on test augmentation 7\n",
      "   * Predicting on test augmentation 8\n",
      "   * Predicting on test augmentation 9\n",
      "   * Predicting on test augmentation 10\n",
      "=> Loading checkpoint '../intermediate/resnet152_cv8_b16_best7.pth.tar'\n",
      "=> Loaded checkpoint '../intermediate/resnet152_cv8_b16_best7.pth.tar' (epoch 47, loss 0.665806553202502)\n",
      "   * Predicting on test augmentation 1\n",
      "   * Predicting on test augmentation 2\n",
      "   * Predicting on test augmentation 3\n",
      "   * Predicting on test augmentation 4\n",
      "   * Predicting on test augmentation 5\n",
      "   * Predicting on test augmentation 6\n",
      "   * Predicting on test augmentation 7\n",
      "   * Predicting on test augmentation 8\n",
      "   * Predicting on test augmentation 9\n",
      "   * Predicting on test augmentation 10\n"
     ]
    }
   ],
   "source": [
    "print(\"=> Starting to test on '{}' model\".format(args.arch))\n",
    "# create model\n",
    "if args.pretrained:\n",
    "    model = models.__dict__[args.arch](pretrained=True)\n",
    "else:\n",
    "    model = models.__dict__[args.arch]()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.fc = nn.Linear(2048, len(classes))\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "pred_df = test_ensemble(args, test_loader, model, model_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write predictions to csv\n",
    "if pred_df is not None:\n",
    "    sub_fn = \"{}_cv{}_b{}.csv\".format(\n",
    "        args.arch, args.cv, args.batch_size)\n",
    "    pred_df.to_csv(os.path.join(submission_path, sub_fn), index=False)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# validate function\n",
    "def predict(args, data_loader, model, criterion, epoch):\n",
    "    model.train(False) # turn off train mode\n",
    "    losses = AverageValueMeter()\n",
    "    top1 = ClassErrorMeter(accuracy=True)\n",
    "    \n",
    "    for i, (input, target) in enumerate(data_loader):\n",
    "        if args.cuda:\n",
    "            input = input.cuda(async=True)\n",
    "            target = target.cuda(async=True)\n",
    "        input_var = Variable(input, volatile=True) # no gradient\n",
    "        target_var = Variable(target, volatile=True)\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "        losses.add(loss.data[0] * input.size(0), input.size(0))\n",
    "        top1.add(output.data, target)\n",
    "        \n",
    "    print(\"   * EPOCH {:>2} | Accuracy: {:.3f} | Loss: {:.4f}\"\n",
    "          .format(epoch, top1.value()[0], losses.value()[0]))\n",
    "    return losses.value()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "        datasets.ImageFolder(traindir_full,\n",
    "                             transforms.Compose([\n",
    "                                 transforms.Scale(400),\n",
    "                                 transforms.RandomSizedCrop(224),\n",
    "                                 transforms.RandomHorizontalFlip(),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 normalize])),\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.workers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = glob.glob(traindir_full + \"/*/*.jpg\")\n",
    "t = dict()\n",
    "for file in g:\n",
    "    t[file.split(\"/\")[-1]] = file.split(\"/\")[-2]\n",
    "t = pd.DataFrame.from_dict(t, orient=\"index\").sort_index()\n",
    "t.columns = [\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datadir = os.path.join(intermediate_path, \"fulldata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(datadir):\n",
    "    os.makedirs(datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for file in g:\n",
    "    shutil.copy(file, datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "data_loader = DataLoader(\n",
    "    TestImageFolder(datadir, \n",
    "                    transforms.Compose([\n",
    "                        transforms.Scale(400),\n",
    "                        transforms.RandomSizedCrop(224),\n",
    "                        transforms.RandomHorizontalFlip(),\n",
    "                        transforms.ToTensor(),\n",
    "                        normalize])),\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=args.workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Starting to test on 'resnet152' model\n",
      "=> Loading checkpoint '../intermediate/resnet152_cv8_b16_best0.pth.tar'\n",
      "=> Loaded checkpoint '../intermediate/resnet152_cv8_b16_best0.pth.tar' (epoch 63, loss 0.6652075572539184)\n",
      "   * Predicting on test augmentation 1\n",
      "   * Predicting on test augmentation 2\n",
      "   * Predicting on test augmentation 3\n",
      "   * Predicting on test augmentation 4\n",
      "   * Predicting on test augmentation 5\n",
      "   * Predicting on test augmentation 6\n",
      "   * Predicting on test augmentation 7\n",
      "   * Predicting on test augmentation 8\n",
      "   * Predicting on test augmentation 9\n",
      "   * Predicting on test augmentation 10\n",
      "=> Loading checkpoint '../intermediate/resnet152_cv8_b16_best1.pth.tar'\n",
      "=> Loaded checkpoint '../intermediate/resnet152_cv8_b16_best1.pth.tar' (epoch 62, loss 0.6558186881623026)\n",
      "   * Predicting on test augmentation 1\n",
      "   * Predicting on test augmentation 2\n",
      "   * Predicting on test augmentation 3\n",
      "   * Predicting on test augmentation 4\n",
      "   * Predicting on test augmentation 5\n",
      "   * Predicting on test augmentation 6\n",
      "   * Predicting on test augmentation 7\n",
      "   * Predicting on test augmentation 8\n",
      "   * Predicting on test augmentation 9\n",
      "   * Predicting on test augmentation 10\n",
      "=> Loading checkpoint '../intermediate/resnet152_cv8_b16_best2.pth.tar'\n",
      "=> Loaded checkpoint '../intermediate/resnet152_cv8_b16_best2.pth.tar' (epoch 97, loss 0.6760462077997499)\n",
      "   * Predicting on test augmentation 1\n",
      "   * Predicting on test augmentation 2\n",
      "   * Predicting on test augmentation 3\n",
      "   * Predicting on test augmentation 4\n",
      "   * Predicting on test augmentation 5\n",
      "   * Predicting on test augmentation 6\n",
      "   * Predicting on test augmentation 7\n",
      "   * Predicting on test augmentation 8\n",
      "   * Predicting on test augmentation 9\n",
      "   * Predicting on test augmentation 10\n"
     ]
    }
   ],
   "source": [
    "print(\"=> Starting to test on '{}' model\".format(args.arch))\n",
    "# create model\n",
    "if args.pretrained:\n",
    "    model = models.__dict__[args.arch](pretrained=True)\n",
    "else:\n",
    "    model = models.__dict__[args.arch]()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.fc = nn.Linear(2048, len(classes))\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "# model_filepaths = ['../intermediate/resnet101_cv5_b16_best0.pth.tar',\n",
    "#                    '../intermediate/resnet101_cv5_b16_best1.pth.tar',\n",
    "#                    '../intermediate/resnet101_cv5_b16_best2.pth.tar',\n",
    "#                    '../intermediate/resnet101_cv5_b16_best3.pth.tar',\n",
    "#                    '../intermediate/resnet101_cv5_b16_best4.pth.tar']\n",
    "pred_df = test_ensemble(args, data_loader, model, model_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = pd.DataFrame(pred_df.set_index(\"image\").idxmax(axis=1).sort_index())\n",
    "p.columns = [\"label\"]\n",
    "p.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_classes = {x[1]: x[0] for x in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = [dict_classes[x] for x in t[\"label\"].values]\n",
    "pp = [dict_classes[x] for x in p[\"label\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ALB       0.81      0.96      0.88      1719\n",
      "        BET       0.88      0.63      0.73       200\n",
      "        DOL       0.80      0.83      0.82       117\n",
      "        LAG       1.00      0.99      0.99        67\n",
      "        NoF       0.97      0.78      0.87       465\n",
      "      OTHER       0.91      0.75      0.82       299\n",
      "      SHARK       0.91      0.97      0.94       176\n",
      "        YFT       0.88      0.73      0.80       734\n",
      "\n",
      "avg / total       0.86      0.86      0.85      3777\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAekAAAHKCAYAAAAn9rMPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa69a5a53c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(classification_report(tt, pp,\n",
    "                            target_names=classes))\n",
    "\n",
    "conf_mat = confusion_matrix(tt, pp)\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "width = conf_mat.shape[1]\n",
    "height = conf_mat.shape[0]\n",
    "\n",
    "res = plt.imshow(np.array(conf_mat), cmap=plt.cm.summer, interpolation='nearest')\n",
    "for i, row in enumerate(conf_mat):\n",
    "    for j, c in enumerate(row):\n",
    "        if c > 0:\n",
    "            plt.text(j , i, c, horizontalalignment=\"center\", fontsize=16)\n",
    "            \n",
    "cb = fig.colorbar(res)\n",
    "plt.title('Confusion Matrix')\n",
    "_ = plt.xticks(range(8), classes, rotation=90)\n",
    "_ = plt.yticks(range(8), classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"../intermediate/resnet152_prediction.pkl\", \"wb\") as f:\n",
    "    pickle.dump((t, p, pred_df), f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"../intermediate/resnet152_prediction.pkl\", \"rb\") as f:\n",
    "    t1, p1, pred_df1 =  pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALB</th>\n",
       "      <th>BET</th>\n",
       "      <th>DOL</th>\n",
       "      <th>LAG</th>\n",
       "      <th>NoF</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>SHARK</th>\n",
       "      <th>YFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>img_02271.jpg</th>\n",
       "      <td>0.159961</td>\n",
       "      <td>0.025183</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.006698</td>\n",
       "      <td>0.474273</td>\n",
       "      <td>0.058976</td>\n",
       "      <td>0.273606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_04131.jpg</th>\n",
       "      <td>0.167234</td>\n",
       "      <td>0.027533</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.005771</td>\n",
       "      <td>0.426788</td>\n",
       "      <td>0.070585</td>\n",
       "      <td>0.301625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_05685.jpg</th>\n",
       "      <td>0.189599</td>\n",
       "      <td>0.032845</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>0.415732</td>\n",
       "      <td>0.058164</td>\n",
       "      <td>0.295362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_05942.jpg</th>\n",
       "      <td>0.124096</td>\n",
       "      <td>0.027714</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>0.006940</td>\n",
       "      <td>0.571836</td>\n",
       "      <td>0.047188</td>\n",
       "      <td>0.221508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_06006.jpg</th>\n",
       "      <td>0.526943</td>\n",
       "      <td>0.287085</td>\n",
       "      <td>0.040153</td>\n",
       "      <td>0.002802</td>\n",
       "      <td>0.006354</td>\n",
       "      <td>0.026889</td>\n",
       "      <td>0.004351</td>\n",
       "      <td>0.105422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_06082.jpg</th>\n",
       "      <td>0.701710</td>\n",
       "      <td>0.056445</td>\n",
       "      <td>0.003191</td>\n",
       "      <td>0.019982</td>\n",
       "      <td>0.021256</td>\n",
       "      <td>0.101674</td>\n",
       "      <td>0.010896</td>\n",
       "      <td>0.084845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ALB       BET       DOL       LAG       NoF     OTHER  \\\n",
       "img_02271.jpg  0.159961  0.025183  0.001199  0.000104  0.006698  0.474273   \n",
       "img_04131.jpg  0.167234  0.027533  0.000357  0.000108  0.005771  0.426788   \n",
       "img_05685.jpg  0.189599  0.032845  0.000720  0.000304  0.007275  0.415732   \n",
       "img_05942.jpg  0.124096  0.027714  0.000351  0.000367  0.006940  0.571836   \n",
       "img_06006.jpg  0.526943  0.287085  0.040153  0.002802  0.006354  0.026889   \n",
       "img_06082.jpg  0.701710  0.056445  0.003191  0.019982  0.021256  0.101674   \n",
       "\n",
       "                  SHARK       YFT  \n",
       "img_02271.jpg  0.058976  0.273606  \n",
       "img_04131.jpg  0.070585  0.301625  \n",
       "img_05685.jpg  0.058164  0.295362  \n",
       "img_05942.jpg  0.047188  0.221508  \n",
       "img_06006.jpg  0.004351  0.105422  \n",
       "img_06082.jpg  0.010896  0.084845  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df1.set_index(\"image\").loc[t1[(t1[\"label\"] != p1[\"label\"]) & (t1[\"label\"] == \"SHARK\")].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALB</th>\n",
       "      <th>BET</th>\n",
       "      <th>DOL</th>\n",
       "      <th>LAG</th>\n",
       "      <th>NoF</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>SHARK</th>\n",
       "      <th>YFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>img_00379.jpg</th>\n",
       "      <td>0.395713</td>\n",
       "      <td>0.016171</td>\n",
       "      <td>0.004252</td>\n",
       "      <td>0.047692</td>\n",
       "      <td>0.368396</td>\n",
       "      <td>0.110072</td>\n",
       "      <td>0.002979</td>\n",
       "      <td>0.054726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_00435.jpg</th>\n",
       "      <td>0.603494</td>\n",
       "      <td>0.167979</td>\n",
       "      <td>0.005680</td>\n",
       "      <td>0.001985</td>\n",
       "      <td>0.003047</td>\n",
       "      <td>0.079044</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.138175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_00491.jpg</th>\n",
       "      <td>0.408152</td>\n",
       "      <td>0.353464</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.054282</td>\n",
       "      <td>0.069568</td>\n",
       "      <td>0.081962</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.031204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_00531.jpg</th>\n",
       "      <td>0.063676</td>\n",
       "      <td>0.272453</td>\n",
       "      <td>0.053974</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.012240</td>\n",
       "      <td>0.009484</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>0.587106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_00883.jpg</th>\n",
       "      <td>0.465342</td>\n",
       "      <td>0.120209</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.007259</td>\n",
       "      <td>0.104696</td>\n",
       "      <td>0.067466</td>\n",
       "      <td>0.233744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_01118.jpg</th>\n",
       "      <td>0.143321</td>\n",
       "      <td>0.027097</td>\n",
       "      <td>0.003027</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.460373</td>\n",
       "      <td>0.053714</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.311890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_01166.jpg</th>\n",
       "      <td>0.469974</td>\n",
       "      <td>0.336881</td>\n",
       "      <td>0.005936</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.025655</td>\n",
       "      <td>0.009244</td>\n",
       "      <td>0.005386</td>\n",
       "      <td>0.146171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_01215.jpg</th>\n",
       "      <td>0.306556</td>\n",
       "      <td>0.208700</td>\n",
       "      <td>0.009192</td>\n",
       "      <td>0.003803</td>\n",
       "      <td>0.071641</td>\n",
       "      <td>0.136666</td>\n",
       "      <td>0.002602</td>\n",
       "      <td>0.260839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_01299.jpg</th>\n",
       "      <td>0.041001</td>\n",
       "      <td>0.035562</td>\n",
       "      <td>0.391090</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.012277</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>0.001889</td>\n",
       "      <td>0.515048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_01450.jpg</th>\n",
       "      <td>0.088803</td>\n",
       "      <td>0.304980</td>\n",
       "      <td>0.078712</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.019770</td>\n",
       "      <td>0.013894</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.492755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_01477.jpg</th>\n",
       "      <td>0.481686</td>\n",
       "      <td>0.141394</td>\n",
       "      <td>0.006211</td>\n",
       "      <td>0.012438</td>\n",
       "      <td>0.040986</td>\n",
       "      <td>0.136342</td>\n",
       "      <td>0.012669</td>\n",
       "      <td>0.168275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_01696.jpg</th>\n",
       "      <td>0.466329</td>\n",
       "      <td>0.284772</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.019522</td>\n",
       "      <td>0.015956</td>\n",
       "      <td>0.002289</td>\n",
       "      <td>0.209547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_01707.jpg</th>\n",
       "      <td>0.048795</td>\n",
       "      <td>0.301104</td>\n",
       "      <td>0.059576</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.013092</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.569465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_01905.jpg</th>\n",
       "      <td>0.039014</td>\n",
       "      <td>0.036122</td>\n",
       "      <td>0.449892</td>\n",
       "      <td>0.000923</td>\n",
       "      <td>0.057541</td>\n",
       "      <td>0.001638</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>0.414315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_01912.jpg</th>\n",
       "      <td>0.520142</td>\n",
       "      <td>0.368246</td>\n",
       "      <td>0.007361</td>\n",
       "      <td>0.005940</td>\n",
       "      <td>0.005992</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>0.066394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_01943.jpg</th>\n",
       "      <td>0.655461</td>\n",
       "      <td>0.137827</td>\n",
       "      <td>0.012050</td>\n",
       "      <td>0.002201</td>\n",
       "      <td>0.032751</td>\n",
       "      <td>0.023453</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>0.135089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_02016.jpg</th>\n",
       "      <td>0.319100</td>\n",
       "      <td>0.284618</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.007950</td>\n",
       "      <td>0.050795</td>\n",
       "      <td>0.161637</td>\n",
       "      <td>0.174860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_02097.jpg</th>\n",
       "      <td>0.553023</td>\n",
       "      <td>0.328700</td>\n",
       "      <td>0.007210</td>\n",
       "      <td>0.007676</td>\n",
       "      <td>0.005937</td>\n",
       "      <td>0.025321</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.071840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_02128.jpg</th>\n",
       "      <td>0.278035</td>\n",
       "      <td>0.199340</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.001878</td>\n",
       "      <td>0.006921</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>0.080939</td>\n",
       "      <td>0.301015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_02138.jpg</th>\n",
       "      <td>0.643841</td>\n",
       "      <td>0.204931</td>\n",
       "      <td>0.006567</td>\n",
       "      <td>0.009743</td>\n",
       "      <td>0.022795</td>\n",
       "      <td>0.076779</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.034908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_02168.jpg</th>\n",
       "      <td>0.558423</td>\n",
       "      <td>0.268355</td>\n",
       "      <td>0.009104</td>\n",
       "      <td>0.033004</td>\n",
       "      <td>0.015842</td>\n",
       "      <td>0.027538</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.086802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_02196.jpg</th>\n",
       "      <td>0.669097</td>\n",
       "      <td>0.240239</td>\n",
       "      <td>0.001869</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.043294</td>\n",
       "      <td>0.005448</td>\n",
       "      <td>0.006925</td>\n",
       "      <td>0.033086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_02647.jpg</th>\n",
       "      <td>0.184003</td>\n",
       "      <td>0.036024</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.011393</td>\n",
       "      <td>0.313806</td>\n",
       "      <td>0.186737</td>\n",
       "      <td>0.267092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_02742.jpg</th>\n",
       "      <td>0.753154</td>\n",
       "      <td>0.095205</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.007746</td>\n",
       "      <td>0.073269</td>\n",
       "      <td>0.000659</td>\n",
       "      <td>0.067017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_02981.jpg</th>\n",
       "      <td>0.323679</td>\n",
       "      <td>0.302829</td>\n",
       "      <td>0.002664</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.019059</td>\n",
       "      <td>0.017713</td>\n",
       "      <td>0.003492</td>\n",
       "      <td>0.329618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_03287.jpg</th>\n",
       "      <td>0.412695</td>\n",
       "      <td>0.352162</td>\n",
       "      <td>0.014284</td>\n",
       "      <td>0.004644</td>\n",
       "      <td>0.014562</td>\n",
       "      <td>0.123882</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.076981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_03315.jpg</th>\n",
       "      <td>0.366185</td>\n",
       "      <td>0.061858</td>\n",
       "      <td>0.004552</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.065433</td>\n",
       "      <td>0.051546</td>\n",
       "      <td>0.003934</td>\n",
       "      <td>0.445881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_03453.jpg</th>\n",
       "      <td>0.309263</td>\n",
       "      <td>0.219912</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.012099</td>\n",
       "      <td>0.089378</td>\n",
       "      <td>0.152831</td>\n",
       "      <td>0.215739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_03565.jpg</th>\n",
       "      <td>0.608215</td>\n",
       "      <td>0.236146</td>\n",
       "      <td>0.010506</td>\n",
       "      <td>0.030861</td>\n",
       "      <td>0.012095</td>\n",
       "      <td>0.026985</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>0.073714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_03647.jpg</th>\n",
       "      <td>0.325046</td>\n",
       "      <td>0.192994</td>\n",
       "      <td>0.015492</td>\n",
       "      <td>0.023834</td>\n",
       "      <td>0.034683</td>\n",
       "      <td>0.185911</td>\n",
       "      <td>0.017004</td>\n",
       "      <td>0.205036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_04670.jpg</th>\n",
       "      <td>0.389124</td>\n",
       "      <td>0.380099</td>\n",
       "      <td>0.018219</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.022409</td>\n",
       "      <td>0.140363</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.046249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_04672.jpg</th>\n",
       "      <td>0.097206</td>\n",
       "      <td>0.295699</td>\n",
       "      <td>0.073754</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.031515</td>\n",
       "      <td>0.013331</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.486848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_04853.jpg</th>\n",
       "      <td>0.357001</td>\n",
       "      <td>0.262047</td>\n",
       "      <td>0.007457</td>\n",
       "      <td>0.006187</td>\n",
       "      <td>0.052442</td>\n",
       "      <td>0.140748</td>\n",
       "      <td>0.015837</td>\n",
       "      <td>0.158281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_04891.jpg</th>\n",
       "      <td>0.286816</td>\n",
       "      <td>0.163442</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.013563</td>\n",
       "      <td>0.016535</td>\n",
       "      <td>0.393588</td>\n",
       "      <td>0.012350</td>\n",
       "      <td>0.113028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_04914.jpg</th>\n",
       "      <td>0.247547</td>\n",
       "      <td>0.264667</td>\n",
       "      <td>0.007578</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>0.064239</td>\n",
       "      <td>0.112687</td>\n",
       "      <td>0.001569</td>\n",
       "      <td>0.300290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_05069.jpg</th>\n",
       "      <td>0.664352</td>\n",
       "      <td>0.022131</td>\n",
       "      <td>0.005646</td>\n",
       "      <td>0.015861</td>\n",
       "      <td>0.091326</td>\n",
       "      <td>0.130085</td>\n",
       "      <td>0.004484</td>\n",
       "      <td>0.066116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_05164.jpg</th>\n",
       "      <td>0.762337</td>\n",
       "      <td>0.039367</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>0.045649</td>\n",
       "      <td>0.077090</td>\n",
       "      <td>0.002943</td>\n",
       "      <td>0.054389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_05194.jpg</th>\n",
       "      <td>0.263910</td>\n",
       "      <td>0.186751</td>\n",
       "      <td>0.010837</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>0.061899</td>\n",
       "      <td>0.011522</td>\n",
       "      <td>0.308408</td>\n",
       "      <td>0.155944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_05262.jpg</th>\n",
       "      <td>0.628189</td>\n",
       "      <td>0.253618</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.069284</td>\n",
       "      <td>0.007074</td>\n",
       "      <td>0.001437</td>\n",
       "      <td>0.040010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_05344.jpg</th>\n",
       "      <td>0.166459</td>\n",
       "      <td>0.038711</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.014996</td>\n",
       "      <td>0.324141</td>\n",
       "      <td>0.212567</td>\n",
       "      <td>0.242439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_05346.jpg</th>\n",
       "      <td>0.459701</td>\n",
       "      <td>0.358904</td>\n",
       "      <td>0.006648</td>\n",
       "      <td>0.016639</td>\n",
       "      <td>0.025224</td>\n",
       "      <td>0.040251</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.091354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_05369.jpg</th>\n",
       "      <td>0.446500</td>\n",
       "      <td>0.362104</td>\n",
       "      <td>0.004633</td>\n",
       "      <td>0.002977</td>\n",
       "      <td>0.023768</td>\n",
       "      <td>0.037217</td>\n",
       "      <td>0.003189</td>\n",
       "      <td>0.119612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_05450.jpg</th>\n",
       "      <td>0.549871</td>\n",
       "      <td>0.069353</td>\n",
       "      <td>0.018604</td>\n",
       "      <td>0.081774</td>\n",
       "      <td>0.106677</td>\n",
       "      <td>0.131841</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>0.039749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_05658.jpg</th>\n",
       "      <td>0.077434</td>\n",
       "      <td>0.338904</td>\n",
       "      <td>0.039937</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.009507</td>\n",
       "      <td>0.018835</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.514723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_05797.jpg</th>\n",
       "      <td>0.301040</td>\n",
       "      <td>0.260039</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.016437</td>\n",
       "      <td>0.115507</td>\n",
       "      <td>0.182605</td>\n",
       "      <td>0.123549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_06035.jpg</th>\n",
       "      <td>0.463882</td>\n",
       "      <td>0.023773</td>\n",
       "      <td>0.004905</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.024460</td>\n",
       "      <td>0.042221</td>\n",
       "      <td>0.013530</td>\n",
       "      <td>0.426531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_06287.jpg</th>\n",
       "      <td>0.578859</td>\n",
       "      <td>0.043541</td>\n",
       "      <td>0.024290</td>\n",
       "      <td>0.049463</td>\n",
       "      <td>0.029343</td>\n",
       "      <td>0.142590</td>\n",
       "      <td>0.001469</td>\n",
       "      <td>0.130445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_06532.jpg</th>\n",
       "      <td>0.169508</td>\n",
       "      <td>0.019610</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.008590</td>\n",
       "      <td>0.241253</td>\n",
       "      <td>0.310823</td>\n",
       "      <td>0.249647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_06613.jpg</th>\n",
       "      <td>0.254292</td>\n",
       "      <td>0.270774</td>\n",
       "      <td>0.004936</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>0.077809</td>\n",
       "      <td>0.095071</td>\n",
       "      <td>0.001707</td>\n",
       "      <td>0.294125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_06623.jpg</th>\n",
       "      <td>0.367637</td>\n",
       "      <td>0.217414</td>\n",
       "      <td>0.009506</td>\n",
       "      <td>0.013246</td>\n",
       "      <td>0.051375</td>\n",
       "      <td>0.126799</td>\n",
       "      <td>0.014018</td>\n",
       "      <td>0.200004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_06636.jpg</th>\n",
       "      <td>0.779146</td>\n",
       "      <td>0.098250</td>\n",
       "      <td>0.005760</td>\n",
       "      <td>0.000769</td>\n",
       "      <td>0.005893</td>\n",
       "      <td>0.065466</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>0.044110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_06777.jpg</th>\n",
       "      <td>0.676803</td>\n",
       "      <td>0.215106</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.053670</td>\n",
       "      <td>0.012111</td>\n",
       "      <td>0.001738</td>\n",
       "      <td>0.038577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_06825.jpg</th>\n",
       "      <td>0.416840</td>\n",
       "      <td>0.342973</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.061774</td>\n",
       "      <td>0.009649</td>\n",
       "      <td>0.006327</td>\n",
       "      <td>0.160652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_06827.jpg</th>\n",
       "      <td>0.495442</td>\n",
       "      <td>0.389031</td>\n",
       "      <td>0.005055</td>\n",
       "      <td>0.005572</td>\n",
       "      <td>0.017109</td>\n",
       "      <td>0.025489</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.060889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_06867.jpg</th>\n",
       "      <td>0.218374</td>\n",
       "      <td>0.164249</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.023651</td>\n",
       "      <td>0.011458</td>\n",
       "      <td>0.453216</td>\n",
       "      <td>0.013711</td>\n",
       "      <td>0.114822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_06983.jpg</th>\n",
       "      <td>0.459067</td>\n",
       "      <td>0.253417</td>\n",
       "      <td>0.016872</td>\n",
       "      <td>0.005337</td>\n",
       "      <td>0.032857</td>\n",
       "      <td>0.178937</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>0.052028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_07321.jpg</th>\n",
       "      <td>0.064933</td>\n",
       "      <td>0.353479</td>\n",
       "      <td>0.050878</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.027767</td>\n",
       "      <td>0.011101</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.490807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_07378.jpg</th>\n",
       "      <td>0.468241</td>\n",
       "      <td>0.351398</td>\n",
       "      <td>0.004922</td>\n",
       "      <td>0.001225</td>\n",
       "      <td>0.072502</td>\n",
       "      <td>0.003641</td>\n",
       "      <td>0.009256</td>\n",
       "      <td>0.088814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_07545.jpg</th>\n",
       "      <td>0.321095</td>\n",
       "      <td>0.118545</td>\n",
       "      <td>0.006475</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>0.015783</td>\n",
       "      <td>0.018169</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>0.517988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_07795.jpg</th>\n",
       "      <td>0.286686</td>\n",
       "      <td>0.152543</td>\n",
       "      <td>0.011893</td>\n",
       "      <td>0.005120</td>\n",
       "      <td>0.088096</td>\n",
       "      <td>0.142414</td>\n",
       "      <td>0.002147</td>\n",
       "      <td>0.311101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ALB       BET       DOL       LAG       NoF     OTHER  \\\n",
       "img_00379.jpg  0.395713  0.016171  0.004252  0.047692  0.368396  0.110072   \n",
       "img_00435.jpg  0.603494  0.167979  0.005680  0.001985  0.003047  0.079044   \n",
       "img_00491.jpg  0.408152  0.353464  0.000670  0.054282  0.069568  0.081962   \n",
       "img_00531.jpg  0.063676  0.272453  0.053974  0.000451  0.012240  0.009484   \n",
       "img_00883.jpg  0.465342  0.120209  0.000404  0.000881  0.007259  0.104696   \n",
       "img_01118.jpg  0.143321  0.027097  0.003027  0.000082  0.460373  0.053714   \n",
       "img_01166.jpg  0.469974  0.336881  0.005936  0.000753  0.025655  0.009244   \n",
       "img_01215.jpg  0.306556  0.208700  0.009192  0.003803  0.071641  0.136666   \n",
       "img_01299.jpg  0.041001  0.035562  0.391090  0.000368  0.012277  0.002765   \n",
       "img_01450.jpg  0.088803  0.304980  0.078712  0.000483  0.019770  0.013894   \n",
       "img_01477.jpg  0.481686  0.141394  0.006211  0.012438  0.040986  0.136342   \n",
       "img_01696.jpg  0.466329  0.284772  0.001100  0.000486  0.019522  0.015956   \n",
       "img_01707.jpg  0.048795  0.301104  0.059576  0.000338  0.013092  0.007143   \n",
       "img_01905.jpg  0.039014  0.036122  0.449892  0.000923  0.057541  0.001638   \n",
       "img_01912.jpg  0.520142  0.368246  0.007361  0.005940  0.005992  0.025200   \n",
       "img_01943.jpg  0.655461  0.137827  0.012050  0.002201  0.032751  0.023453   \n",
       "img_02016.jpg  0.319100  0.284618  0.000665  0.000374  0.007950  0.050795   \n",
       "img_02097.jpg  0.553023  0.328700  0.007210  0.007676  0.005937  0.025321   \n",
       "img_02128.jpg  0.278035  0.199340  0.000724  0.001878  0.006921  0.131148   \n",
       "img_02138.jpg  0.643841  0.204931  0.006567  0.009743  0.022795  0.076779   \n",
       "img_02168.jpg  0.558423  0.268355  0.009104  0.033004  0.015842  0.027538   \n",
       "img_02196.jpg  0.669097  0.240239  0.001869  0.000041  0.043294  0.005448   \n",
       "img_02647.jpg  0.184003  0.036024  0.000595  0.000350  0.011393  0.313806   \n",
       "img_02742.jpg  0.753154  0.095205  0.001649  0.001300  0.007746  0.073269   \n",
       "img_02981.jpg  0.323679  0.302829  0.002664  0.000946  0.019059  0.017713   \n",
       "img_03287.jpg  0.412695  0.352162  0.014284  0.004644  0.014562  0.123882   \n",
       "img_03315.jpg  0.366185  0.061858  0.004552  0.000612  0.065433  0.051546   \n",
       "img_03453.jpg  0.309263  0.219912  0.000610  0.000168  0.012099  0.089378   \n",
       "img_03565.jpg  0.608215  0.236146  0.010506  0.030861  0.012095  0.026985   \n",
       "img_03647.jpg  0.325046  0.192994  0.015492  0.023834  0.034683  0.185911   \n",
       "...                 ...       ...       ...       ...       ...       ...   \n",
       "img_04670.jpg  0.389124  0.380099  0.018219  0.002123  0.022409  0.140363   \n",
       "img_04672.jpg  0.097206  0.295699  0.073754  0.001109  0.031515  0.013331   \n",
       "img_04853.jpg  0.357001  0.262047  0.007457  0.006187  0.052442  0.140748   \n",
       "img_04891.jpg  0.286816  0.163442  0.000678  0.013563  0.016535  0.393588   \n",
       "img_04914.jpg  0.247547  0.264667  0.007578  0.001423  0.064239  0.112687   \n",
       "img_05069.jpg  0.664352  0.022131  0.005646  0.015861  0.091326  0.130085   \n",
       "img_05164.jpg  0.762337  0.039367  0.001023  0.017202  0.045649  0.077090   \n",
       "img_05194.jpg  0.263910  0.186751  0.010837  0.000729  0.061899  0.011522   \n",
       "img_05262.jpg  0.628189  0.253618  0.000318  0.000070  0.069284  0.007074   \n",
       "img_05344.jpg  0.166459  0.038711  0.000423  0.000263  0.014996  0.324141   \n",
       "img_05346.jpg  0.459701  0.358904  0.006648  0.016639  0.025224  0.040251   \n",
       "img_05369.jpg  0.446500  0.362104  0.004633  0.002977  0.023768  0.037217   \n",
       "img_05450.jpg  0.549871  0.069353  0.018604  0.081774  0.106677  0.131841   \n",
       "img_05658.jpg  0.077434  0.338904  0.039937  0.000206  0.009507  0.018835   \n",
       "img_05797.jpg  0.301040  0.260039  0.000629  0.000193  0.016437  0.115507   \n",
       "img_06035.jpg  0.463882  0.023773  0.004905  0.000698  0.024460  0.042221   \n",
       "img_06287.jpg  0.578859  0.043541  0.024290  0.049463  0.029343  0.142590   \n",
       "img_06532.jpg  0.169508  0.019610  0.000385  0.000184  0.008590  0.241253   \n",
       "img_06613.jpg  0.254292  0.270774  0.004936  0.001287  0.077809  0.095071   \n",
       "img_06623.jpg  0.367637  0.217414  0.009506  0.013246  0.051375  0.126799   \n",
       "img_06636.jpg  0.779146  0.098250  0.005760  0.000769  0.005893  0.065466   \n",
       "img_06777.jpg  0.676803  0.215106  0.001825  0.000171  0.053670  0.012111   \n",
       "img_06825.jpg  0.416840  0.342973  0.001411  0.000374  0.061774  0.009649   \n",
       "img_06827.jpg  0.495442  0.389031  0.005055  0.005572  0.017109  0.025489   \n",
       "img_06867.jpg  0.218374  0.164249  0.000518  0.023651  0.011458  0.453216   \n",
       "img_06983.jpg  0.459067  0.253417  0.016872  0.005337  0.032857  0.178937   \n",
       "img_07321.jpg  0.064933  0.353479  0.050878  0.000624  0.027767  0.011101   \n",
       "img_07378.jpg  0.468241  0.351398  0.004922  0.001225  0.072502  0.003641   \n",
       "img_07545.jpg  0.321095  0.118545  0.006475  0.000720  0.015783  0.018169   \n",
       "img_07795.jpg  0.286686  0.152543  0.011893  0.005120  0.088096  0.142414   \n",
       "\n",
       "                  SHARK       YFT  \n",
       "img_00379.jpg  0.002979  0.054726  \n",
       "img_00435.jpg  0.000596  0.138175  \n",
       "img_00491.jpg  0.000698  0.031204  \n",
       "img_00531.jpg  0.000616  0.587106  \n",
       "img_00883.jpg  0.067466  0.233744  \n",
       "img_01118.jpg  0.000496  0.311890  \n",
       "img_01166.jpg  0.005386  0.146171  \n",
       "img_01215.jpg  0.002602  0.260839  \n",
       "img_01299.jpg  0.001889  0.515048  \n",
       "img_01450.jpg  0.000605  0.492755  \n",
       "img_01477.jpg  0.012669  0.168275  \n",
       "img_01696.jpg  0.002289  0.209547  \n",
       "img_01707.jpg  0.000487  0.569465  \n",
       "img_01905.jpg  0.000555  0.414315  \n",
       "img_01912.jpg  0.000725  0.066394  \n",
       "img_01943.jpg  0.001168  0.135089  \n",
       "img_02016.jpg  0.161637  0.174860  \n",
       "img_02097.jpg  0.000293  0.071840  \n",
       "img_02128.jpg  0.080939  0.301015  \n",
       "img_02138.jpg  0.000435  0.034908  \n",
       "img_02168.jpg  0.000932  0.086802  \n",
       "img_02196.jpg  0.006925  0.033086  \n",
       "img_02647.jpg  0.186737  0.267092  \n",
       "img_02742.jpg  0.000659  0.067017  \n",
       "img_02981.jpg  0.003492  0.329618  \n",
       "img_03287.jpg  0.000790  0.076981  \n",
       "img_03315.jpg  0.003934  0.445881  \n",
       "img_03453.jpg  0.152831  0.215739  \n",
       "img_03565.jpg  0.001478  0.073714  \n",
       "img_03647.jpg  0.017004  0.205036  \n",
       "...                 ...       ...  \n",
       "img_04670.jpg  0.001413  0.046249  \n",
       "img_04672.jpg  0.000537  0.486848  \n",
       "img_04853.jpg  0.015837  0.158281  \n",
       "img_04891.jpg  0.012350  0.113028  \n",
       "img_04914.jpg  0.001569  0.300290  \n",
       "img_05069.jpg  0.004484  0.066116  \n",
       "img_05164.jpg  0.002943  0.054389  \n",
       "img_05194.jpg  0.308408  0.155944  \n",
       "img_05262.jpg  0.001437  0.040010  \n",
       "img_05344.jpg  0.212567  0.242439  \n",
       "img_05346.jpg  0.001280  0.091354  \n",
       "img_05369.jpg  0.003189  0.119612  \n",
       "img_05450.jpg  0.002130  0.039749  \n",
       "img_05658.jpg  0.000453  0.514723  \n",
       "img_05797.jpg  0.182605  0.123549  \n",
       "img_06035.jpg  0.013530  0.426531  \n",
       "img_06287.jpg  0.001469  0.130445  \n",
       "img_06532.jpg  0.310823  0.249647  \n",
       "img_06613.jpg  0.001707  0.294125  \n",
       "img_06623.jpg  0.014018  0.200004  \n",
       "img_06636.jpg  0.000606  0.044110  \n",
       "img_06777.jpg  0.001738  0.038577  \n",
       "img_06825.jpg  0.006327  0.160652  \n",
       "img_06827.jpg  0.001413  0.060889  \n",
       "img_06867.jpg  0.013711  0.114822  \n",
       "img_06983.jpg  0.001485  0.052028  \n",
       "img_07321.jpg  0.000412  0.490807  \n",
       "img_07378.jpg  0.009256  0.088814  \n",
       "img_07545.jpg  0.001224  0.517988  \n",
       "img_07795.jpg  0.002147  0.311101  \n",
       "\n",
       "[74 rows x 8 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df1.set_index(\"image\").loc[t1[(t1[\"label\"] != p1[\"label\"]) & (t1[\"label\"] == \"BET\")].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALB</th>\n",
       "      <th>BET</th>\n",
       "      <th>DOL</th>\n",
       "      <th>LAG</th>\n",
       "      <th>NoF</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>SHARK</th>\n",
       "      <th>YFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>img_07817.jpg</th>\n",
       "      <td>0.869566</td>\n",
       "      <td>0.01673</td>\n",
       "      <td>0.004796</td>\n",
       "      <td>0.044627</td>\n",
       "      <td>0.018501</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.023767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ALB      BET       DOL       LAG       NoF     OTHER  \\\n",
       "img_07817.jpg  0.869566  0.01673  0.004796  0.044627  0.018501  0.021739   \n",
       "\n",
       "                  SHARK       YFT  \n",
       "img_07817.jpg  0.000274  0.023767  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df1.set_index(\"image\").loc[t1[(t1[\"label\"] != p1[\"label\"]) & (t1[\"label\"] == \"LAG\")].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALB</th>\n",
       "      <th>BET</th>\n",
       "      <th>DOL</th>\n",
       "      <th>LAG</th>\n",
       "      <th>NoF</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>SHARK</th>\n",
       "      <th>YFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>img_00165.jpg</th>\n",
       "      <td>0.574717</td>\n",
       "      <td>0.031825</td>\n",
       "      <td>0.237246</td>\n",
       "      <td>0.007847</td>\n",
       "      <td>0.030876</td>\n",
       "      <td>0.056571</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.060203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_01311.jpg</th>\n",
       "      <td>0.520977</td>\n",
       "      <td>0.065573</td>\n",
       "      <td>0.217117</td>\n",
       "      <td>0.009072</td>\n",
       "      <td>0.035538</td>\n",
       "      <td>0.083177</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>0.067104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_01471.jpg</th>\n",
       "      <td>0.056272</td>\n",
       "      <td>0.015766</td>\n",
       "      <td>0.366781</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.024903</td>\n",
       "      <td>0.017368</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.517457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_01757.jpg</th>\n",
       "      <td>0.638906</td>\n",
       "      <td>0.061735</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.005148</td>\n",
       "      <td>0.028785</td>\n",
       "      <td>0.078134</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.056791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_01812.jpg</th>\n",
       "      <td>0.372514</td>\n",
       "      <td>0.035319</td>\n",
       "      <td>0.372362</td>\n",
       "      <td>0.005681</td>\n",
       "      <td>0.022276</td>\n",
       "      <td>0.098504</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.092531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_02100.jpg</th>\n",
       "      <td>0.632069</td>\n",
       "      <td>0.041884</td>\n",
       "      <td>0.131085</td>\n",
       "      <td>0.004509</td>\n",
       "      <td>0.016551</td>\n",
       "      <td>0.116094</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.057544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_02333.jpg</th>\n",
       "      <td>0.056499</td>\n",
       "      <td>0.028042</td>\n",
       "      <td>0.314321</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.008546</td>\n",
       "      <td>0.021979</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.570014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_02798.jpg</th>\n",
       "      <td>0.145158</td>\n",
       "      <td>0.045632</td>\n",
       "      <td>0.279840</td>\n",
       "      <td>0.003301</td>\n",
       "      <td>0.036706</td>\n",
       "      <td>0.052965</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.432697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_04676.jpg</th>\n",
       "      <td>0.063606</td>\n",
       "      <td>0.028433</td>\n",
       "      <td>0.372511</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.026750</td>\n",
       "      <td>0.017522</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.490543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_04824.jpg</th>\n",
       "      <td>0.419283</td>\n",
       "      <td>0.061777</td>\n",
       "      <td>0.325979</td>\n",
       "      <td>0.008111</td>\n",
       "      <td>0.020039</td>\n",
       "      <td>0.062972</td>\n",
       "      <td>0.002510</td>\n",
       "      <td>0.099330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_05367.jpg</th>\n",
       "      <td>0.638616</td>\n",
       "      <td>0.032541</td>\n",
       "      <td>0.149411</td>\n",
       "      <td>0.003325</td>\n",
       "      <td>0.035021</td>\n",
       "      <td>0.089043</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.051933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_05420.jpg</th>\n",
       "      <td>0.076013</td>\n",
       "      <td>0.032263</td>\n",
       "      <td>0.405624</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.047134</td>\n",
       "      <td>0.017535</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.420956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_05444.jpg</th>\n",
       "      <td>0.738310</td>\n",
       "      <td>0.026906</td>\n",
       "      <td>0.024726</td>\n",
       "      <td>0.050512</td>\n",
       "      <td>0.065626</td>\n",
       "      <td>0.062752</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.030734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_05557.jpg</th>\n",
       "      <td>0.582999</td>\n",
       "      <td>0.063136</td>\n",
       "      <td>0.075989</td>\n",
       "      <td>0.006908</td>\n",
       "      <td>0.023639</td>\n",
       "      <td>0.198160</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.048715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_05934.jpg</th>\n",
       "      <td>0.512672</td>\n",
       "      <td>0.038996</td>\n",
       "      <td>0.292534</td>\n",
       "      <td>0.008099</td>\n",
       "      <td>0.039262</td>\n",
       "      <td>0.052066</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.055925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_06117.jpg</th>\n",
       "      <td>0.593896</td>\n",
       "      <td>0.064133</td>\n",
       "      <td>0.095135</td>\n",
       "      <td>0.027977</td>\n",
       "      <td>0.037612</td>\n",
       "      <td>0.126588</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.054288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_06204.jpg</th>\n",
       "      <td>0.422789</td>\n",
       "      <td>0.038703</td>\n",
       "      <td>0.342214</td>\n",
       "      <td>0.009818</td>\n",
       "      <td>0.020751</td>\n",
       "      <td>0.085779</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.079520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_06773.jpg</th>\n",
       "      <td>0.062268</td>\n",
       "      <td>0.020679</td>\n",
       "      <td>0.385133</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.019737</td>\n",
       "      <td>0.016644</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.494651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_07212.jpg</th>\n",
       "      <td>0.638361</td>\n",
       "      <td>0.056866</td>\n",
       "      <td>0.011071</td>\n",
       "      <td>0.012249</td>\n",
       "      <td>0.040141</td>\n",
       "      <td>0.086763</td>\n",
       "      <td>0.010828</td>\n",
       "      <td>0.143721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_07643.jpg</th>\n",
       "      <td>0.496835</td>\n",
       "      <td>0.035609</td>\n",
       "      <td>0.262703</td>\n",
       "      <td>0.006122</td>\n",
       "      <td>0.031643</td>\n",
       "      <td>0.100236</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.066040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ALB       BET       DOL       LAG       NoF     OTHER  \\\n",
       "img_00165.jpg  0.574717  0.031825  0.237246  0.007847  0.030876  0.056571   \n",
       "img_01311.jpg  0.520977  0.065573  0.217117  0.009072  0.035538  0.083177   \n",
       "img_01471.jpg  0.056272  0.015766  0.366781  0.000288  0.024903  0.017368   \n",
       "img_01757.jpg  0.638906  0.061735  0.130400  0.005148  0.028785  0.078134   \n",
       "img_01812.jpg  0.372514  0.035319  0.372362  0.005681  0.022276  0.098504   \n",
       "img_02100.jpg  0.632069  0.041884  0.131085  0.004509  0.016551  0.116094   \n",
       "img_02333.jpg  0.056499  0.028042  0.314321  0.000105  0.008546  0.021979   \n",
       "img_02798.jpg  0.145158  0.045632  0.279840  0.003301  0.036706  0.052965   \n",
       "img_04676.jpg  0.063606  0.028433  0.372511  0.000140  0.026750  0.017522   \n",
       "img_04824.jpg  0.419283  0.061777  0.325979  0.008111  0.020039  0.062972   \n",
       "img_05367.jpg  0.638616  0.032541  0.149411  0.003325  0.035021  0.089043   \n",
       "img_05420.jpg  0.076013  0.032263  0.405624  0.000074  0.047134  0.017535   \n",
       "img_05444.jpg  0.738310  0.026906  0.024726  0.050512  0.065626  0.062752   \n",
       "img_05557.jpg  0.582999  0.063136  0.075989  0.006908  0.023639  0.198160   \n",
       "img_05934.jpg  0.512672  0.038996  0.292534  0.008099  0.039262  0.052066   \n",
       "img_06117.jpg  0.593896  0.064133  0.095135  0.027977  0.037612  0.126588   \n",
       "img_06204.jpg  0.422789  0.038703  0.342214  0.009818  0.020751  0.085779   \n",
       "img_06773.jpg  0.062268  0.020679  0.385133  0.000078  0.019737  0.016644   \n",
       "img_07212.jpg  0.638361  0.056866  0.011071  0.012249  0.040141  0.086763   \n",
       "img_07643.jpg  0.496835  0.035609  0.262703  0.006122  0.031643  0.100236   \n",
       "\n",
       "                  SHARK       YFT  \n",
       "img_00165.jpg  0.000714  0.060203  \n",
       "img_01311.jpg  0.001441  0.067104  \n",
       "img_01471.jpg  0.001165  0.517457  \n",
       "img_01757.jpg  0.000101  0.056791  \n",
       "img_01812.jpg  0.000814  0.092531  \n",
       "img_02100.jpg  0.000265  0.057544  \n",
       "img_02333.jpg  0.000493  0.570014  \n",
       "img_02798.jpg  0.003700  0.432697  \n",
       "img_04676.jpg  0.000496  0.490543  \n",
       "img_04824.jpg  0.002510  0.099330  \n",
       "img_05367.jpg  0.000111  0.051933  \n",
       "img_05420.jpg  0.000402  0.420956  \n",
       "img_05444.jpg  0.000435  0.030734  \n",
       "img_05557.jpg  0.000454  0.048715  \n",
       "img_05934.jpg  0.000447  0.055925  \n",
       "img_06117.jpg  0.000371  0.054288  \n",
       "img_06204.jpg  0.000426  0.079520  \n",
       "img_06773.jpg  0.000811  0.494651  \n",
       "img_07212.jpg  0.010828  0.143721  \n",
       "img_07643.jpg  0.000813  0.066040  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df1.set_index(\"image\").loc[t1[(t1[\"label\"] != p1[\"label\"]) & (t1[\"label\"] == \"DOL\")].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALB</th>\n",
       "      <th>BET</th>\n",
       "      <th>DOL</th>\n",
       "      <th>LAG</th>\n",
       "      <th>NoF</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>SHARK</th>\n",
       "      <th>YFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>img_00136.jpg</th>\n",
       "      <td>0.269772</td>\n",
       "      <td>0.311739</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.005435</td>\n",
       "      <td>0.026962</td>\n",
       "      <td>0.007731</td>\n",
       "      <td>0.345697</td>\n",
       "      <td>0.032268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_00248.jpg</th>\n",
       "      <td>0.255272</td>\n",
       "      <td>0.370278</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>0.008642</td>\n",
       "      <td>0.006702</td>\n",
       "      <td>0.322051</td>\n",
       "      <td>0.034654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_00288.jpg</th>\n",
       "      <td>0.228087</td>\n",
       "      <td>0.061502</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.011681</td>\n",
       "      <td>0.133718</td>\n",
       "      <td>0.355709</td>\n",
       "      <td>0.208342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_00351.jpg</th>\n",
       "      <td>0.234675</td>\n",
       "      <td>0.017929</td>\n",
       "      <td>0.008389</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.004263</td>\n",
       "      <td>0.414202</td>\n",
       "      <td>0.047530</td>\n",
       "      <td>0.272966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_00361.jpg</th>\n",
       "      <td>0.328094</td>\n",
       "      <td>0.434292</td>\n",
       "      <td>0.005640</td>\n",
       "      <td>0.002344</td>\n",
       "      <td>0.124282</td>\n",
       "      <td>0.008169</td>\n",
       "      <td>0.004470</td>\n",
       "      <td>0.092709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_00366.jpg</th>\n",
       "      <td>0.207232</td>\n",
       "      <td>0.015984</td>\n",
       "      <td>0.003724</td>\n",
       "      <td>0.007631</td>\n",
       "      <td>0.458124</td>\n",
       "      <td>0.081228</td>\n",
       "      <td>0.002846</td>\n",
       "      <td>0.223232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_01300.jpg</th>\n",
       "      <td>0.266856</td>\n",
       "      <td>0.024503</td>\n",
       "      <td>0.004301</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.008790</td>\n",
       "      <td>0.316010</td>\n",
       "      <td>0.101391</td>\n",
       "      <td>0.277699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_01363.jpg</th>\n",
       "      <td>0.283923</td>\n",
       "      <td>0.374032</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.003815</td>\n",
       "      <td>0.016936</td>\n",
       "      <td>0.006555</td>\n",
       "      <td>0.280258</td>\n",
       "      <td>0.034302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_01654.jpg</th>\n",
       "      <td>0.132141</td>\n",
       "      <td>0.008131</td>\n",
       "      <td>0.583581</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.032804</td>\n",
       "      <td>0.087141</td>\n",
       "      <td>0.010772</td>\n",
       "      <td>0.145129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_01738.jpg</th>\n",
       "      <td>0.135408</td>\n",
       "      <td>0.003183</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.838679</td>\n",
       "      <td>0.008777</td>\n",
       "      <td>0.005606</td>\n",
       "      <td>0.007486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_02609.jpg</th>\n",
       "      <td>0.213303</td>\n",
       "      <td>0.079774</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.011427</td>\n",
       "      <td>0.174682</td>\n",
       "      <td>0.357514</td>\n",
       "      <td>0.163138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_02645.jpg</th>\n",
       "      <td>0.206184</td>\n",
       "      <td>0.023211</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.004156</td>\n",
       "      <td>0.379179</td>\n",
       "      <td>0.061102</td>\n",
       "      <td>0.325302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_02820.jpg</th>\n",
       "      <td>0.205710</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.006159</td>\n",
       "      <td>0.694634</td>\n",
       "      <td>0.053588</td>\n",
       "      <td>0.004532</td>\n",
       "      <td>0.027931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_02996.jpg</th>\n",
       "      <td>0.382785</td>\n",
       "      <td>0.442854</td>\n",
       "      <td>0.018268</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.066546</td>\n",
       "      <td>0.008046</td>\n",
       "      <td>0.005220</td>\n",
       "      <td>0.075203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_03037.jpg</th>\n",
       "      <td>0.306881</td>\n",
       "      <td>0.489481</td>\n",
       "      <td>0.006799</td>\n",
       "      <td>0.001862</td>\n",
       "      <td>0.072296</td>\n",
       "      <td>0.004806</td>\n",
       "      <td>0.003233</td>\n",
       "      <td>0.114644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_03429.jpg</th>\n",
       "      <td>0.263850</td>\n",
       "      <td>0.026743</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.003901</td>\n",
       "      <td>0.372307</td>\n",
       "      <td>0.052687</td>\n",
       "      <td>0.278442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_03509.jpg</th>\n",
       "      <td>0.273065</td>\n",
       "      <td>0.487864</td>\n",
       "      <td>0.006614</td>\n",
       "      <td>0.012447</td>\n",
       "      <td>0.108852</td>\n",
       "      <td>0.006541</td>\n",
       "      <td>0.005970</td>\n",
       "      <td>0.098648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_03984.jpg</th>\n",
       "      <td>0.216663</td>\n",
       "      <td>0.009330</td>\n",
       "      <td>0.005072</td>\n",
       "      <td>0.011726</td>\n",
       "      <td>0.435854</td>\n",
       "      <td>0.106382</td>\n",
       "      <td>0.002429</td>\n",
       "      <td>0.212544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_04065.jpg</th>\n",
       "      <td>0.262527</td>\n",
       "      <td>0.026045</td>\n",
       "      <td>0.001469</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.005588</td>\n",
       "      <td>0.308517</td>\n",
       "      <td>0.149539</td>\n",
       "      <td>0.245435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_04093.jpg</th>\n",
       "      <td>0.127946</td>\n",
       "      <td>0.005297</td>\n",
       "      <td>0.626186</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.046316</td>\n",
       "      <td>0.085254</td>\n",
       "      <td>0.014201</td>\n",
       "      <td>0.094431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_04500.jpg</th>\n",
       "      <td>0.311772</td>\n",
       "      <td>0.494976</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>0.007745</td>\n",
       "      <td>0.046729</td>\n",
       "      <td>0.003203</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>0.131941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_06086.jpg</th>\n",
       "      <td>0.444975</td>\n",
       "      <td>0.030888</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.452319</td>\n",
       "      <td>0.004077</td>\n",
       "      <td>0.007359</td>\n",
       "      <td>0.059789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_06338.jpg</th>\n",
       "      <td>0.333847</td>\n",
       "      <td>0.432674</td>\n",
       "      <td>0.005599</td>\n",
       "      <td>0.001851</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.008998</td>\n",
       "      <td>0.012893</td>\n",
       "      <td>0.116225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_06554.jpg</th>\n",
       "      <td>0.358749</td>\n",
       "      <td>0.490988</td>\n",
       "      <td>0.004944</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>0.055173</td>\n",
       "      <td>0.004957</td>\n",
       "      <td>0.004183</td>\n",
       "      <td>0.078199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_06563.jpg</th>\n",
       "      <td>0.318200</td>\n",
       "      <td>0.015903</td>\n",
       "      <td>0.006127</td>\n",
       "      <td>0.003523</td>\n",
       "      <td>0.491072</td>\n",
       "      <td>0.022383</td>\n",
       "      <td>0.052711</td>\n",
       "      <td>0.090080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_06727.jpg</th>\n",
       "      <td>0.376380</td>\n",
       "      <td>0.063566</td>\n",
       "      <td>0.003806</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.495720</td>\n",
       "      <td>0.013356</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.046778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_07195.jpg</th>\n",
       "      <td>0.265069</td>\n",
       "      <td>0.354318</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>0.021411</td>\n",
       "      <td>0.009718</td>\n",
       "      <td>0.299484</td>\n",
       "      <td>0.045347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_07210.jpg</th>\n",
       "      <td>0.201502</td>\n",
       "      <td>0.014479</td>\n",
       "      <td>0.007199</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.005661</td>\n",
       "      <td>0.416526</td>\n",
       "      <td>0.034841</td>\n",
       "      <td>0.319749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_07698.jpg</th>\n",
       "      <td>0.150864</td>\n",
       "      <td>0.080817</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.022088</td>\n",
       "      <td>0.029891</td>\n",
       "      <td>0.444917</td>\n",
       "      <td>0.009345</td>\n",
       "      <td>0.261918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_07771.jpg</th>\n",
       "      <td>0.262581</td>\n",
       "      <td>0.087051</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.011261</td>\n",
       "      <td>0.124735</td>\n",
       "      <td>0.331276</td>\n",
       "      <td>0.182764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_07851.jpg</th>\n",
       "      <td>0.375133</td>\n",
       "      <td>0.014965</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.558485</td>\n",
       "      <td>0.003892</td>\n",
       "      <td>0.001278</td>\n",
       "      <td>0.045245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ALB       BET       DOL       LAG       NoF     OTHER  \\\n",
       "img_00136.jpg  0.269772  0.311739  0.000396  0.005435  0.026962  0.007731   \n",
       "img_00248.jpg  0.255272  0.370278  0.000117  0.002284  0.008642  0.006702   \n",
       "img_00288.jpg  0.228087  0.061502  0.000918  0.000043  0.011681  0.133718   \n",
       "img_00351.jpg  0.234675  0.017929  0.008389  0.000046  0.004263  0.414202   \n",
       "img_00361.jpg  0.328094  0.434292  0.005640  0.002344  0.124282  0.008169   \n",
       "img_00366.jpg  0.207232  0.015984  0.003724  0.007631  0.458124  0.081228   \n",
       "img_01300.jpg  0.266856  0.024503  0.004301  0.000449  0.008790  0.316010   \n",
       "img_01363.jpg  0.283923  0.374032  0.000179  0.003815  0.016936  0.006555   \n",
       "img_01654.jpg  0.132141  0.008131  0.583581  0.000300  0.032804  0.087141   \n",
       "img_01738.jpg  0.135408  0.003183  0.000491  0.000370  0.838679  0.008777   \n",
       "img_02609.jpg  0.213303  0.079774  0.000101  0.000061  0.011427  0.174682   \n",
       "img_02645.jpg  0.206184  0.023211  0.000519  0.000346  0.004156  0.379179   \n",
       "img_02820.jpg  0.205710  0.007301  0.000145  0.006159  0.694634  0.053588   \n",
       "img_02996.jpg  0.382785  0.442854  0.018268  0.001078  0.066546  0.008046   \n",
       "img_03037.jpg  0.306881  0.489481  0.006799  0.001862  0.072296  0.004806   \n",
       "img_03429.jpg  0.263850  0.026743  0.001951  0.000118  0.003901  0.372307   \n",
       "img_03509.jpg  0.273065  0.487864  0.006614  0.012447  0.108852  0.006541   \n",
       "img_03984.jpg  0.216663  0.009330  0.005072  0.011726  0.435854  0.106382   \n",
       "img_04065.jpg  0.262527  0.026045  0.001469  0.000879  0.005588  0.308517   \n",
       "img_04093.jpg  0.127946  0.005297  0.626186  0.000368  0.046316  0.085254   \n",
       "img_04500.jpg  0.311772  0.494976  0.002623  0.007745  0.046729  0.003203   \n",
       "img_06086.jpg  0.444975  0.030888  0.000541  0.000051  0.452319  0.004077   \n",
       "img_06338.jpg  0.333847  0.432674  0.005599  0.001851  0.087912  0.008998   \n",
       "img_06554.jpg  0.358749  0.490988  0.004944  0.002807  0.055173  0.004957   \n",
       "img_06563.jpg  0.318200  0.015903  0.006127  0.003523  0.491072  0.022383   \n",
       "img_06727.jpg  0.376380  0.063566  0.003806  0.000103  0.495720  0.013356   \n",
       "img_07195.jpg  0.265069  0.354318  0.000348  0.004307  0.021411  0.009718   \n",
       "img_07210.jpg  0.201502  0.014479  0.007199  0.000042  0.005661  0.416526   \n",
       "img_07698.jpg  0.150864  0.080817  0.000160  0.022088  0.029891  0.444917   \n",
       "img_07771.jpg  0.262581  0.087051  0.000300  0.000032  0.011261  0.124735   \n",
       "img_07851.jpg  0.375133  0.014965  0.000911  0.000091  0.558485  0.003892   \n",
       "\n",
       "                  SHARK       YFT  \n",
       "img_00136.jpg  0.345697  0.032268  \n",
       "img_00248.jpg  0.322051  0.034654  \n",
       "img_00288.jpg  0.355709  0.208342  \n",
       "img_00351.jpg  0.047530  0.272966  \n",
       "img_00361.jpg  0.004470  0.092709  \n",
       "img_00366.jpg  0.002846  0.223232  \n",
       "img_01300.jpg  0.101391  0.277699  \n",
       "img_01363.jpg  0.280258  0.034302  \n",
       "img_01654.jpg  0.010772  0.145129  \n",
       "img_01738.jpg  0.005606  0.007486  \n",
       "img_02609.jpg  0.357514  0.163138  \n",
       "img_02645.jpg  0.061102  0.325302  \n",
       "img_02820.jpg  0.004532  0.027931  \n",
       "img_02996.jpg  0.005220  0.075203  \n",
       "img_03037.jpg  0.003233  0.114644  \n",
       "img_03429.jpg  0.052687  0.278442  \n",
       "img_03509.jpg  0.005970  0.098648  \n",
       "img_03984.jpg  0.002429  0.212544  \n",
       "img_04065.jpg  0.149539  0.245435  \n",
       "img_04093.jpg  0.014201  0.094431  \n",
       "img_04500.jpg  0.001011  0.131941  \n",
       "img_06086.jpg  0.007359  0.059789  \n",
       "img_06338.jpg  0.012893  0.116225  \n",
       "img_06554.jpg  0.004183  0.078199  \n",
       "img_06563.jpg  0.052711  0.090080  \n",
       "img_06727.jpg  0.000291  0.046778  \n",
       "img_07195.jpg  0.299484  0.045347  \n",
       "img_07210.jpg  0.034841  0.319749  \n",
       "img_07698.jpg  0.009345  0.261918  \n",
       "img_07771.jpg  0.331276  0.182764  \n",
       "img_07851.jpg  0.001278  0.045245  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df1.set_index(\"image\").loc[t1[(t1[\"label\"] != p1[\"label\"]) & (t1[\"label\"] == \"ALB\") & (p1[\"label\"] != \"YFT\")].index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pydata]",
   "language": "python",
   "name": "conda-env-pydata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
